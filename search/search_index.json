{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":""},{"location":"index.html#fast-accurate-and-scalable-probabilistic-data-linkage","title":"Fast, accurate and scalable probabilistic data linkage","text":"<p>Splink is a Python package for probabilistic record linkage (entity resolution) that allows you to deduplicate and link records from datasets without unique identifiers.</p>"},{"location":"index.html#key-features","title":"Key Features","text":"<p>\u26a1 Speed: Capable of linking a million records on a laptop in approximately one minute. \ud83c\udfaf Accuracy: Full support for term frequency adjustments and user-defined fuzzy matching logic. \ud83c\udf10 Scalability: Execute linkage jobs in Python (using DuckDB) or big-data backends like AWS Athena or Spark for 100+ million records. \ud83c\udf93 Unsupervised Learning: No training data is required, as models can be trained using an unsupervised approach. \ud83d\udcca Interactive Outputs: Provides a wide range of interactive outputs to help users understand their model and diagnose linkage problems.  </p> <p>Splink's core linkage algorithm is based on Fellegi-Sunter's model of record linkage, with various customizations to improve accuracy.</p>"},{"location":"index.html#what-does-splink-do","title":"What does Splink do?","text":"<p>Consider the following records that lack a unique person identifier:</p> <p></p> <p>Splink predicts which rows link together:</p> <p></p> <p>and clusters these links to produce an estimated person ID:</p> <p></p>"},{"location":"index.html#what-data-does-splink-work-best-with","title":"What data does Splink work best with?","text":"<p>Before using Splink, input data should be standardized, with consistent column names and formatting (e.g., lowercased, punctuation cleaned up, etc.).</p> <p>Splink performs best with input data containing multiple columns that are not highly correlated. For instance, if the entity type is persons, you may have columns for full name, date of birth, and city. If the entity type is companies, you could have columns for name, turnover, sector, and telephone number.</p> <p>High correlation occurs when the value of a column is highly constrained (predictable) from the value of another column. For example, a 'city' field is almost perfectly correlated with 'postcode'. Gender is highly correlated with 'first name'. Correlation is particularly problematic if all of your input columns are highly correlated.</p> <p>Splink is not designed for linking a single column containing a 'bag of words'. For example, a table with a single 'company name' column, and no other details.</p>"},{"location":"index.html#documentation","title":"Documentation","text":"<p>The homepage for the Splink documentation can be found here. Interactive demos can be found here, or by clicking the following Binder link:</p> <p></p> <p>The specification of the Fellegi Sunter statistical model behind <code>splink</code> is similar as that used in the R fastLink package. Accompanying the fastLink package is an academic paper that describes this model. A series of interactive articles also explores the theory behind Splink.</p> <p>The Office for National Statistics have written a case study about using Splink to link 2021 Census data to itself.</p>"},{"location":"index.html#installation","title":"Installation","text":"<p>Splink supports python 3.7+. To obtain the latest released version of splink you can install from PyPI using pip:</p> <pre><code>pip install splink\n</code></pre> <p>or, if you prefer, you can instead install splink using conda:</p> <pre><code>conda install -c conda-forge splink\n</code></pre> <p>Should you require a more bare-bones version of Splink without DuckDB, please see the following area of the docs:</p> <p>DuckDBless Splink Installation</p>"},{"location":"index.html#quickstart","title":"Quickstart","text":"<p>The following code demonstrates how to estimate the parameters of a deduplication model, use it to identify duplicate records, and then use clustering to generate an estimated unique person ID.</p> <p>For more detailed tutorials, please see here.</p> <pre><code>from splink.duckdb.duckdb_linker import DuckDBLinker\nimport splink.duckdb.duckdb_comparison_library as cl\nimport splink.duckdb.duckdb_comparison_template_library as ctl\n\nimport pandas as pd\n\ndf = pd.read_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\")\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\"),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        cl.levenshtein_at_thresholds(\"email\", 2),\n    ],\n}\n\nlinker = DuckDBLinker(df, settings)\nlinker.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = \"l.first_name = r.first_name and l.surname = r.surname\"\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\nblocking_rule_for_training = \"l.dob = r.dob\"\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\npairwise_predictions = linker.predict()\n\nclusters = linker.cluster_pairwise_predictions_at_threshold(pairwise_predictions, 0.95)\nclusters.as_pandas_dataframe(limit=5)\n</code></pre>"},{"location":"index.html#videos","title":"Videos","text":"<ul> <li>A introductory presentation on Splink</li> <li>An introduction to the Splink Comparison Viewer dashboard</li> </ul>"},{"location":"index.html#support","title":"Support","text":"<p>Please post on the discussion forums if you have any questions. If you think you have found a bug, pleaase raise an issue.</p>"},{"location":"index.html#awards","title":"Awards","text":"<p>\ud83e\udd47 Analysis in Government Awards 2020: Innovative Methods: Winner</p> <p>\ud83e\udd47 MoJ DASD Awards 2020: Innovation and Impact - Winner</p> <p>\ud83e\udd47 Analysis in Government Awards 2022: People's Choice Award - Winner</p> <p>\ud83e\udd48 Analysis in Government Awards 2022: Innovative Methods Runner up</p>"},{"location":"index.html#citation","title":"Citation","text":"<p>If you use Splink in your research, we'd be grateful for a citation as follows:</p> <pre><code>@article{Linacre_Lindsay_Manassis_Slade_Hepworth_2022,\ntitle        = {Splink: Free software for probabilistic record linkage at scale.},\nauthor       = {Linacre, Robin and Lindsay, Sam and Manassis, Theodore and Slade, Zoe and Hepworth, Tom and Kennedy, Ross and Bond, Andrew},\nyear         = 2022,\nmonth        = {Aug.},\njournal      = {International Journal of Population Data Science},\nvolume       = 7,\nnumber       = 3,\ndoi          = {10.23889/ijpds.v7i3.1794},\nurl          = {https://ijpds.org/article/view/1794},\n}\n</code></pre>"},{"location":"index.html#acknowledgements","title":"Acknowledgements","text":"<p>We are very grateful to ADR UK (Administrative Data Research UK) for providing the initial funding for this work as part of the Data First project.</p> <p>We are extremely grateful to professors Katie Harron, James Doidge and Peter Christen for their expert advice and guidance in the development of Splink. We are also very grateful to colleagues at the UK's Office for National Statistics for their expert advice and peer review of this work. Any errors remain our own.</p>"},{"location":"LICENSE.html","title":"LICENSE","text":"<p>MIT License</p> <p>Copyright (c) 2020 Ministry of Justice</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"SplinkDataFrame.html","title":"Documentation for <code>SplinkDataFrame</code> object","text":"<p>Abstraction over dataframe to handle basic operations like retrieving data and retrieving column names, which need different implementations depending on whether it's a spark dataframe, sqlite table etc. Uses methods like <code>as_pandas_dataframe()</code> and <code>as_record_dict()</code> to retrieve data</p> Source code in <code>splink/splink_dataframe.py</code> <pre><code>class SplinkDataFrame:\n\"\"\"Abstraction over dataframe to handle basic operations like retrieving data and\n    retrieving column names, which need different implementations depending on whether\n    it's a spark dataframe, sqlite table etc.\n    Uses methods like `as_pandas_dataframe()` and `as_record_dict()` to retrieve data\n    \"\"\"\n\n    def __init__(self, templated_name: str, physical_name: str, linker: Linker):\n        self.templated_name = templated_name\n        self.physical_name = physical_name\n        self.linker = linker\n\n    @property\n    def columns(self):\n        pass\n\n    @property\n    def columns_escaped(self):\n        cols = self.columns\n        return [c.name() for c in cols]\n\n    def validate():\n        pass\n\n    def _random_sample_sql(percent):\n        raise NotImplementedError(\"Random sample sql not implemented for this linker\")\n\n    @property\n    def physical_and_template_names_equal(self):\n        return self.templated_name == self.physical_name\n\n    def _check_drop_table_created_by_splink(self, force_non_splink_table=False):\n        if not self.physical_name.startswith(\"__splink__\"):\n            if not force_non_splink_table:\n                raise ValueError(\n                    f\"You've asked to drop table {self.physical_name} from your \"\n                    \"database which is not a table created by Splink.  If you really \"\n                    \"want to drop this table, you can do so by setting \"\n                    \"force_non_splink_table=True\"\n                )\n        logger.debug(\n            f\"Dropping table with templated name {self.templated_name} and \"\n            f\"physical name {self.physical_name}\"\n        )\n\n    def drop_table_from_database(self, force_non_splink_table=False):\n        raise NotImplementedError(\n            \"Drop table from database not implemented for this linker\"\n        )\n\n    def as_record_dict(self, limit=None):\n        pass\n\n    def as_pandas_dataframe(self, limit=None):\n\"\"\"Return the dataframe as a pandas dataframe.\n\n        This can be computationally expensive if the dataframe is large.\n\n        Args:\n            limit (int, optional): If provided, return this number of rows (equivalent\n            to a limit statement in SQL). Defaults to None, meaning return all rows\n\n        Returns:\n            pandas.DataFrame: pandas Dataframe\n        \"\"\"\n        import pandas as pd\n\n        return pd.DataFrame(self.as_record_dict(limit=limit))\n\n    def _repr_pretty_(self, p, cycle):\n        msg = (\n            f\"Table name in database: `{self.physical_name}`\\n\"\n            \"\\nTo retrieve records, you can call the following methods on this object:\"\n            \"\\n`.as_record_dict(limit=5)` or \"\n            \"`.as_pandas_dataframe(limit=5)`.\\n\"\n            \"\\nYou may omit the `limit` argument to return all records.\"\n            \"\\n\\nThis table represents the following splink entity: \"\n            f\"{self.templated_name}\"\n        )\n        p.text(msg)\n\n    def to_parquet(self, filepath, overwrite=False):\n        raise NotImplementedError(\"`to_parquet` not implemented for this linker\")\n\n    def to_csv(self, filepath, overwrite=False):\n        raise NotImplementedError(\"`to_csv` not implemented for this linker\")\n\n    def check_file_exists(self, filepath):\n        p = Path(filepath)\n        if p.exists():\n            raise FileExistsError(\n                \"The filepath you've supplied already exists. Please use \"\n                \"either `overwrite = True` or manually move or delete the \"\n                \"existing file.\"\n            )\n</code></pre>","tags":["API"]},{"location":"SplinkDataFrame.html#splink.splink_dataframe.SplinkDataFrame.drop_table_from_database","title":"<code>drop_table_from_database(force_non_splink_table=False)</code>","text":"Source code in <code>splink/splink_dataframe.py</code> <pre><code>def drop_table_from_database(self, force_non_splink_table=False):\n    raise NotImplementedError(\n        \"Drop table from database not implemented for this linker\"\n    )\n</code></pre>","tags":["API"]},{"location":"SplinkDataFrame.html#splink.splink_dataframe.SplinkDataFrame.as_pandas_dataframe","title":"<code>as_pandas_dataframe(limit=None)</code>","text":"<p>Return the dataframe as a pandas dataframe.</p> <p>This can be computationally expensive if the dataframe is large.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>If provided, return this number of rows (equivalent</p> <code>None</code> <p>Returns:</p> Type Description <p>pandas.DataFrame: pandas Dataframe</p> Source code in <code>splink/splink_dataframe.py</code> <pre><code>def as_pandas_dataframe(self, limit=None):\n\"\"\"Return the dataframe as a pandas dataframe.\n\n    This can be computationally expensive if the dataframe is large.\n\n    Args:\n        limit (int, optional): If provided, return this number of rows (equivalent\n        to a limit statement in SQL). Defaults to None, meaning return all rows\n\n    Returns:\n        pandas.DataFrame: pandas Dataframe\n    \"\"\"\n    import pandas as pd\n\n    return pd.DataFrame(self.as_record_dict(limit=limit))\n</code></pre>","tags":["API"]},{"location":"comparison.html","title":"Documentation for <code>Comparison</code> object","text":"<p>Each Comparison defines how data from one or more input columns is compared to assess its similarity.</p> <p>For example, one Comparison may represent how similarity is assessed for a person's date of birth.  Others may represent the comparison of a person's name or location.</p> <p>The method used to assess similarity will depend on the type of data - for instance, the method used to assess similarity of a company's turnover would be different to the method used to assess the similarity of a person's first name.</p> <p>A linking model thus usually contains several Comparisons.</p> <p>As far as possible, Comparisons should be configured to satisfy the assumption of independece conditional on the true match status, a key assumption of the Fellegi Sunter probabilistic linkage model.  This would be broken, for example, if a model contained one Comparison for city, and another for postcode. Instead, in this example, a single comparison should be modelled, which may to capture similarity taking account of both the city and postcode field.</p> <p>Each Comparison contains two or more <code>ComparisonLevel</code>s which define the gradations of similarity between the input columns within the Comparison.</p> <p>For example, for the date of birth Comparison there may be a ComparisonLevel for an exact match, another for a one-character difference, and another for all other comparisons.</p> <p>To summarise:</p> <pre><code>Data Linking Model\n\u251c\u2500-- Comparison: Date of birth\n\u2502    \u251c\u2500-- ComparisonLevel: Exact match\n\u2502    \u251c\u2500-- ComparisonLevel: One character difference\n\u2502    \u251c\u2500-- ComparisonLevel: All other\n\u251c\u2500-- Comparison: Name\n\u2502    \u251c\u2500-- ComparisonLevel: Exact match on first name and surname\n\u2502    \u251c\u2500-- ComparisonLevel: Exact match on first name\n\u2502    \u251c\u2500-- etc.\n</code></pre> Source code in <code>splink/comparison.py</code> <pre><code>class Comparison:\n\"\"\"Each Comparison defines how data from one or more input columns is\n    compared to assess its similarity.\n\n    For example, one Comparison may represent how similarity is assessed for a\n    person's date of birth.  Others may represent the comparison of a person's name or\n    location.\n\n    The method used to assess similarity will depend on the type of data -\n    for instance, the method used to assess similarity of a company's turnover would\n    be different to the method used to assess the similarity of a person's first name.\n\n    A linking model thus usually contains several Comparisons.\n\n    As far as possible, Comparisons should be configured to satisfy the assumption of\n    independece conditional on the true match status, a key assumption of the Fellegi\n    Sunter probabilistic linkage model.  This would be broken, for example, if a model\n    contained one Comparison for city, and another for postcode. Instead, in this\n    example, a single comparison should be modelled, which may to capture similarity\n    taking account of both the city and postcode field.\n\n    Each Comparison contains two or more `ComparisonLevel`s which define the gradations\n    of similarity between the input columns within the Comparison.\n\n    For example, for the date of birth Comparison there may be a ComparisonLevel for an\n    exact match, another for a one-character difference, and another for all other\n    comparisons.\n\n    To summarise:\n\n    ```\n    Data Linking Model\n    \u251c\u2500-- Comparison: Date of birth\n    \u2502    \u251c\u2500-- ComparisonLevel: Exact match\n    \u2502    \u251c\u2500-- ComparisonLevel: One character difference\n    \u2502    \u251c\u2500-- ComparisonLevel: All other\n    \u251c\u2500-- Comparison: Name\n    \u2502    \u251c\u2500-- ComparisonLevel: Exact match on first name and surname\n    \u2502    \u251c\u2500-- ComparisonLevel: Exact match on first name\n    \u2502    \u251c\u2500-- etc.\n    ```\n\n    \"\"\"\n\n    def __init__(self, comparison_dict, settings_obj: Settings = None):\n        # Protected because we don't want to modify\n        self._comparison_dict = comparison_dict\n        comparison_level_list = comparison_dict[\"comparison_levels\"]\n        self.comparison_levels: list[ComparisonLevel] = []\n\n        # If comparison_levels are already of type ComparisonLevel, register\n        # the settings object on them\n        # otherwise turn the dictionaries into ComparisonLevel\n\n        for cl in comparison_level_list:\n            if isinstance(cl, ComparisonLevel):\n                cl.comparison = self\n            elif settings_obj is None:\n                cl = ComparisonLevel(cl, self)\n            else:\n                cl = ComparisonLevel(cl, self, sql_dialect=settings_obj._sql_dialect)\n\n            self.comparison_levels.append(cl)\n\n        self._settings_obj: Settings = settings_obj\n\n        # Assign comparison vector values starting at highest level, count down to 0\n        num_levels = self._num_levels\n        counter = num_levels - 1\n\n        for level in self.comparison_levels:\n            if level.is_null_level:\n                level._comparison_vector_value = -1\n                level._max_level = False\n            else:\n                level._comparison_vector_value = counter\n                if counter == num_levels - 1:\n                    level._max_level = True\n                else:\n                    level._max_level = False\n                counter -= 1\n\n    def __deepcopy__(self, memo):\n\"\"\"When we do EM training, we need a copy of the Comparison which is independent\n        of the original e.g. modifying the copy will not affect the original.\n        This method implements ensures the Comparison can be deepcopied.\n        \"\"\"\n        cc = Comparison(self.as_dict(), self._settings_obj)\n        return cc\n\n    @property\n    def _num_levels(self):\n        return len([cl for cl in self.comparison_levels if not cl.is_null_level])\n\n    @property\n    def _comparison_levels_excluding_null(self):\n        return [cl for cl in self.comparison_levels if not cl.is_null_level]\n\n    @property\n    def _gamma_prefix(self):\n        return self._settings_obj._gamma_prefix\n\n    @property\n    def _retain_intermediate_calculation_columns(self):\n        return self._settings_obj._retain_intermediate_calculation_columns\n\n    @property\n    def _bf_column_name(self):\n        return f\"{self._settings_obj._bf_prefix}{self._output_column_name}\".replace(\n            \" \", \"_\"\n        )\n\n    @property\n    def _has_null_level(self):\n        return any([cl.is_null_level for cl in self.comparison_levels])\n\n    @property\n    def _bf_tf_adj_column_name(self):\n        bf = self._settings_obj._bf_prefix\n        tf = self._settings_obj._tf_prefix\n        cc_name = self._output_column_name\n        return f\"{bf}{tf}adj_{cc_name}\".replace(\" \", \"_\")\n\n    @property\n    def _has_tf_adjustments(self):\n        return any([cl._has_tf_adjustments for cl in self.comparison_levels])\n\n    @property\n    def _case_statement(self):\n        sqls = [\n            cl._when_then_comparison_vector_value_sql for cl in self.comparison_levels\n        ]\n        sql = \" \".join(sqls)\n        sql = f\"CASE {sql} END as {self._gamma_column_name}\"\n\n        return sql\n\n    @property\n    def _input_columns_used_by_case_statement(self):\n        cols = []\n        for cl in self.comparison_levels:\n            cols.extend(cl._input_columns_used_by_sql_condition)\n\n        # dedupe_preserving_order on input column\n        already_observed = []\n        deduped_cols = []\n        for col in cols:\n            if col.input_name not in already_observed:\n                deduped_cols.append(col)\n                already_observed.append(col.input_name)\n\n        return deduped_cols\n\n    @property\n    def _output_column_name(self):\n        if \"output_column_name\" in self._comparison_dict:\n            return self._comparison_dict[\"output_column_name\"]\n        else:\n            cols = self._input_columns_used_by_case_statement\n            cols = [c.input_name for c in cols]\n            if len(cols) == 1:\n                return cols[0]\n            else:\n                return f\"custom_{'_'.join(cols)}\"\n\n    @property\n    def _comparison_description(self):\n        if \"comparison_description\" in self._comparison_dict:\n            return self._comparison_dict[\"comparison_description\"]\n        else:\n            return self._output_column_name\n\n    @property\n    def _gamma_column_name(self):\n        return f\"{self._gamma_prefix}{self._output_column_name}\".replace(\" \", \"_\")\n\n    @property\n    def _tf_adjustment_input_col_names(self):\n        cols = [cl._tf_adjustment_input_column_name for cl in self.comparison_levels]\n        cols = [c for c in cols if c]\n\n        return cols\n\n    @property\n    def _columns_to_select_for_blocking(self):\n        cols = []\n        for cl in self.comparison_levels:\n            cols.extend(cl._columns_to_select_for_blocking)\n\n        return dedupe_preserving_order(cols)\n\n    @property\n    def _columns_to_select_for_comparison_vector_values(self):\n        input_cols = []\n        for cl in self.comparison_levels:\n            input_cols.extend(cl._input_columns_used_by_sql_condition)\n\n        output_cols = []\n        for col in input_cols:\n            if self._settings_obj._retain_matching_columns:\n                output_cols.extend(col.names_l_r())\n\n        output_cols.append(self._case_statement)\n\n        for cl in self.comparison_levels:\n            if cl._has_tf_adjustments:\n                col = cl._tf_adjustment_input_column\n                output_cols.extend(col.tf_name_l_r())\n\n        return dedupe_preserving_order(output_cols)\n\n    @property\n    def _columns_to_select_for_bayes_factor_parts(self):\n        input_cols = []\n        for cl in self.comparison_levels:\n            input_cols.extend(cl._input_columns_used_by_sql_condition)\n\n        output_cols = []\n        for col in input_cols:\n            if self._settings_obj._retain_matching_columns:\n                output_cols.extend(col.names_l_r())\n\n        output_cols.append(self._gamma_column_name)\n\n        for cl in self.comparison_levels:\n            if (\n                cl._has_tf_adjustments\n                and self._settings_obj._retain_intermediate_calculation_columns\n            ):\n                col = cl._tf_adjustment_input_column\n                output_cols.extend(col.tf_name_l_r())\n\n        # Bayes factor case when statement\n        sqls = [cl._bayes_factor_sql for cl in self.comparison_levels]\n        sql = \" \".join(sqls)\n        sql = f\"CASE {sql} END as {self._bf_column_name} \"\n        output_cols.append(sql)\n\n        # tf adjustment case when statement\n\n        if self._has_tf_adjustments:\n            sqls = [cl._tf_adjustment_sql for cl in self.comparison_levels]\n            sql = \" \".join(sqls)\n            sql = f\"CASE {sql} END as {self._bf_tf_adj_column_name} \"\n            output_cols.append(sql)\n        output_cols.append(self._gamma_column_name)\n\n        return dedupe_preserving_order(output_cols)\n\n    @property\n    def _columns_to_select_for_predict(self):\n        input_cols = []\n        for cl in self.comparison_levels:\n            input_cols.extend(cl._input_columns_used_by_sql_condition)\n\n        output_cols = []\n        for col in input_cols:\n            if self._settings_obj._retain_matching_columns:\n                output_cols.extend(col.names_l_r())\n\n        if (\n            self._settings_obj._training_mode\n            or self._settings_obj._retain_matching_columns\n        ):\n            output_cols.append(self._gamma_column_name)\n\n        for cl in self.comparison_levels:\n            if (\n                cl._has_tf_adjustments\n                and self._settings_obj._retain_intermediate_calculation_columns\n            ):\n                col = cl._tf_adjustment_input_column\n                output_cols.extend(col.tf_name_l_r())\n\n        for _col in input_cols:\n            if self._settings_obj._retain_intermediate_calculation_columns:\n                output_cols.extend(self._match_weight_columns_to_multiply)\n\n        return dedupe_preserving_order(output_cols)\n\n    @property\n    def _match_weight_columns_to_multiply(self):\n        cols = []\n        cols.append(self._bf_column_name)\n        if self._has_tf_adjustments:\n            cols.append(self._bf_tf_adj_column_name)\n        return cols\n\n    @property\n    def _term_frequency_columns(self):\n        cols = set()\n        for cl in self.comparison_levels:\n            cols.add(cl.tf_adjustment_input_col_name)\n        return list(cols)\n\n    def as_dict(self):\n        d = {\n            \"output_column_name\": self._output_column_name,\n            \"comparison_levels\": [cl.as_dict() for cl in self.comparison_levels],\n        }\n        if \"comparison_description\" in self._comparison_dict:\n            d[\"comparison_description\"] = self._comparison_dict[\n                \"comparison_description\"\n            ]\n        return d\n\n    def _as_completed_dict(self):\n        return {\n            \"column_name\": self._output_column_name,\n            \"comparison_levels\": [\n                cl._as_completed_dict() for cl in self.comparison_levels\n            ],\n            \"input_columns_used_by_case_statement\": [\n                c.input_name for c in self._input_columns_used_by_case_statement\n            ],\n        }\n\n    @property\n    def _has_estimated_m_values(self):\n        return all(cl._has_estimated_m_values for cl in self.comparison_levels)\n\n    @property\n    def _has_estimated_u_values(self):\n        return all(cl._has_estimated_u_values for cl in self.comparison_levels)\n\n    @property\n    def _all_m_are_trained(self):\n        return all(cl._m_is_trained for cl in self.comparison_levels)\n\n    @property\n    def _all_u_are_trained(self):\n        return all(cl._u_is_trained for cl in self.comparison_levels)\n\n    @property\n    def _some_m_are_trained(self):\n        return any(cl._m_is_trained for cl in self._comparison_levels_excluding_null)\n\n    @property\n    def _some_u_are_trained(self):\n        return any(cl._u_is_trained for cl in self._comparison_levels_excluding_null)\n\n    @property\n    def _is_trained_message(self):\n        messages = []\n        if self._all_m_are_trained and self._all_u_are_trained:\n            return None\n\n        if not self._some_u_are_trained:\n            messages.append(\"no u values are trained\")\n        elif self._some_u_are_trained and not self._all_u_are_trained:\n            messages.append(\"some u values are not trained\")\n\n        if not self._some_m_are_trained:\n            messages.append(\"no m values are trained\")\n        elif self._some_m_are_trained and not self._all_m_are_trained:\n            messages.append(\"some m values are not trained\")\n\n        message = \", \".join(messages)\n        message = f\"    - {self._output_column_name} ({message}).\"\n        return message\n\n    @property\n    def _is_trained(self):\n        return self._all_m_are_trained and self._all_u_are_trained\n\n    @property\n    def _as_detailed_records(self):\n        records = []\n        for cl in self.comparison_levels:\n            record = {}\n            record[\"comparison_name\"] = self._output_column_name\n            record = {**record, **cl._as_detailed_record}\n            records.append(record)\n        return records\n\n    @property\n    def _parameter_estimates_as_records(self):\n        records = []\n        for cl in self.comparison_levels:\n            new_records = cl._parameter_estimates_as_records\n            for r in new_records:\n                r[\"comparison_name\"] = self._output_column_name\n            records.extend(new_records)\n\n        return records\n\n    def _get_comparison_level_by_comparison_vector_value(\n        self, value\n    ) -&gt; ComparisonLevel:\n        for cl in self.comparison_levels:\n            if cl._comparison_vector_value == value:\n                return cl\n        raise ValueError(f\"No comparison level with comparison vector value {value}\")\n\n    def __repr__(self):\n        return (\n            f\"&lt;Comparison {self._comparison_description} with \"\n            f\"{self._num_levels} levels at {hex(id(self))}&gt;\"\n        )\n\n    @property\n    def _not_trained_messages(self):\n        msgs = []\n\n        cname = self._output_column_name\n\n        header = f\"Comparison: '{cname}':\\n\"\n\n        msg_template = \"{header}    {m_or_u} values not fully trained\"\n\n        if not self._all_m_are_trained:\n            msgs.append(msg_template.format(header=header, m_or_u=\"m\"))\n        if not self._all_u_are_trained:\n            msgs.append(msg_template.format(header=header, m_or_u=\"u\"))\n\n        return msgs\n\n    @property\n    def _comparison_level_description_list(self):\n        cl_template = \"    - '{label}' with SQL rule: {sql}\\n\"\n\n        comp_levels = [\n            cl_template.format(\n                cvv=cl._comparison_vector_value,\n                label=cl.label_for_charts,\n                sql=cl.sql_condition,\n            )\n            for cl in self.comparison_levels\n        ]\n        comp_levels = \"\".join(comp_levels)\n        return comp_levels\n\n    @property\n    def _human_readable_description_succinct(self):\n        input_cols = join_list_with_commas_final_and(\n            [c.name() for c in self._input_columns_used_by_case_statement]\n        )\n\n        comp_levels = self._comparison_level_description_list\n\n        if \"comparison_description\" in self._comparison_dict:\n            main_desc = (\n                f\"of {input_cols}\\nDescription: '{self._comparison_description}'\"\n            )\n        else:\n            main_desc = f\"of {input_cols}\"\n\n        desc = f\"Comparison {main_desc}\\nComparison levels:\\n{comp_levels}\"\n        return desc\n\n    @property\n    def human_readable_description(self):\n        input_cols = join_list_with_commas_final_and(\n            [c.name() for c in self._input_columns_used_by_case_statement]\n        )\n\n        comp_levels = self._comparison_level_description_list\n\n        if \"comparison_description\" in self._comparison_dict:\n            main_desc = f\"'{self._comparison_description}' of {input_cols}\"\n        else:\n            main_desc = f\"of {input_cols}\"\n\n        desc = (\n            f\"Comparison {main_desc}.\\n\"\n            \"Similarity is assessed using the following \"\n            f\"ComparisonLevels:\\n{comp_levels}\"\n        )\n\n        return desc\n\n    def match_weights_chart(self, as_dict=False):\n\"\"\"Display a chart of comparison levels of the comparison\"\"\"\n        from .charts import comparison_match_weights_chart\n\n        records = self._as_detailed_records\n        return comparison_match_weights_chart(records, as_dict=as_dict)\n</code></pre>","tags":["API"]},{"location":"comparison.html#splink.comparison.Comparison.human_readable_description","title":"<code>human_readable_description</code>  <code>property</code>","text":"","tags":["API"]},{"location":"comparison.html#splink.comparison.Comparison.__init__","title":"<code>__init__(comparison_dict, settings_obj=None)</code>","text":"Source code in <code>splink/comparison.py</code> <pre><code>def __init__(self, comparison_dict, settings_obj: Settings = None):\n    # Protected because we don't want to modify\n    self._comparison_dict = comparison_dict\n    comparison_level_list = comparison_dict[\"comparison_levels\"]\n    self.comparison_levels: list[ComparisonLevel] = []\n\n    # If comparison_levels are already of type ComparisonLevel, register\n    # the settings object on them\n    # otherwise turn the dictionaries into ComparisonLevel\n\n    for cl in comparison_level_list:\n        if isinstance(cl, ComparisonLevel):\n            cl.comparison = self\n        elif settings_obj is None:\n            cl = ComparisonLevel(cl, self)\n        else:\n            cl = ComparisonLevel(cl, self, sql_dialect=settings_obj._sql_dialect)\n\n        self.comparison_levels.append(cl)\n\n    self._settings_obj: Settings = settings_obj\n\n    # Assign comparison vector values starting at highest level, count down to 0\n    num_levels = self._num_levels\n    counter = num_levels - 1\n\n    for level in self.comparison_levels:\n        if level.is_null_level:\n            level._comparison_vector_value = -1\n            level._max_level = False\n        else:\n            level._comparison_vector_value = counter\n            if counter == num_levels - 1:\n                level._max_level = True\n            else:\n                level._max_level = False\n            counter -= 1\n</code></pre>","tags":["API"]},{"location":"comparison.html#splink.comparison.Comparison.match_weights_chart","title":"<code>match_weights_chart(as_dict=False)</code>","text":"<p>Display a chart of comparison levels of the comparison</p> Source code in <code>splink/comparison.py</code> <pre><code>def match_weights_chart(self, as_dict=False):\n\"\"\"Display a chart of comparison levels of the comparison\"\"\"\n    from .charts import comparison_match_weights_chart\n\n    records = self._as_detailed_records\n    return comparison_match_weights_chart(records, as_dict=as_dict)\n</code></pre>","tags":["API"]},{"location":"comparison_level.html","title":"Documentation for <code>ComparisonLevel</code> object","text":"<p>Each ComparisonLevel defines a gradation (category) of similarity within a <code>Comparison</code>.</p> <p>For example, a <code>Comparison</code> that uses the first_name and surname columns may define three <code>ComparisonLevel</code>s:     An exact match on first name and surname     First name and surname have a JaroWinkler score of above 0.95     All other comparisons</p> <p>The method used to assess similarity will depend on the type of data - for instance, the method used to assess similarity of a company's turnover would be different to the method used to assess the similarity of a person's first name.</p> <p>To summarise:</p> <pre><code>Data Linking Model\n\u251c\u2500-- Comparison: Name\n\u2502    \u251c\u2500-- ComparisonLevel: Exact match on first_name and surname\n\u2502    \u251c\u2500-- ComparisonLevel: first_name and surname have JaroWinkler &gt; 0.95\n\u2502    \u251c\u2500-- ComparisonLevel: All other\n\u251c\u2500-- Comparison: Date of birth\n\u2502    \u251c\u2500-- ComparisonLevel: Exact match\n\u2502    \u251c\u2500-- ComparisonLevel: One character difference\n\u2502    \u251c\u2500-- ComparisonLevel: All other\n\u251c\u2500-- etc.\n</code></pre> Source code in <code>splink/comparison_level.py</code> <pre><code>class ComparisonLevel:\n\"\"\"Each ComparisonLevel defines a gradation (category) of similarity within a\n    `Comparison`.\n\n    For example, a `Comparison` that uses the first_name and surname columns may\n    define three `ComparisonLevel`s:\n        An exact match on first name and surname\n        First name and surname have a JaroWinkler score of above 0.95\n        All other comparisons\n\n    The method used to assess similarity will depend on the type of data - for\n    instance, the method used to assess similarity of a company's turnover would be\n    different to the method used to assess the similarity of a person's first name.\n\n    To summarise:\n\n    ```\n    Data Linking Model\n    \u251c\u2500-- Comparison: Name\n    \u2502    \u251c\u2500-- ComparisonLevel: Exact match on first_name and surname\n    \u2502    \u251c\u2500-- ComparisonLevel: first_name and surname have JaroWinkler &gt; 0.95\n    \u2502    \u251c\u2500-- ComparisonLevel: All other\n    \u251c\u2500-- Comparison: Date of birth\n    \u2502    \u251c\u2500-- ComparisonLevel: Exact match\n    \u2502    \u251c\u2500-- ComparisonLevel: One character difference\n    \u2502    \u251c\u2500-- ComparisonLevel: All other\n    \u251c\u2500-- etc.\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        level_dict,\n        comparison: Comparison = None,\n        sql_dialect: str = None,\n    ):\n        # Protected, because we don't want to modify the original dict\n        self._level_dict = level_dict\n\n        self.comparison: Comparison = comparison\n        if not hasattr(self, \"_sql_dialect\"):\n            self._sql_dialect = sql_dialect\n\n        self._sql_condition = self._level_dict[\"sql_condition\"]\n        self._is_null_level = self._level_dict_val_else_default(\"is_null_level\")\n        self._tf_adjustment_weight = self._level_dict_val_else_default(\n            \"tf_adjustment_weight\"\n        )\n\n        self._tf_minimum_u_value = self._level_dict_val_else_default(\n            \"tf_minimum_u_value\"\n        )\n\n        # Private values controlled with getter/setter\n        self._m_probability = self._level_dict.get(\"m_probability\")\n        self._u_probability = self._level_dict.get(\"u_probability\")\n\n        # These will be set when the ComparisonLevel is passed into a Comparison\n        self._comparison_vector_value: int = None\n        self._max_level: bool = None\n\n        # Enable the level to 'know' when it's been trained\n        self._trained_m_probabilities: list = []\n        self._trained_u_probabilities: list = []\n\n        self._validate()\n\n    @property\n    def is_null_level(self) -&gt; bool:\n        return self._is_null_level\n\n    @property\n    def sql_condition(self) -&gt; str:\n        return self._sql_condition\n\n    def _level_dict_val_else_default(self, key):\n        val = self._level_dict.get(key)\n        if not val:\n            val = default_value_from_schema(key, \"comparison_level\")\n        return val\n\n    @property\n    def _tf_adjustment_input_column(self):\n        val = self._level_dict_val_else_default(\"tf_adjustment_column\")\n        if val:\n            return InputColumn(val, sql_dialect=self._sql_dialect)\n        else:\n            return None\n\n    @property\n    def _tf_adjustment_input_column_name(self):\n        input_column = self._tf_adjustment_input_column\n        if input_column:\n            return input_column.unquote().name()\n\n    @property\n    def _has_comparison(self):\n        from .comparison import Comparison\n\n        return isinstance(self.comparison, Comparison)\n\n    @property\n    def m_probability(self):\n        if self.is_null_level:\n            return None\n        if self._m_probability == LEVEL_NOT_OBSERVED_TEXT:\n            return 1e-6\n        if self._m_probability is None and self._has_comparison:\n            vals = _default_m_values(self.comparison._num_levels)\n            return vals[self._comparison_vector_value]\n        return self._m_probability\n\n    @m_probability.setter\n    def m_probability(self, value):\n        if self.is_null_level:\n            raise AttributeError(\"Cannot set m_probability when is_null_level is true\")\n        if value == LEVEL_NOT_OBSERVED_TEXT:\n            cc_n = self.comparison._output_column_name\n            cl_n = self.label_for_charts\n            logger.warning(\n                \"\\nWARNING:\\n\"\n                f\"Level {cl_n} on comparison {cc_n} not observed in dataset, \"\n                \"unable to train m value\"\n            )\n\n        self._m_probability = value\n\n    @property\n    def u_probability(self):\n        if self.is_null_level:\n            return None\n        if self._u_probability == LEVEL_NOT_OBSERVED_TEXT:\n            return 1e-6\n        if self._u_probability is None:\n            vals = _default_u_values(self.comparison._num_levels)\n            return vals[self._comparison_vector_value]\n        return self._u_probability\n\n    @u_probability.setter\n    def u_probability(self, value):\n        if self.is_null_level:\n            raise AttributeError(\"Cannot set u_probability when is_null_level is true\")\n        if value == LEVEL_NOT_OBSERVED_TEXT:\n            cc_n = self.comparison._output_column_name\n            cl_n = self.label_for_charts\n            logger.warning(\n                \"\\nWARNING:\\n\"\n                f\"Level {cl_n} on comparison {cc_n} not observed in dataset, \"\n                \"unable to train u value\"\n            )\n        self._u_probability = value\n\n    @property\n    def _m_probability_description(self):\n        if self.m_probability is not None:\n            return (\n                \"Amongst matching record comparisons, \"\n                f\"{self.m_probability:.2%} of records are in the \"\n                f\"{self.label_for_charts.lower()} comparison level\"\n            )\n\n    @property\n    def _u_probability_description(self):\n        if self.u_probability is not None:\n            return (\n                \"Amongst non-matching record comparisons, \"\n                f\"{self.u_probability:.2%} of records are in the \"\n                f\"{self.label_for_charts.lower()} comparison level\"\n            )\n\n    def _add_trained_u_probability(self, val, desc=\"no description given\"):\n        self._trained_u_probabilities.append(\n            {\"probability\": val, \"description\": desc, \"m_or_u\": \"u\"}\n        )\n\n    def _add_trained_m_probability(self, val, desc=\"no description given\"):\n        self._trained_m_probabilities.append(\n            {\"probability\": val, \"description\": desc, \"m_or_u\": \"m\"}\n        )\n\n    @property\n    def _has_estimated_u_values(self):\n        if self.is_null_level:\n            return True\n        vals = [r[\"probability\"] for r in self._trained_u_probabilities]\n        vals = [v for v in vals if isinstance(v, (int, float))]\n        return len(vals) &gt; 0\n\n    @property\n    def _has_estimated_m_values(self):\n        if self.is_null_level:\n            return True\n        vals = [r[\"probability\"] for r in self._trained_m_probabilities]\n        vals = [v for v in vals if isinstance(v, (int, float))]\n        return len(vals) &gt; 0\n\n    @property\n    def _has_estimated_values(self):\n        return self._has_estimated_m_values and self._has_estimated_u_values\n\n    @property\n    def _trained_m_median(self):\n        vals = [r[\"probability\"] for r in self._trained_m_probabilities]\n        vals = [v for v in vals if isinstance(v, (int, float))]\n        if len(vals) == 0:\n            return None\n        return median(vals)\n\n    @property\n    def _trained_u_median(self):\n        vals = [r[\"probability\"] for r in self._trained_u_probabilities]\n        vals = [v for v in vals if isinstance(v, (int, float))]\n        if len(vals) == 0:\n            return None\n        return median(vals)\n\n    @property\n    def _m_is_trained(self):\n        if self.is_null_level:\n            return True\n        if self._m_probability == \"level not observed in data\":\n            return False\n        if self._m_probability is None:\n            return False\n        return True\n\n    @property\n    def _u_is_trained(self):\n        if self.is_null_level:\n            return True\n        if self._u_probability == \"level not observed in data\":\n            return False\n        if self._u_probability is None:\n            return False\n        return True\n\n    @property\n    def _is_trained(self):\n        return self._m_is_trained and self._u_is_trained\n\n    @property\n    def _bayes_factor(self):\n        if self.is_null_level:\n            return 1.0\n        if self.m_probability is None or self.u_probability is None:\n            return None\n        elif self.u_probability == 0:\n            return math.inf\n        else:\n            return self.m_probability / self.u_probability\n\n    @property\n    def _log2_bayes_factor(self):\n        if self.is_null_level:\n            return 0.0\n        else:\n            return math.log2(self._bayes_factor)\n\n    @property\n    def _bayes_factor_description(self):\n        text = (\n            f\"If comparison level is `{self.label_for_charts.lower()}` \"\n            \"then comparison is\"\n        )\n        if self._bayes_factor == math.inf:\n            return f\"{text} certain to be a match\"\n        elif self._bayes_factor == 0.0:\n            return f\"{text} impossible to be a match\"\n        elif self._bayes_factor &gt;= 1.0:\n            return f\"{text} {self._bayes_factor:,.2f} times more likely to be a match\"\n        else:\n            mult = 1 / self._bayes_factor\n            return f\"{text}  {mult:,.2f} times less likely to be a match\"\n\n    @property\n    def label_for_charts(self):\n        return self._level_dict.get(\n            \"label_for_charts\", str(self._comparison_vector_value)\n        )\n\n    @property\n    def _label_for_charts_no_duplicates(self):\n        if self._has_comparison:\n            labels = []\n            for cl in self.comparison.comparison_levels:\n                labels.append(cl.label_for_charts)\n\n        if len(labels) == len(set(labels)):\n            return self.label_for_charts\n\n        # Make label unique\n        cvv = str(self._comparison_vector_value)\n        label = self._level_dict[\"label_for_charts\"]\n        return f\"{cvv}. {label}\"\n\n    @property\n    def _is_else_level(self):\n        if self.sql_condition.strip().upper() == \"ELSE\":\n            return True\n\n    @property\n    def _has_tf_adjustments(self):\n        col = self._level_dict.get(\"tf_adjustment_column\")\n        return col is not None\n\n    def _validate_sql(self):\n        sql = self.sql_condition\n        if self._is_else_level:\n            return True\n        dialect = self._sql_dialect\n        # TODO: really self._sql_dialect should always be set, something gets\n        # messed up during the deepcopy()ing of a Comparison\n        if dialect is None:\n            dialect = \"spark\"\n        try:\n            sqlglot.parse_one(sql, read=dialect)\n        except sqlglot.ParseError as e:\n            raise ValueError(f\"Error parsing sql_statement:\\n{sql}\") from e\n\n        return True\n\n    @property\n    def _input_columns_used_by_sql_condition(self) -&gt; list[InputColumn]:\n        # returns e.g. InputColumn(first_name), InputColumn(surname)\n\n        if self._is_else_level:\n            return []\n\n        cols = get_columns_used_from_sql(self.sql_condition, dialect=self._sql_dialect)\n        # Parsed order seems to be roughly in reverse order of apearance\n        cols = cols[::-1]\n\n        cols = [re.sub(r\"_L$|_R$\", \"\", c, flags=re.IGNORECASE) for c in cols]\n        cols = dedupe_preserving_order(cols)\n\n        input_cols = []\n        for c in cols:\n            # We could have tf adjustments for surname on a dmeta_surname column\n            # If so, we want to set the tf adjustments against the surname col,\n            # not the dmeta_surname one\n\n            input_cols.append(InputColumn(c, sql_dialect=self._sql_dialect))\n\n        return input_cols\n\n    @property\n    def _columns_to_select_for_blocking(self):\n        # e.g. l.first_name as first_name_l, r.first_name as first_name_r\n        output_cols = []\n        cols = self._input_columns_used_by_sql_condition\n\n        for c in cols:\n            output_cols.extend(c.l_r_names_as_l_r())\n            if self._tf_adjustment_input_column:\n                output_cols.extend(\n                    self._tf_adjustment_input_column.l_r_tf_names_as_l_r()\n                )\n\n        return dedupe_preserving_order(output_cols)\n\n    @property\n    def _when_then_comparison_vector_value_sql(self):\n        # e.g. when first_name_l = first_name_r then 1\n        if not hasattr(self, \"_comparison_vector_value\"):\n            raise ValueError(\n                \"Cannot get the 'when .. then ...' sql expression because \"\n                \"this comparison level does not belong to a parent Comparison. \"\n                \"The comparison_vector_value is only defined in the \"\n                \"context of a list of ComparisonLevels within a Comparison.\"\n            )\n        if self._is_else_level:\n            return f\"{self.sql_condition} {self._comparison_vector_value}\"\n        else:\n            return f\"WHEN {self.sql_condition} THEN {self._comparison_vector_value}\"\n\n    @property\n    def _is_exact_match(self):\n        if self._is_else_level:\n            return False\n\n        sql_syntax_tree = sqlglot.parse_one(\n            self.sql_condition.lower(), read=self._sql_dialect\n        )\n        sql_cnf = normalize(sql_syntax_tree)\n\n        exprs = _get_and_subclauses(sql_cnf)\n        for expr in exprs:\n            if not _is_exact_match(expr):\n                return False\n        return True\n\n    @property\n    def _exact_match_colnames(self):\n        sql_syntax_tree = sqlglot.parse_one(\n            self.sql_condition.lower(), read=self._sql_dialect\n        )\n        sql_cnf = normalize(sql_syntax_tree)\n\n        exprs = _get_and_subclauses(sql_cnf)\n        for expr in exprs:\n            if not _is_exact_match(expr):\n                raise ValueError(\n                    \"sql_cond not an exact match so can't get exact match column name\"\n                )\n\n        cols = []\n        for expr in exprs:\n            col = _exact_match_colname(expr)\n            cols.append(col)\n        return cols\n\n    @property\n    def _u_probability_corresponding_to_exact_match(self):\n        levels = self.comparison.comparison_levels\n\n        # Find a level with a single exact match colname\n        # which is equal to the tf adjustment input colname\n\n        for level in levels:\n            if not level._is_exact_match:\n                continue\n            colnames = level._exact_match_colnames\n            if len(colnames) != 1:\n                continue\n            if colnames[0] == self._tf_adjustment_input_column_name.lower():\n                return level.u_probability\n        raise ValueError(\n            \"Could not find an exact match level for \"\n            f\"{self._tf_adjustment_input_column_name}.\"\n            \"\\nAn exact match level is required to make a term frequency adjustment \"\n            \"on a comparison level that is not an exact match.\"\n        )\n\n    @property\n    def _bayes_factor_sql(self):\n        bayes_factor = (\n            self._bayes_factor if self._bayes_factor != math.inf else \"'Infinity'\"\n        )\n        sql = f\"\"\"\n        WHEN\n{self.comparison._gamma_column_name} = {self._comparison_vector_value}\n        THEN cast({bayes_factor} as double)\n        \"\"\"\n        return dedent(sql)\n\n    @property\n    def _tf_adjustment_sql(self):\n        gamma_column_name = self.comparison._gamma_column_name\n        gamma_colname_value_is_this_level = (\n            f\"{gamma_column_name} = {self._comparison_vector_value}\"\n        )\n\n        # A tf adjustment of 1D is a multiplier of 1.0, i.e. no adjustment\n        if self._comparison_vector_value == -1:\n            sql = f\"WHEN  {gamma_colname_value_is_this_level} then cast(1 as double)\"\n        elif not self._has_tf_adjustments:\n            sql = f\"WHEN  {gamma_colname_value_is_this_level} then cast(1 as double)\"\n        elif self._tf_adjustment_weight == 0:\n            sql = f\"WHEN  {gamma_colname_value_is_this_level} then cast(1 as double)\"\n        elif self._is_else_level:\n            sql = f\"WHEN  {gamma_colname_value_is_this_level} then cast(1 as double)\"\n        else:\n            tf_adj_col = self._tf_adjustment_input_column\n\n            coalesce_l_r = (\n                f\"coalesce({tf_adj_col.tf_name_l()}, {tf_adj_col.tf_name_r()})\"\n            )\n            coalesce_r_l = (\n                f\"coalesce({tf_adj_col.tf_name_r()}, {tf_adj_col.tf_name_l()})\"\n            )\n\n            tf_adjustment_exists = f\"{coalesce_l_r} is not null\"\n            u_prob_exact_match = self._u_probability_corresponding_to_exact_match\n\n            # Using coalesce protects against one of the tf adjustments being null\n            # Which would happen if the user provided their own tf adjustment table\n            # That didn't contain some of the values in this data\n\n            # In this case rather than taking the greater of the two, we take\n            # whichever value exists\n\n            if self._tf_minimum_u_value == 0.0:\n                divisor_sql = f\"\"\"\n                (CASE\n                    WHEN {coalesce_l_r} &gt;= {coalesce_r_l}\n                    THEN {coalesce_l_r}\n                    ELSE {coalesce_r_l}\n                END)\n                \"\"\"\n            else:\n                # This sql works correctly even when the tf_minimum_u_value is 0.0\n                # but is less efficient to execute, hence the above if statement\n                divisor_sql = f\"\"\"\n                (CASE\n                    WHEN {coalesce_l_r} &gt;= {coalesce_r_l}\n                    AND {coalesce_l_r} &gt; cast({self._tf_minimum_u_value} as double)\n                        THEN {coalesce_l_r}\n                    WHEN {coalesce_r_l}  &gt; cast({self._tf_minimum_u_value} as double)\n                        THEN {coalesce_r_l}\n                    ELSE cast({self._tf_minimum_u_value} as double)\n                END)\n                \"\"\"\n\n            sql = f\"\"\"\n            WHEN  {gamma_colname_value_is_this_level} then\n                (CASE WHEN {tf_adjustment_exists}\n                THEN\n                POW(\n                    cast({u_prob_exact_match} as double) /{divisor_sql},\n                    cast({self._tf_adjustment_weight} as double)\n                )\n                ELSE cast(1 as double)\n                END)\n            \"\"\"\n        return dedent(sql).strip()\n\n    def as_dict(self):\n        \"The minimal representation of this level to use as an input to Splink\"\n        output = {}\n\n        output[\"sql_condition\"] = self.sql_condition\n\n        if self._level_dict.get(\"label_for_charts\"):\n            output[\"label_for_charts\"] = self.label_for_charts\n\n        if self._m_probability and self._m_is_trained:\n            output[\"m_probability\"] = self.m_probability\n\n        if self._u_probability and self._u_is_trained:\n            output[\"u_probability\"] = self.u_probability\n\n        if self._has_tf_adjustments:\n            output[\"tf_adjustment_column\"] = self._tf_adjustment_input_column.input_name\n            if self._tf_adjustment_weight != 0:\n                output[\"tf_adjustment_weight\"] = self._tf_adjustment_weight\n\n        if self.is_null_level:\n            output[\"is_null_level\"] = True\n\n        return output\n\n    def _as_completed_dict(self):\n        comp_dict = self.as_dict()\n        comp_dict[\"comparison_vector_value\"] = self._comparison_vector_value\n        return comp_dict\n\n    @property\n    def _as_detailed_record(self):\n        \"A detailed representation of this level to describe it in charting outputs\"\n        output = {}\n        output[\"sql_condition\"] = self.sql_condition\n        output[\"label_for_charts\"] = self._label_for_charts_no_duplicates\n\n        output[\"m_probability\"] = self.m_probability\n        output[\"u_probability\"] = self.u_probability\n\n        output[\"m_probability_description\"] = self._m_probability_description\n        output[\"u_probability_description\"] = self._u_probability_description\n\n        output[\"has_tf_adjustments\"] = self._has_tf_adjustments\n        if self._has_tf_adjustments:\n            output[\"tf_adjustment_column\"] = self._tf_adjustment_input_column.input_name\n        else:\n            output[\"tf_adjustment_column\"] = None\n        output[\"tf_adjustment_weight\"] = self._tf_adjustment_weight\n\n        output[\"is_null_level\"] = self.is_null_level\n        output[\"bayes_factor\"] = self._bayes_factor\n        output[\"log2_bayes_factor\"] = self._log2_bayes_factor\n        output[\"comparison_vector_value\"] = self._comparison_vector_value\n        output[\"max_comparison_vector_value\"] = self.comparison._num_levels - 1\n        output[\"bayes_factor_description\"] = self._bayes_factor_description\n\n        return output\n\n    @property\n    def _parameter_estimates_as_records(self):\n        output_records = []\n\n        cl_record = self._as_detailed_record\n        trained_values = self._trained_u_probabilities + self._trained_m_probabilities\n        for trained_value in trained_values:\n            record = {}\n            record[\"m_or_u\"] = trained_value[\"m_or_u\"]\n            p = trained_value[\"probability\"]\n            record[\"estimated_probability\"] = p\n            record[\"estimate_description\"] = trained_value[\"description\"]\n            if p is not None and p != LEVEL_NOT_OBSERVED_TEXT and p &gt; 0.0 and p &lt; 1.0:\n                record[\"estimated_probability_as_log_odds\"] = math.log2(p / (1 - p))\n            else:\n                record[\"estimated_probability_as_log_odds\"] = None\n\n            record[\"sql_condition\"] = cl_record[\"sql_condition\"]\n            record[\"comparison_level_label\"] = cl_record[\"label_for_charts\"]\n            record[\"comparison_vector_value\"] = cl_record[\"comparison_vector_value\"]\n            output_records.append(record)\n\n        return output_records\n\n    def _validate(self):\n        self._validate_sql()\n\n    def _abbreviated_sql(self, cutoff=75):\n        sql = self.sql_condition\n        return (sql[:75] + \"...\") if len(sql) &gt; 75 else sql\n\n    def __repr__(self):\n        return f\"&lt;{self._human_readable_succinct}&gt;\"\n\n    @property\n    def _human_readable_succinct(self):\n        sql = self._abbreviated_sql(75)\n        return f\"Comparison level '{self.label_for_charts}' using SQL rule: {sql}\"\n\n    @property\n    def human_readable_description(self):\n        input_cols = join_list_with_commas_final_and(\n            [c.name() for c in self._input_columns_used_by_sql_condition]\n        )\n        desc = (\n            f\"Comparison level: {self.label_for_charts} of {input_cols}\\n\"\n            \"Assesses similarity between pairwise comparisons of the input columns \"\n            f\"using the following rule\\n{self.sql_condition}\"\n        )\n\n        return desc\n</code></pre>","tags":["API","comparisons"]},{"location":"comparison_level.html#splink.comparison_level.ComparisonLevel.m_probability","title":"<code>m_probability</code>  <code>writable</code> <code>property</code>","text":"","tags":["API","comparisons"]},{"location":"comparison_level.html#splink.comparison_level.ComparisonLevel.u_probability","title":"<code>u_probability</code>  <code>writable</code> <code>property</code>","text":"","tags":["API","comparisons"]},{"location":"comparison_level.html#splink.comparison_level.ComparisonLevel.__init__","title":"<code>__init__(level_dict, comparison=None, sql_dialect=None)</code>","text":"Source code in <code>splink/comparison_level.py</code> <pre><code>def __init__(\n    self,\n    level_dict,\n    comparison: Comparison = None,\n    sql_dialect: str = None,\n):\n    # Protected, because we don't want to modify the original dict\n    self._level_dict = level_dict\n\n    self.comparison: Comparison = comparison\n    if not hasattr(self, \"_sql_dialect\"):\n        self._sql_dialect = sql_dialect\n\n    self._sql_condition = self._level_dict[\"sql_condition\"]\n    self._is_null_level = self._level_dict_val_else_default(\"is_null_level\")\n    self._tf_adjustment_weight = self._level_dict_val_else_default(\n        \"tf_adjustment_weight\"\n    )\n\n    self._tf_minimum_u_value = self._level_dict_val_else_default(\n        \"tf_minimum_u_value\"\n    )\n\n    # Private values controlled with getter/setter\n    self._m_probability = self._level_dict.get(\"m_probability\")\n    self._u_probability = self._level_dict.get(\"u_probability\")\n\n    # These will be set when the ComparisonLevel is passed into a Comparison\n    self._comparison_vector_value: int = None\n    self._max_level: bool = None\n\n    # Enable the level to 'know' when it's been trained\n    self._trained_m_probabilities: list = []\n    self._trained_u_probabilities: list = []\n\n    self._validate()\n</code></pre>","tags":["API","comparisons"]},{"location":"comparison_level_composition.html","title":"Documentation for <code>comparison_level_composition</code> functions","text":"<p><code>comparison_composition</code> allows the merging of existing comparison levels by a logical SQL clause - <code>OR</code>, <code>AND</code> or <code>NOT</code>.</p> <p>This extends the functionality of our base comparison levels by allowing users to \"join\" existing comparisons by various SQL clauses.</p> <p>For example, <code>or_(null_level(\"first_name\"), null_level(\"surname\"))</code> creates a check for nulls in either <code>first_name</code> or <code>surname</code>, rather than restricting the user to a single column.</p> <p>The detailed API for each of these are outlined below.</p>","tags":["API","comparisons"]},{"location":"comparison_level_composition.html#library-comparison-composition-apis","title":"Library comparison composition APIs","text":"","tags":["API","comparisons"]},{"location":"comparison_level_composition.html#splink.comparison_level_composition.and_","title":"<code>and_(*clls, label_for_charts=None, m_probability=None, is_null_level=None)</code>","text":"<p>Merge ComparisonLevels using logical \"AND\".</p> <p>Merge multiple ComparisonLevels into a single ComparisonLevel by merging their SQL conditions using a logical \"AND\".</p> <p>By default, we generate a new <code>label_for_charts</code> for the new ComparisonLevel. You can override this, and any other ComparisonLevel attributes, by passing them as keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>*clls</code> <code>ComparisonLevel | dict</code> <p>ComparisonLevels or comparison level dictionaries to merge</p> <code>()</code> <code>label_for_charts</code> <code>str</code> <p>A label for this comparson level, which will appear on charts as a reminder of what the level represents. Defaults to a composition of - <code>label_1 AND label_2</code></p> <code>None</code> <code>m_probability</code> <code>float</code> <p>Starting value for m probability. Defaults to None.</p> <code>None</code> <code>is_null_level</code> <code>bool</code> <p>If true, m and u values will not be estimated and instead the match weight will be zero for this column. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSparkAthenaSQLite <p>Simple null level composition with an <code>AND</code> clause <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.and_(cll.null_level(\"first_name\"), cll.null_level(\"surname\"))\n</code></pre> Composing a levenshtein level with a custom <code>contains</code> level <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\nmisspelling = cll.levenshtein_level(\"name\", 1)\ncontains = {\n    \"sql_condition\": \"(contains(name_l, name_r) OR \"                 \"contains(name_r, name_l))\"\n}\nmerged = cll.and_(misspelling, contains, label_for_charts=\"Spelling error\")\n</code></pre> <pre><code>merged.as_dict()\n</code></pre></p> <p>{ 'sql_condition': '(levenshtein(\"name_l\", \"name_r\") &lt;= 1) '             &gt;  'AND ((contains(name_l, name_r) OR contains(name_r, name_l)))',  'label_for_charts': 'Spelling error' }</p> <p>Simple null level composition with an <code>AND</code> clause <pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.and_(cll.null_level(\"first_name\"), cll.null_level(\"surname\"))\n</code></pre> Composing a levenshtein level with a custom <code>contains</code> level <pre><code>import splink.spark.spark_comparison_level_library as cll\nmisspelling = cll.levenshtein_level(\"name\", 1)\ncontains = {\n    \"sql_condition\": \"(contains(name_l, name_r) OR \"                 \"contains(name_r, name_l))\"\n}\nmerged = cll.and_(misspelling, contains, label_for_charts=\"Spelling error\")\n</code></pre> <pre><code>merged.as_dict()\n</code></pre></p> <p>{ 'sql_condition': '(levenshtein(\"name_l\", \"name_r\") &lt;= 1) '             &gt;  'AND ((contains(name_l, name_r) OR contains(name_r, name_l)))',  'label_for_charts': 'Spelling error' }</p> <p>Simple null level composition with an <code>AND</code> clause <pre><code>import splink.athena.athena_comparison_level_library as cll\ncll.and_(cll.null_level(\"first_name\"), cll.null_level(\"surname\"))\n</code></pre> Composing a levenshtein level with a custom <code>contains</code> level <pre><code>import splink.athena.athena_comparison_level_library as cll\nmisspelling = cll.levenshtein_level(\"name\", 1)\ncontains = {\n    \"sql_condition\": \"(contains(name_l, name_r) OR \"                 \"contains(name_r, name_l))\"\n}\nmerged = cll.and_(misspelling, contains, label_for_charts=\"Spelling error\")\n</code></pre> <pre><code>merged.as_dict()\n</code></pre></p> <p>{ 'sql_condition': '(levenshtein(\"name_l\", \"name_r\") &lt;= 1) '             &gt;  'AND ((contains(name_l, name_r) OR contains(name_r, name_l)))',  'label_for_charts': 'Spelling error' }</p> <p>Simple null level composition with an <code>AND</code> clause <pre><code>import splink.sqlite.sqlite_comparison_level_library as cll\ncll.and_(cll.null_level(\"first_name\"), cll.null_level(\"surname\"))\n</code></pre></p> <p>Returns:</p> Name Type Description <code>ComparisonLevel</code> <code>ComparisonLevel</code> <p>A new ComparisonLevel with the merged SQL condition</p>","tags":["API","comparisons"]},{"location":"comparison_level_composition.html#splink.comparison_level_composition.or_","title":"<code>or_(*clls, label_for_charts=None, m_probability=None, is_null_level=None)</code>","text":"<p>Merge ComparisonLevels using logical \"OR\".</p> <p>Merge multiple ComparisonLevels into a single ComparisonLevel by merging their SQL conditions using a logical \"OR\".</p> <p>By default, we generate a new <code>label_for_charts</code> for the new ComparisonLevel. You can override this, and any other ComparisonLevel attributes, by passing them as keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>*clls</code> <code>ComparisonLevel | dict</code> <p>ComparisonLevels or comparison level dictionaries to merge</p> <code>()</code> <code>label_for_charts</code> <code>str</code> <p>A label for this comparson level, which will appear on charts as a reminder of what the level represents. Defaults to a composition of - <code>label_1 OR label_2</code></p> <code>None</code> <code>m_probability</code> <code>float</code> <p>Starting value for m probability. Defaults to None.</p> <code>None</code> <code>is_null_level</code> <code>bool</code> <p>If true, m and u values will not be estimated and instead the match weight will be zero for this column. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSparkAthenaSQLite <p>Simple null level composition with an <code>OR</code> clause <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.or_(cll.null_level(\"first_name\"), cll.null_level(\"surname\"))\n</code></pre> Composing a levenshtein level with a custom <code>contains</code> level <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\nmisspelling = cll.levenshtein_level(\"name\", 1)\ncontains = {\n    \"sql_condition\": \"(contains(name_l, name_r) OR \"                 \"contains(name_r, name_l))\"\n}\nmerged = cll.or_(misspelling, contains, label_for_charts=\"Spelling error\")\n</code></pre> <pre><code>merged.as_dict()\n</code></pre></p> <p>{ sql_condition': '(levenshtein(\"name_l\", \"name_r\") &lt;= 1) '             &gt;  'OR ((contains(name_l, name_r) OR contains(name_r, name_l)))',  'label_for_charts': 'Spelling error' }</p> <p>Simple null level composition with an <code>OR</code> clause <pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.or_(cll.null_level(\"first_name\"), cll.null_level(\"surname\"))\n</code></pre> Composing a levenshtein level with a custom <code>contains</code> level <pre><code>import splink.spark.spark_comparison_level_library as cll\nmisspelling = cll.levenshtein_level(\"name\", 1)\ncontains = {\n    \"sql_condition\": \"(contains(name_l, name_r) OR \"                 \"contains(name_r, name_l))\"\n}\nmerged = cll.or_(misspelling, contains, label_for_charts=\"Spelling error\")\n</code></pre> <pre><code>merged.as_dict()\n</code></pre></p> <p>{ sql_condition': '(levenshtein(\"name_l\", \"name_r\") &lt;= 1) '             &gt;  'OR ((contains(name_l, name_r) OR contains(name_r, name_l)))',  'label_for_charts': 'Spelling error' }</p> <p>Simple null level composition with an <code>OR</code> clause <pre><code>import splink.athena.athena_comparison_level_library as cll\ncll.or_(cll.null_level(\"first_name\"), cll.null_level(\"surname\"))\n</code></pre> Composing a levenshtein level with a custom <code>contains</code> level <pre><code>import splink.athena.athena_comparison_level_library as cll\nmisspelling = cll.levenshtein_level(\"name\", 1)\ncontains = {\n    \"sql_condition\": \"(contains(name_l, name_r) OR \"                 \"contains(name_r, name_l))\"\n}\nmerged = cll.or_(misspelling, contains, label_for_charts=\"Spelling error\")\n</code></pre> <pre><code>merged.as_dict()\n</code></pre></p> <p>{ sql_condition': '(levenshtein(\"name_l\", \"name_r\") &lt;= 1) '             &gt;  'OR ((contains(name_l, name_r) OR contains(name_r, name_l)))',  'label_for_charts': 'Spelling error' }</p> <p>Simple null level composition with an <code>OR</code> clause <pre><code>import splink.sqlite.sqlite_comparison_level_library as cll\ncll.or_(cll.null_level(\"first_name\"), cll.null_level(\"surname\"))\n</code></pre></p> <p>Returns:</p> Name Type Description <code>ComparisonLevel</code> <code>ComparisonLevel</code> <p>A new ComparisonLevel with the merged SQL condition</p>","tags":["API","comparisons"]},{"location":"comparison_level_composition.html#splink.comparison_level_composition.not_","title":"<code>not_(cll, label_for_charts=None, m_probability=None)</code>","text":"<p>Negate a ComparisonLevel.</p> <p>Returns a ComparisonLevel with the same SQL condition as the input, but prefixed with \"NOT\".</p> <p>By default, we generate a new <code>label_for_charts</code> for the new ComparisonLevel. You can override this, and any other ComparisonLevel attributes, by passing them as keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>cll</code> <code>ComparisonLevel | dict</code> <p>ComparisonLevel or comparison level dictionary</p> required <code>label_for_charts</code> <code>str</code> <p>A label for this comparson level, which will appear on charts as a reminder of what the level represents.</p> <code>None</code> <code>m_probability</code> <code>float</code> <p>Starting value for m probability. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSparkAthenaSQLite <p>Not an exact match on first name <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.not_(cll.exact_match(\"first_name\"))\n</code></pre> Find all exact matches not on the first of January <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ndob_first_jan =  {\n   \"sql_condition\": \"SUBSTR(dob_std_l, -5) = '01-01'\",\n   \"label_for_charts\": \"Date is 1st Jan\",\n}\nexact_match_not_first_jan = cll.and_(\n    cll.exact_match_level(\"dob\"),\n    cll.not_(dob_first_jan),\n    label_for_charts = \"Exact match and not the 1st Jan\"\n)\n</code></pre></p> <p>Not an exact match on first name <pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.not_(cll.exact_match(\"first_name\"))\n</code></pre> Find all exact matches not on the first of January <pre><code>import splink.spark.spark_comparison_level_library as cll\ndob_first_jan =  {\n   \"sql_condition\": \"SUBSTR(dob_std_l, -5) = '01-01'\",\n   \"label_for_charts\": \"Date is 1st Jan\",\n}\nexact_match_not_first_jan = cll.and_(\n    cll.exact_match_level(\"dob\"),\n    cll.not_(dob_first_jan),\n    label_for_charts = \"Exact match and not the 1st Jan\"\n)\n</code></pre></p> <p>Not an exact match on first name <pre><code>import splink.athena.athena_comparison_level_library as cll\ncll.not_(cll.exact_match(\"first_name\"))\n</code></pre> Find all exact matches not on the first of January <pre><code>import splink.athena.athena_comparison_level_library as cll\ndob_first_jan =  {\n   \"sql_condition\": \"SUBSTR(dob_std_l, -5) = '01-01'\",\n   \"label_for_charts\": \"Date is 1st Jan\",\n}\nexact_match_not_first_jan = cll.and_(\n    cll.exact_match_level(\"dob\"),\n    cll.not_(dob_first_jan),\n    label_for_charts = \"Exact match and not the 1st Jan\"\n)\n</code></pre></p> <p>Not an exact match on first name <pre><code>import splink.sqlite.sqlite_comparison_level_library as cll\ncll.not_(cll.exact_match(\"first_name\"))\n</code></pre> Find all exact matches not on the first of January <pre><code>import splink.sqlite.sqlite_comparison_level_library as cll\ndob_first_jan =  {\n   \"sql_condition\": \"SUBSTR(dob_std_l, -5) = '01-01'\",\n   \"label_for_charts\": \"Date is 1st Jan\",\n}\nexact_match_not_first_jan = cll.and_(\n    cll.exact_match_level(\"dob\"),\n    cll.not_(dob_first_jan),\n    label_for_charts = \"Exact match and not the 1st Jan\"\n)\n</code></pre></p> <p>Returns:</p> Type Description <code>ComparisonLevel</code> <p>ComparisonLevel A new ComparisonLevel with the negated SQL condition and label_for_charts</p>","tags":["API","comparisons"]},{"location":"comparison_level_library.html","title":"Documentation for <code>comparison_level_library</code>","text":"<p>The <code>comparison_level_library</code> contains pre-made comparison levels available for use to construct custom comparisons as described in this topic guide. However, not every comparison level is available for every Splink-compatible SQL backend.</p> <p>The pre-made Splink comparison levels available for each SQL dialect are as given in this table:</p> duckdb spark athena sqlite <code>array_intersect_level</code> \u2713 \u2713 \u2713 <code>columns_reversed_level</code> \u2713 \u2713 \u2713 \u2713 <code>damerau_levenshtein_level</code> \u2713 \u2713 <code>datediff_level</code> \u2713 \u2713 <code>distance_function_level</code> \u2713 \u2713 \u2713 \u2713 <code>distance_in_km_level</code> \u2713 \u2713 \u2713 <code>else_level</code> \u2713 \u2713 \u2713 \u2713 <code>exact_match_level</code> \u2713 \u2713 \u2713 \u2713 <code>jaccard_level</code> \u2713 \u2713 <code>jaro_level</code> \u2713 \u2713 <code>jaro_winkler_level</code> \u2713 \u2713 <code>levenshtein_level</code> \u2713 \u2713 \u2713 \u2713 <code>null_level</code> \u2713 \u2713 \u2713 \u2713 <code>percentage_difference_level</code> \u2713 \u2713 \u2713 \u2713 <p>The detailed API for each of these are outlined below.</p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#library-comparison-level-apis","title":"Library comparison level APIs","text":"","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.NullLevelBase","title":"<code>splink.comparison_level_library.NullLevelBase</code>","text":"<p>         Bases: <code>ComparisonLevel</code></p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.NullLevelBase.__init__","title":"<code>__init__(col_name, valid_string_regex=None)</code>","text":"<p>Represents comparisons level where one or both sides of the comparison contains null values so the similarity cannot be evaluated. Assumed to have a partial match weight of zero (null effect on overall match weight)</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>Input column name</p> required <code>valid_string_regex</code> <code>str</code> <p>regular expression pattern that if not matched will result in column being treated as a null.</p> <code>None</code> <p>Examples:</p> DuckDBSparkAthenaSQLite <p>Simple null comparison level <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.null_level(\"name\")\n</code></pre> Null comparison level including strings that do not match a given regex pattern <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.null_level(\"name\", valid_string_regex=\"^[A-Z]{1,7}$\")\n</code></pre></p> <p>Simple null level <pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.null_level(\"name\")\n</code></pre> Null comparison level including strings that do not match a given regex pattern <pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.null_level(\"name\", valid_string_regex=\"^[A-Z]{1,7}$\")\n</code></pre></p> <p>Simple null level <pre><code>import splink.athena.athena_comparison_level_library as cll\ncll.null_level(\"name\")\n</code></pre> Null comparison level including strings that do not match a given regex pattern <pre><code>import splink.athena.athena_comparison_level_library as cll\ncll.null_level(\"name\", valid_string_regex=\"^[A-Z]{1,7}$\")\n</code></pre></p> <p>Simple null level <pre><code>import splink.sqlite.sqlite_comparison_level_library as cll\ncll.null_level(\"name\")\n</code></pre></p> <p>Returns:</p> Name Type Description <code>ComparisonLevel</code> <code>ComparisonLevel</code> <p>Comparison level for null entries</p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.ExactMatchLevelBase","title":"<code>splink.comparison_level_library.ExactMatchLevelBase</code>","text":"<p>         Bases: <code>ComparisonLevel</code></p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.ExactMatchLevelBase.__init__","title":"<code>__init__(col_name, regex_extract=None, set_to_lowercase=False, m_probability=None, term_frequency_adjustments=False, include_colname_in_charts_label=False, manual_chart_label=None)</code>","text":"<p>Represents a comparison level where there is an exact match,</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>Input column name</p> required <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>set_to_lowercase</code> <code>bool</code> <p>If True, sets all entries to lowercase.</p> <code>False</code> <code>m_probability</code> <code>float</code> <p>Starting value for m probability Defaults to None.</p> <code>None</code> <code>term_frequency_adjustments</code> <code>bool</code> <p>If True, apply term frequency adjustments to the exact match level. Defaults to False.</p> <code>False</code> <code>include_colname_in_charts_label</code> <code>bool</code> <p>If True, include col_name in chart labels (e.g. linker.match_weights_chart())</p> <code>False</code> <code>chart_label</code> <code>str</code> <p>string to include in chart label. Setting to col_name would recreate behaviour of include_colname_in_charts_label=True</p> required <p>Examples:</p> DuckDBSparkAthenaSQLite <p>Simple Exact match level <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.exact_match_level(\"name\")\n</code></pre> Exact match level with term-frequency adjustments <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.exact_match_level(\"name\", term_frequency_adjustments=True)\n</code></pre> Exact match level on a substring of col_name as  determined by a regular expression <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.exact_match_level(\"name\", regex_extract=\"^[A-Z]{1,4}\")\n</code></pre></p> <p>Simple Exact match level <pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.exact_match_level(\"name\")\n</code></pre> Exact match level with term-frequency adjustments <pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.exact_match_level(\"name\", term_frequency_adjustments=True)\n</code></pre> Exact match level on a substring of col_name as  determined by a regular expression <pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.exact_match_level(\"name\", regex_extract=\"^[A-Z]{1,4}\")\n</code></pre></p> <p>Simple Exact match level <pre><code>import splink.athena.athena_comparison_level_library as cll\ncll.exact_match_level(\"name\")\n</code></pre> Exact match level with term-frequency adjustments <pre><code>import splink.athena.athena_comparison_level_library as cll\ncll.exact_match_level(\"name\", term_frequency_adjustments=True)\n</code></pre> Exact match level on a substring of col_name as  determined by a regular expression <pre><code>import splink.athena.athena_comparison_level_library as cll\ncll.exact_match_level(\"name\", regex_extract=\"^[A-Z]{1,4}\")\n</code></pre></p> <p>Simple Exact match level <pre><code>import splink.sqlite.sqlite_comparison_level_library as cll\ncll.exact_match_level(\"name\")\n</code></pre> Exact match level with term-frequency adjustments <pre><code>import splink.sqlite.sqlite_comparison_level_library as cll\ncll.exact_match_level(\"name\", term_frequency_adjustments=True)\n</code></pre></p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.ElseLevelBase","title":"<code>splink.comparison_level_library.ElseLevelBase</code>","text":"<p>         Bases: <code>ComparisonLevel</code></p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.ElseLevelBase.__init__","title":"<code>__init__(m_probability=None)</code>","text":"<p>Represents a comparison level for all cases which have not been considered by preceding comparison levels,</p> <p>Examples:</p> DuckDBSparkAthenaSQLite <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.else_level(\"name\")\n</code></pre> <pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.else_level(\"name\")\n</code></pre> <pre><code>import splink.athena.athena_comparison_level_library as cll\ncll.else_level(\"name\")\n</code></pre> <pre><code>import splink.sqlite.sqlite_comparison_level_library as cll\ncll.else_level(\"name\")\n</code></pre>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.DistanceFunctionLevelBase","title":"<code>splink.comparison_level_library.DistanceFunctionLevelBase</code>","text":"<p>         Bases: <code>ComparisonLevel</code></p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.DistanceFunctionLevelBase.__init__","title":"<code>__init__(col_name, distance_function_name, distance_threshold, regex_extract=None, set_to_lowercase=False, higher_is_more_similar=True, include_colname_in_charts_label=False, m_probability=None)</code>","text":"<p>Represents a comparison level using a user-provided distance function, where the similarity</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>Input column name</p> required <code>distance_function_name</code> <code>str</code> <p>The name of the distance function</p> required <code>distance_threshold</code> <code>Union[int, float]</code> <p>The threshold to use to assess similarity</p> required <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>set_to_lowercase</code> <code>bool</code> <p>If True, sets all entries to lowercase.</p> <code>False</code> <code>higher_is_more_similar</code> <code>bool</code> <p>If True, a higher value of the distance function indicates a higher similarity (e.g. jaro_winkler). If false, a higher value indicates a lower similarity (e.g. levenshtein).</p> <code>True</code> <code>include_colname_in_charts_label</code> <code>bool</code> <p>If True, includes col_name in charts label</p> <code>False</code> <code>m_probability</code> <code>float</code> <p>Starting value for m probability Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSparkAthena <p>Apply the <code>levenshtein</code> function to a comparison level <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.distance_function_level(\"name\",\n                            \"levenshtein\",\n                            2,\n                            False)\n</code></pre></p> <p>Apply the <code>levenshtein</code> function to a comparison level <pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.distance_function_level(\"name\",\n                            \"levenshtein\",\n                            2,\n                            False)\n</code></pre></p> <p>Apply the <code>levenshtein_distance</code> function to a comparison level <pre><code>import splink.athena.athena_comparison_level_library as cll\ncll.distance_function_level(\"name\",\n                            \"levenshtein_distance\",\n                            2,\n                            False)\n</code></pre></p> <p>Returns:</p> Name Type Description <code>ComparisonLevel</code> <code>ComparisonLevel</code> <p>A comparison level for a given distance function</p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.LevenshteinLevelBase","title":"<code>splink.comparison_level_library.LevenshteinLevelBase</code>","text":"<p>         Bases: <code>DistanceFunctionLevelBase</code></p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.LevenshteinLevelBase.__init__","title":"<code>__init__(col_name, distance_threshold, regex_extract=None, set_to_lowercase=False, include_colname_in_charts_label=False, m_probability=None)</code>","text":"<p>Represents a comparison level using a levenshtein distance function,</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>Input column name</p> required <code>distance_threshold</code> <code>Union[int, float]</code> <p>The threshold to use to assess similarity</p> required <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>set_to_lowercase</code> <code>bool</code> <p>If True, sets all entries to lowercase.</p> <code>False</code> <code>include_colname_in_charts_label</code> <code>bool</code> <p>If True, includes col_name in charts label</p> <code>False</code> <code>m_probability</code> <code>float</code> <p>Starting value for m probability. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSparkAthena <p>Comparison level with levenshtein distance score less than (or equal  to) 1 <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.levenshtein_level(\"name\", 1)\n</code></pre></p> <p>Comparison level with levenshtein distance score less than (or equal  to) 1 on a subtring of name column as determined by a regular expression. <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.levenshtein_level(\"name\", 1, regex_extract=\"^[A-Z]{1,4}\")\n</code></pre></p> <p>Comparison level with levenshtein distance score less than (or equal  to) 1 <pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.levenshtein_level(\"name\", 1)\n</code></pre></p> <p>Comparison level with levenshtein distance score less than (or equal  to) 1 on a subtring of name column as determined by a regular expression. <pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.levenshtein_level(\"name\", 1, regex_extract=\"^[A-Z]{1,4}\")\n</code></pre></p> <p>Comparison level with levenshtein distance score less than (or equal  to) 1 <pre><code>import splink.athena.athena_comparison_level_library as cll\ncll.levenshtein_level(\"name\", 1)\n</code></pre></p> <p>Comparison level with levenshtein distance score less than (or equal  to) 1 on a subtring of name column as determined by a regular expression. <pre><code>import splink.athena.athena_comparison_level_library as cll\ncll.levenshtein_level(\"name\", 1, regex_extract=\"^[A-Z]{1,4}\")\n</code></pre></p> <p>Returns:</p> Name Type Description <code>ComparisonLevel</code> <code>ComparisonLevel</code> <p>A comparison level that evaluates the levenshtein similarity</p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.DamerauLevenshteinLevelBase","title":"<code>splink.comparison_level_library.DamerauLevenshteinLevelBase</code>","text":"<p>         Bases: <code>DistanceFunctionLevelBase</code></p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.DamerauLevenshteinLevelBase.__init__","title":"<code>__init__(col_name, distance_threshold, regex_extract=None, set_to_lowercase=False, include_colname_in_charts_label=False, m_probability=None)</code>","text":"<p>Represents a comparison level using a damerau-levenshtein distance function,</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>Input column name</p> required <code>distance_threshold</code> <code>Union[int, float]</code> <p>The threshold to use to assess similarity</p> required <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>set_to_lowercase</code> <code>bool</code> <p>If True, sets all entries to lowercase.</p> <code>False</code> <code>include_colname_in_charts_label</code> <code>bool</code> <p>If True, includes col_name in charts label</p> <code>False</code> <code>m_probability</code> <code>float</code> <p>Starting value for m probability. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSpark <p>Comparison level with damerau-levenshtein distance score less than (or equal to) 1 <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.damerau_levenshtein_level(\"name\", 1)\n</code></pre></p> <p>Comparison level with damerau-levenshtein distance score less than (or equal to) 1 on a subtring of name column as determined by a regular expression. <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.damerau_levenshtein_level(\"name\", 1, regex_extract=\"^[A-Z]{1,4}\")\n</code></pre></p> <p>Comparison level with damerau-levenshtein distance score less than (or equal to) 1 <pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.damerau_levenshtein_level(\"name\", 1)\n</code></pre></p> <p>Comparison level with damerau-levenshtein distance score less than (or equal to) 1 on a subtring of name column as determined by a regular expression. <pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.damerau_levenshtein_level(\"name\", 1, regex_extract=\"^[A-Z]{1,4}\")\n</code></pre></p> <p>Returns:</p> Name Type Description <code>ComparisonLevel</code> <code>ComparisonLevel</code> <p>A comparison level that evaluates the Damerau-Levenshtein similarity</p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.JaroLevelBase","title":"<code>splink.comparison_level_library.JaroLevelBase</code>","text":"<p>         Bases: <code>DistanceFunctionLevelBase</code></p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.JaroLevelBase.__init__","title":"<code>__init__(col_name, distance_threshold, regex_extract=None, set_to_lowercase=False, include_colname_in_charts_label=False, m_probability=None)</code>","text":"<p>Represents a comparison using the jaro distance function</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>Input column name</p> required <code>distance_threshold</code> <code>Union[int, float]</code> <p>The threshold to use to assess similarity</p> required <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>set_to_lowercase</code> <code>bool</code> <p>If True, sets all entries to lowercase.</p> <code>False</code> <code>include_colname_in_charts_label</code> <code>bool</code> <p>If True, includes col_name in charts label</p> <code>False</code> <code>m_probability</code> <code>float</code> <p>Starting value for m probability. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSpark <p>Comparison level with jaro score greater than 0.9 <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.jaro_level(\"name\", 0.9)\n</code></pre> Comparison level with a jaro score greater than 0.9 on a substring of name column as determined by a regular expression.</p> <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.jaro_level(\"name\", 0.9, regex_extract=\"^[A-Z]{1,4}\")\n</code></pre> <p>Comparison level with jaro score greater than 0.9 <pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.jaro_level(\"name\", 0.9)\n</code></pre> Comparison level with a jaro score greater than 0.9 on a substring of name column as determined by a regular expression.</p> <pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.jaro_level(\"name\", 0.9, regex_extract=\"^[A-Z]{1,4}\")\n</code></pre> <p>Returns:</p> Name Type Description <code>ComparisonLevel</code> <p>A comparison level that evaluates the jaro similarity</p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.JaroWinklerLevelBase","title":"<code>splink.comparison_level_library.JaroWinklerLevelBase</code>","text":"<p>         Bases: <code>DistanceFunctionLevelBase</code></p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.JaroWinklerLevelBase.__init__","title":"<code>__init__(col_name, distance_threshold, regex_extract=None, set_to_lowercase=False, include_colname_in_charts_label=False, m_probability=None)</code>","text":"<p>Represents a comparison level using the jaro winkler distance function</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>Input column name</p> required <code>distance_threshold</code> <code>Union[int, float]</code> <p>The threshold to use to assess similarity</p> required <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>set_to_lowercase</code> <code>bool</code> <p>If True, sets all entries to lowercase.</p> <code>False</code> <code>include_colname_in_charts_label</code> <code>bool</code> <p>If True, includes col_name in charts label</p> <code>False</code> <code>m_probability</code> <code>float</code> <p>Starting value for m probability. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSpark <p>Comparison level with jaro-winkler score greater than 0.9 <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.jaro_winkler_level(\"name\", 0.9)\n</code></pre> Comparison level with jaro-winkler score greater than 0.9 on a substring of name column as determined by a regular expression. <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.jaro_winkler_level(\"name\", 0.9, regex_extract=\"^[A-Z]{1,4}\")\n</code></pre></p> <p>Comparison level with jaro score greater than 0.9 <pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.jaro_winkler_level(\"name\", 0.9)\n</code></pre> Comparison level with jaro-winkler score greater than 0.9 on a substring of name column as determined by a regular expression. <pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.jaro_winkler_level(\"name\", 0.9, regex_extract=\"^[A-Z]{1,4}\")\n</code></pre></p> <p>Returns:</p> Name Type Description <code>ComparisonLevel</code> <code>ComparisonLevel</code> <p>A comparison level that evaluates the jaro winkler similarity</p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.JaccardLevelBase","title":"<code>splink.comparison_level_library.JaccardLevelBase</code>","text":"<p>         Bases: <code>DistanceFunctionLevelBase</code></p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.JaccardLevelBase.__init__","title":"<code>__init__(col_name, distance_threshold, regex_extract=None, set_to_lowercase=False, include_colname_in_charts_label=False, m_probability=None)</code>","text":"<p>Represents a comparison level using a jaccard distance function</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>Input column name</p> required <code>distance_threshold</code> <code>Union[int, float]</code> <p>The threshold to use to assess similarity</p> required <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>set_to_lowercase</code> <code>bool</code> <p>If True, sets all entries to lowercase.</p> <code>False</code> <code>include_colname_in_charts_label</code> <code>bool</code> <p>If True, includes col_name in charts label</p> <code>False</code> <code>m_probability</code> <code>float</code> <p>Starting value for m probability. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSpark <p>Comparison level with jaccard score greater than 0.9 <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.jaccard_level(\"name\", 0.9)\n</code></pre> Comparison level with jaccard score greater than 0.9 on a substring of name column as determined by a regular expression. <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.jaccard_level(\"name\", 0.9, regex_extract=\"^[A-Z]{1,4}\")\n</code></pre></p> <p>Comparison level with jaccard score greater than 0.9 <pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.jaccard_level(\"name\", 0.9)\n</code></pre> Comparison level with jaccard score greater than 0.9 on a substring of name column as determined by a regular expression. <pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.jaccard_level(\"name\", 0.9, regex_extract=\"^[A-Z]{1,4}\")\n</code></pre></p> <p>Returns:</p> Name Type Description <code>ComparisonLevel</code> <code>ComparisonLevel</code> <p>A comparison level that evaluates the jaccard similarity</p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.ColumnsReversedLevelBase","title":"<code>splink.comparison_level_library.ColumnsReversedLevelBase</code>","text":"<p>         Bases: <code>ComparisonLevel</code></p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.ColumnsReversedLevelBase.__init__","title":"<code>__init__(col_name_1, col_name_2, regex_extract=None, set_to_lowercase=False, m_probability=None, tf_adjustment_column=None)</code>","text":"<p>Represents a comparison level where the columns are reversed.  For example, if surname is in the forename field and vice versa</p> <p>Parameters:</p> Name Type Description Default <code>col_name_1</code> <code>str</code> <p>First column, e.g. forename</p> required <code>col_name_2</code> <code>str</code> <p>Second column, e.g. surname</p> required <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>set_to_lowercase</code> <code>bool</code> <p>If True, sets all entries to lowercase.</p> <code>False</code> <code>m_probability</code> <code>float</code> <p>Starting value for m probability. Defaults to None.</p> <code>None</code> <code>tf_adjustment_column</code> <code>str</code> <p>Column to use for term frequency adjustments if an exact match is observed. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSparkAthenaSQLite <p>Comparison level on first_name and surname columns reversed</p> <p><pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.columns_reversed_level(\"first_name\", \"surname\")\n</code></pre> Comparison level on first_name and surname column reversed on a substring of each column as determined by a regular expression. <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.columns_reversed_level(\"first_name\",\n                           \"surname\",\n                           regex_extract=\"^[A-Z]{1,4}\")\n</code></pre></p> <p><pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.columns_reversed_level(\"first_name\", \"surname\")\n</code></pre> Comparison level on first_name and surname column reversed on a substring of each column as determined by a regular expression. <pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.columns_reversed_level(\"first_name\",\n                           \"surname\",\n                           regex_extract=\"^[A-Z]{1,4}\")\n</code></pre></p> <p><pre><code>import splink.athena.athena_comparison_level_library as cll\ncll.columns_reversed_level(\"first_name\", \"surname\")\n</code></pre> Comparison level on first_name and surname column reversed on a substring of each column as determined by a regular expression. <pre><code>import splink.athena.athena_comparison_level_library as cll\ncll.columns_reversed_level(\"first_name\",\n                           \"surname\",\n                           regex_extract=\"^[A-Z]{1,4}\")\n</code></pre></p> <pre><code>import splink.sqlite.sqlite_comparison_level_library as cll\ncll.columns_reversed_level(\"first_name\", \"surname\")\n</code></pre> <p>Returns:</p> Name Type Description <code>ComparisonLevel</code> <code>ComparisonLevel</code> <p>A comparison level that evaluates the exact match of two columns.</p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.DistanceInKMLevelBase","title":"<code>splink.comparison_level_library.DistanceInKMLevelBase</code>","text":"<p>         Bases: <code>ComparisonLevel</code></p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.DistanceInKMLevelBase.__init__","title":"<code>__init__(lat_col, long_col, km_threshold, not_null=False, m_probability=None)</code>","text":"<p>Use the haversine formula to transform comparisons of lat,lngs into distances measured in kilometers</p> <p>Parameters:</p> Name Type Description Default <code>lat_col</code> <code>str</code> <p>The name of a latitude column or the respective array or struct column column containing the information For example: long_lat['lat'] or long_lat[0]</p> required <code>long_col</code> <code>str</code> <p>The name of a longitudinal column or the respective array or struct column column containing the information, plus an index. For example: long_lat['long'] or long_lat[1]</p> required <code>km_threshold</code> <code>int</code> <p>The total distance in kilometers to evaluate your comparisons against</p> required <code>not_null</code> <code>bool</code> <p>If true, remove any . This is only necessary if you are not capturing nulls elsewhere in your comparison level.</p> <code>False</code> <code>m_probability</code> <code>float</code> <p>Starting value for m probability. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSparkAthena <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.distance_in_km_level(\"lat_col\",\n                        \"long_col\",\n                        km_threshold=5)\n</code></pre> <pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.distance_in_km_level(\"lat_col\",\n                        \"long_col\",\n                        km_threshold=5)\n</code></pre> <pre><code>import splink.athena.athena_comparison_level_library as cll\ncll.distance_in_km_level(\"lat_col\",\n                        \"long_col\",\n                        km_threshold=5)\n</code></pre> <p>Returns:</p> Name Type Description <code>ComparisonLevel</code> <code>ComparisonLevel</code> <p>A comparison level that evaluates the distance between two coordinates</p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.PercentageDifferenceLevelBase","title":"<code>splink.comparison_level_library.PercentageDifferenceLevelBase</code>","text":"<p>         Bases: <code>ComparisonLevel</code></p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.PercentageDifferenceLevelBase.__init__","title":"<code>__init__(col_name, percentage_distance_threshold, m_probability=None)</code>","text":"<p>Represents a comparison level based around the percentage difference between two numbers.</p> <p>Note: the percentage is calculated by dividing the absolute difference between the values by the largest value</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>Input column name</p> required <code>percentage_distance_threshold</code> <code>float</code> <p>Percentage difference threshold for the comparison level</p> required <code>m_probability</code> <code>float</code> <p>Starting value for m probability. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSparkAthenaSQLite <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.percentage_difference_level(\"value\", 0.5)\n</code></pre> <pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.percentage_difference_level(\"value\", 0.5)\n</code></pre> <pre><code>import splink.athena.athena_comparison_level_library as cll\ncll.percentage_difference_level(\"value\", 0.5)\n</code></pre> <pre><code>import splink.sqlite.sqlite_comparison_level_library as cll\ncll.percentage_difference_level(\"value\", 0.5)\n</code></pre> <p>Returns:</p> Name Type Description <code>ComparisonLevel</code> <code>ComparisonLevel</code> <p>A comparison level that evaluates the percentage difference between two values</p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.ArrayIntersectLevelBase","title":"<code>splink.comparison_level_library.ArrayIntersectLevelBase</code>","text":"<p>         Bases: <code>ComparisonLevel</code></p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.ArrayIntersectLevelBase.__init__","title":"<code>__init__(col_name, m_probability=None, term_frequency_adjustments=False, min_intersection=1, include_colname_in_charts_label=False)</code>","text":"<p>Represents a comparison level based around the size of an intersection of arrays</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>Input column name</p> required <code>m_probability</code> <code>float</code> <p>Starting value for m probability. Defaults to None.</p> <code>None</code> <code>term_frequency_adjustments</code> <code>bool</code> <p>If True, apply term frequency adjustments to the exact match level. Defaults to False.</p> <code>False</code> <code>min_intersection</code> <code>int</code> <p>The minimum cardinality of the intersection of arrays for this comparison level. Defaults to 1</p> <code>1</code> <code>include_colname_in_charts_label</code> <code>bool</code> <p>Should the charts label contain the column name? Defaults to False</p> <code>False</code> <p>Examples:</p> DuckDBSparkAthena <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.array_intersect_level(\"name\")\n</code></pre> <pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.array_intersect_level(\"name\")\n</code></pre> <pre><code>import splink.athena.athena_comparison_level_library as cll\ncll.array_intersect_level(\"name\")\n</code></pre> <p>Returns:</p> Name Type Description <code>ComparisonLevel</code> <code>ComparisonLevel</code> <p>A comparison level that evaluates the size of intersection of arrays</p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.DateDiffLevelBase","title":"<code>splink.comparison_level_library.DateDiffLevelBase</code>","text":"<p>         Bases: <code>ComparisonLevel</code></p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_level_library.html#splink.comparison_level_library.DateDiffLevelBase.__init__","title":"<code>__init__(date_col, date_threshold, date_metric='day', m_probability=None, cast_strings_to_date=False, date_format=None)</code>","text":"<p>Represents a comparison level based around the difference between dates within a column</p> <p>Parameters:</p> Name Type Description Default <code>date_col</code> <code>str</code> <p>Input column name</p> required <code>date_threshold</code> <code>int</code> <p>The total difference in time between two given dates. This is used in tandem with <code>date_metric</code> to determine . If you are using <code>year</code> as your metric, then a value of 1 would require that your dates lie within 1 year of one another.</p> required <code>date_metric</code> <code>str</code> <p>The unit of time with which to measure your <code>date_threshold</code>. Your metric should be one of <code>day</code>, <code>month</code> or <code>year</code>. Defaults to <code>day</code>.</p> <code>'day'</code> <code>m_probability</code> <code>float</code> <p>Starting value for m probability. Defaults to None.</p> <code>None</code> <code>cast_strings_to_date</code> <code>bool</code> <p>Set to true and adjust date_format param when input dates are strings to enable date-casting. Defaults to False.</p> <code>False</code> <code>date_format</code> <code>str</code> <p>Format of input dates if date-strings are given. Must be consistent across record pairs. If None (the default), downstream functions for each backend assign date_format to ISO 8601 format (yyyy-mm-dd).</p> <code>None</code> <p>Examples:</p> DuckDBSpark <p>Date Difference comparison level at threshold 1 year <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.datediff_level(\"date\",\n                    date_threshold=1,\n                    date_metric=\"year\"\n                    )\n</code></pre> Date Difference comparison with date-casting and unspecified date_format (default = %Y-%m-%d) <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.datediff_level(\"dob\",\n                    date_threshold=3,\n                    date_metric='month',\n                    cast_strings_to_date=True\n                    )\n</code></pre> Date Difference comparison with date-casting and specified date_format <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\ncll.datediff_level(\"dob\",\n                    date_threshold=3,\n                    date_metric='month',\n                    cast_strings_to_date=True,\n                    date_format='%d/%m/%Y'\n                    )\n</code></pre></p> <p>Date Difference comparison level at threshold 1 year <pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.datediff_level(\"date\",\n                    date_threshold=1,\n                    date_metric=\"year\"\n                    )\n</code></pre> Date Difference comparison with date-casting and unspecified date_format (default = %Y-%m-%d) <pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.datediff_level(\"dob\",\n                    date_threshold=3,\n                    date_metric='month',\n                    cast_strings_to_date=True\n                    )\n</code></pre> Date Difference comparison with date-casting and specified date_format <pre><code>import splink.spark.spark_comparison_level_library as cll\ncll.datediff_level(\"dob\",\n                    date_threshold=3,\n                    date_metric='month',\n                    cast_strings_to_date=True,\n                    date_format='%d/%m/%Y'\n                    )\n</code></pre></p> <p>Returns:</p> Name Type Description <code>ComparisonLevel</code> <code>ComparisonLevel</code> <p>A comparison level that evaluates whether two dates fall within a given interval.</p>","tags":["API","comparisons","Damerau-Levenshtein","Levenshtein","Jaro-Winkler","Jaccard","Date Difference","Distance In KM","Array Intersect","Columns Reversed","Percentage Difference"]},{"location":"comparison_library.html","title":"Documentation for <code>comparison_library</code>","text":"<p>The <code>comparison_library</code> contains pre-made comparisons available for use directly as described in this topic guide. However, not every comparison is available for every Splink-compatible SQL backend.</p> <p>The pre-made Splink comparisons available for each SQL dialect are as given in this table:</p> duckdb spark athena sqlite <code>array_intersect_at_sizes</code> \u2713 \u2713 \u2713 <code>damerau_levenshtein_at_thresholds</code> \u2713 \u2713 <code>datediff_at_thresholds</code> \u2713 \u2713 <code>distance_function_at_thresholds</code> \u2713 \u2713 \u2713 \u2713 <code>distance_in_km_at_thresholds</code> \u2713 \u2713 \u2713 <code>exact_match</code> \u2713 \u2713 \u2713 \u2713 <code>jaccard_at_thresholds</code> \u2713 \u2713 <code>jaro_at_thresholds</code> \u2713 \u2713 <code>jaro_winkler_at_thresholds</code> \u2713 \u2713 <code>levenshtein_at_thresholds</code> \u2713 \u2713 \u2713 \u2713 <p>The detailed API for each of these are outlined below.</p>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#library-comparison-apis","title":"Library comparison APIs","text":"","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.ExactMatchBase","title":"<code>splink.comparison_library.ExactMatchBase</code>","text":"<p>         Bases: <code>Comparison</code></p>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.ExactMatchBase.__init__","title":"<code>__init__(col_name, regex_extract=None, valid_string_regex=None, set_to_lowercase=False, term_frequency_adjustments=False, m_probability_exact_match=None, m_probability_else=None, include_colname_in_charts_label=False)</code>","text":"<p>A comparison of the data in <code>col_name</code> with two levels: - Exact match - Anything else</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column to compare</p> required <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>valid_string_regex</code> <code>str</code> <p>regular expression pattern that if not matched will result in column being treated as a null.</p> <code>None</code> <code>set_to_lowercase</code> <code>bool</code> <p>If True, sets all entries to lowercase.</p> <code>False</code> <code>term_frequency_adjustments</code> <code>bool</code> <p>If True, term frequency adjustments will be made on the exact match level. Defaults to False.</p> <code>False</code> <code>m_probability_exact_match</code> <code>float</code> <p>If provided, overrides the default m probability for the exact match level. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>float</code> <p>If provided, overrides the default m probability for the 'anything else' level. Defaults to None.</p> <code>None</code> <code>include_colname_in_charts_label</code> <p>If true, append col name to label for charts.  Defaults to False.</p> <code>False</code> <p>Examples:</p> DuckDBSparkAthenaSQLite <p>Create comparison with exact match level <pre><code>import splink.duckdb.duckdb_comparison_library as cl\ncl.exact_match(\"first_name\")\n</code></pre> Create comparison with exact match level based on a substring of first_name as determined by a regular expression <pre><code>import splink.duckdb.duckdb_comparison_library as cl\ncl.exact_match(\"first_name\", regex_extract=\"^[A-Z]{1,4}\")\n</code></pre></p> <p>Create comparison with exact match level <pre><code>import splink.spark.spark_comparison_library as cl\ncl.exact_match(\"first_name\")\n</code></pre> Create comparison with exact match level based on a substring of first_name as determined by a regular expression ``` python import splink.spark.spark_comparison_library as cl cl.exact_match(\"first_name\", regex_extract=\"^[A-Z]{1,4}\")</p> <p>Create comparison with exact match level <pre><code>import splink.athena.athena_comparison_library as cl\ncl.exact_match(\"first_name\")\n</code></pre> Create comparison with exact match level based on a substring of first_name as determined by a regular expression ``` python import splink.athena.athena_comparison_library as cl cl.exact_match(\"first_name\", regex_extract=\"^[A-Z]{1,4}\")</p> <p>Create comparison with exact match level <pre><code>import splink.sqlite.sqlite_comparison_library as cl\ncl.exact_match(\"first_name\")\n</code></pre></p> <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code> <p>A comparison for exact match that can be included in the Splink settings dictionary</p>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.DistanceFunctionAtThresholdsComparisonBase","title":"<code>splink.comparison_library.DistanceFunctionAtThresholdsComparisonBase</code>","text":"<p>         Bases: <code>Comparison</code></p>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.DistanceFunctionAtThresholdsComparisonBase.__init__","title":"<code>__init__(col_name, distance_function_name, distance_threshold_or_thresholds, regex_extract=None, valid_string_regex=None, set_to_lowercase=False, higher_is_more_similar=True, include_exact_match_level=True, term_frequency_adjustments=False, m_probability_exact_match=None, m_probability_or_probabilities_thres=None, m_probability_else=None)</code>","text":"<p>A comparison of the data in <code>col_name</code> with a user-provided distance function used to assess middle similarity levels.</p> <p>The user-provided distance function must exist in the SQL backend.</p> <p>An example of the output with default arguments and setting <code>distance_function_name</code> to <code>jaccard</code> and <code>distance_threshold_or_thresholds = [0.9,0.7]</code> would be</p> <ul> <li>Exact match</li> <li>Jaccard distance &lt;= 0.9</li> <li>Jaccard distance &lt;= 0.7</li> <li>Anything else</li> </ul> <p>Note: distance_function_at_thresholds() is primarily used in the backend to create the out-of-the-box cl.XXX_at_thresholds() functions</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column to compare</p> required <code>distance_function_name</code> <code>str</code> <p>The name of the distance function.</p> required <code>distance_threshold_or_thresholds</code> <code>Union[int, list]</code> <p>The threshold(s) to use for the middle similarity level(s). Defaults to [1, 2].</p> required <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>valid_string_regex</code> <code>str</code> <p>regular expression pattern that if not matched will result in column being treated as a null.</p> <code>None</code> <code>set_to_lowercase</code> <code>bool</code> <p>If True, sets all entries to lowercase.</p> <code>False</code> <code>higher_is_more_similar</code> <code>bool</code> <p>If True, a higher value of the distance function indicates a higher similarity (e.g. jaro_winkler). If false, a higher value indicates a lower similarity (e.g. levenshtein).</p> <code>True</code> <code>include_exact_match_level</code> <code>bool</code> <p>If True, include an exact match level. Defaults to True.</p> <code>True</code> <code>term_frequency_adjustments</code> <code>bool</code> <p>If True, apply term frequency adjustments to the exact match level. Defaults to False.</p> <code>False</code> <code>m_probability_exact_match</code> <code>float</code> <p>If provided, overrides the default m probability for the exact match level. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_thres</code> <code>Union[float, list]</code> <p>If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>float</code> <p>If provided, overrides the default m probability for the 'anything else' level. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSparkAthena <p>Apply the <code>jaccard</code> function in a comparison with levels 0.9 and 0.7 <pre><code>import splink.duckdb.duckdb_comparison_library as cl\ncl.distance_function_at_thresholds(\"name\",\n                   distance_function_name = 'jaccard',\n                   distance_threshold_or_thresholds = [0.9, 0.7]\n                   )\n</code></pre> Apply the <code>jaccard</code> function in a comparison with levels 0.9 and 0.7 on a substring of name column as determined by a regular expression <pre><code>import splink.duckdb.duckdb_comparison_library as cl\ncl.distance_function_at_thresholds(\"name\",\n                   distance_function_name = 'jaccard',\n                   distance_threshold_or_thresholds = [0.9, 0.7],\n                   regex_extract=\"^[A-Z]{1,4}\n                   )\n</code></pre></p> <p>Apply the <code>jaccard</code> function in a comparison with levels 0.9 and 0.7 <pre><code>import splink.spark.spark_comparison_library as cl\ncl.distance_function_at_thresholds(\"name\",\n                   distance_function_name = 'jaccard',\n                   distance_threshold_or_thresholds = [0.9, 0.7]\n                   )\n</code></pre> Apply the <code>jaccard</code> function in a comparison with levels 0.9 and 0.7 on a substring of name column as determined by a regular expression <pre><code>import splink.spark.spark_comparison_library as cl\ncl.distance_function_at_thresholds(\"name\",\n                   distance_function_name = 'jaccard',\n                   distance_threshold_or_thresholds = [0.9, 0.7],\n                   regex_extract=\"^[A-Z]{1,4}\n                   )\n</code></pre></p> <p>Apply the <code>levenshtein_distance</code> function in a comparison with levels 1 and 2 <pre><code>import splink.athena.athena_comparison_library as cl\ncl.distance_function_at_thresholds(\"name\",\n                   distance_function_name = 'levenshtein_distance',\n                   distance_threshold_or_thresholds = [1, 2],\n                   higher_is_more_similar = False\n                   )\n</code></pre> Apply the <code>jaccard</code> function in a comparison with levels 0.9 and 0.7 on a substring of name column as determined by a regular expression <pre><code>import splink.athena.athena_comparison_library as cl\ncl.distance_function_at_thresholds(\"name\",\n                   distance_function_name = 'jaccard',\n                   distance_threshold_or_thresholds = [0.9, 0.7],\n                   regex_extract=\"^[A-Z]{1,4}\n                   )\n</code></pre></p> <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code> <p>A comparison for a chosen distance function similarity that can be included in the Splink settings dictionary.</p>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.LevenshteinAtThresholdsComparisonBase","title":"<code>splink.comparison_library.LevenshteinAtThresholdsComparisonBase</code>","text":"<p>         Bases: <code>DistanceFunctionAtThresholdsComparisonBase</code></p>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.LevenshteinAtThresholdsComparisonBase.__init__","title":"<code>__init__(col_name, distance_threshold_or_thresholds=[1, 2], regex_extract=None, valid_string_regex=None, set_to_lowercase=False, include_exact_match_level=True, term_frequency_adjustments=False, m_probability_exact_match=None, m_probability_or_probabilities_lev=None, m_probability_else=None)</code>","text":"<p>A comparison of the data in <code>col_name</code> with the levenshtein distance used to assess middle similarity levels.</p> <p>An example of the output with default arguments and setting <code>distance_threshold_or_thresholds = [1,2]</code> would be</p> <ul> <li>Exact match</li> <li>Levenshtein distance &lt;= 1</li> <li>Levenshtein distance &lt;= 2</li> <li>Anything else</li> </ul> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column to compare</p> required <code>distance_threshold_or_thresholds</code> <code>Union[int, list]</code> <p>The threshold(s) to use for the middle similarity level(s). Defaults to [1, 2].</p> <code>[1, 2]</code> <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>valid_string_regex</code> <code>str</code> <p>regular expression pattern that if not matched will result in column being treated as a null.</p> <code>None</code> <code>include_exact_match_level</code> <code>bool</code> <p>If True, include an exact match level. Defaults to True.</p> <code>True</code> <code>term_frequency_adjustments</code> <code>bool</code> <p>If True, apply term frequency adjustments to the exact match level. Defaults to False.</p> <code>False</code> <code>m_probability_exact_match</code> <code>float</code> <p>If provided, overrides the default m probability for the exact match level. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_lev</code> <code>Union[float, list]</code> <p>If provided, overrides the default m probabilities for the thresholds specified for given function. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>float</code> <p>If provided, overrides the default m probability for the 'anything else' level. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSparkAthena <p>Create comparison with levenshtein match levels with distance &lt;=1 and &lt;=2 <pre><code>import splink.duckdb.duckdb_comparison_library as cl\ncl.levenshtein_at_thresholds(\"first_name\", [1,2])\n</code></pre> Create comparison with levenshtein match levels with distance &lt;=1 and &lt;=2 on a substring of name column as determined by a regular expression <pre><code>import splink.duckdb.duckdb_comparison_library as cl\ncl.levenshtein_at_thresholds(\"first_name\", [1,2], regex_extract=\"^A|B\")\n</code></pre></p> <p>Create comparison with levenshtein match levels with distance &lt;=1 and &lt;=2 <pre><code>import splink.spark.spark_comparison_library as cl\ncl.levenshtein_at_thresholds(\"first_name\", [1,2])\n</code></pre> Create comparison with levenshtein match levels with distance &lt;=1 and &lt;=2 on a substring of name column as determined by a regular expression <pre><code>import splink.spark.spark_comparison_library as cl\ncl.levenshtein_at_thresholds(\"first_name\", [1,2], regex_extract=\"^A|B\")\n</code></pre></p> <p>Create comparison with levenshtein match levels with distance &lt;=1 and &lt;=2 <pre><code>import splink.athena.athena_comparison_library as cl\ncl.levenshtein_at_thresholds(\"first_name\", [1,2])\n</code></pre> Create comparison with levenshtein match levels with distance &lt;=1 and &lt;=2 on a substring of name column as determined by a regular expression <pre><code>import splink.athena.athena_comparison_library as cl\ncl.levenshtein_at_thresholds(\"first_name\", [1,2], regex_extract=\"^A|B\")\n</code></pre></p> <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code> <p>A comparison for Levenshtein similarity that can be included in the Splink settings dictionary.</p>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.DamerauLevenshteinAtThresholdsComparisonBase","title":"<code>splink.comparison_library.DamerauLevenshteinAtThresholdsComparisonBase</code>","text":"<p>         Bases: <code>DistanceFunctionAtThresholdsComparisonBase</code></p>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.DamerauLevenshteinAtThresholdsComparisonBase.__init__","title":"<code>__init__(col_name, distance_threshold_or_thresholds=1, regex_extract=None, valid_string_regex=None, set_to_lowercase=False, include_exact_match_level=True, term_frequency_adjustments=False, m_probability_exact_match=None, m_probability_or_probabilities_dl=None, m_probability_else=None)</code>","text":"<p>A comparison of the data in <code>col_name</code> with the damerau-levenshtein distance used to assess middle similarity levels.</p> <p>An example of the output with default arguments and setting <code>distance_threshold_or_thresholds = [1]</code> would be</p> <ul> <li>Exact match</li> <li>Damerau-Levenshtein distance &lt;= 1</li> <li>Anything else</li> </ul> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column to compare</p> required <code>distance_threshold_or_thresholds</code> <code>Union[int, list]</code> <p>The threshold(s) to use for the middle similarity level(s). Defaults to 1.</p> <code>1</code> <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>valid_string_regex</code> <code>str</code> <p>regular expression pattern that if not matched will result in column being treated as a null.</p> <code>None</code> <code>include_exact_match_level</code> <code>bool</code> <p>If True, include an exact match level. Defaults to True.</p> <code>True</code> <code>term_frequency_adjustments</code> <code>bool</code> <p>If True, apply term frequency adjustments to the exact match level. Defaults to False.</p> <code>False</code> <code>m_probability_exact_match</code> <code>_type_</code> <p>If provided, overrides the default m probability for the exact match level. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_dl</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified for given function. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>_type_</code> <p>If provided, overrides the default m probability for the 'anything else' level. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSpark <p>Create comparison with demerau-levenshtein match levels with distance &lt;= 1 <pre><code>import splink.duckdb.duckdb_comparison_library as cl\ncl.damerau_levenshtein_at_thresholds(\"first_name\", [1,2])\n</code></pre> Create comparison with demerau-levenshtein match levels with distance &lt;= 1 on a substring of name column as determined by a regular expression <pre><code>import splink.duckdb.duckdb_comparison_library as cl\ncl.damerau_levenshtein_at_thresholds(\"first_name\",\n                                     [1,2],\n                                     regex_extract=\"^A|B\")\n</code></pre></p> <p>Create comparison with demerau-levenshtein match levels with distance &lt;= 1                ``` python import splink.spark.spark_comparison_library as cl cl.damerau_levenshtein_at_thresholds(\"first_name\", [1,2]) <pre><code>Create comparison with demerau-evenshtein match levels with\ndistance &lt;= 1\non a substring of name column as determined by a regular expression\n``` python\nimport splink.spark.spark_comparison_library as cl\ncl.damerau_levenshtein_at_thresholds(\"first_name\",\n                                     [1,2],\n                                     regex_extract=\"^A|B\")\n</code></pre></p> <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code> <p>A comparison for Damerau-Levenshtein similarity that can be</p> <code>Comparison</code> <p>included in the Splink settings dictionary.</p>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.JaccardAtThresholdsComparisonBase","title":"<code>splink.comparison_library.JaccardAtThresholdsComparisonBase</code>","text":"<p>         Bases: <code>DistanceFunctionAtThresholdsComparisonBase</code></p>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.JaccardAtThresholdsComparisonBase.__init__","title":"<code>__init__(col_name, distance_threshold_or_thresholds=[0.9, 0.7], regex_extract=None, valid_string_regex=None, set_to_lowercase=False, include_exact_match_level=True, term_frequency_adjustments=False, m_probability_exact_match=None, m_probability_or_probabilities_jac=None, m_probability_else=None)</code>","text":"<p>A comparison of the data in <code>col_name</code> with the jaccard distance used to assess middle similarity levels.</p> <p>An example of the output with default arguments and setting <code>distance_threshold_or_thresholds = [0.9,0.7]</code> would be</p> <ul> <li>Exact match</li> <li>Jaccard distance &lt;= 0.9</li> <li>Jaccard distance &lt;= 0.7</li> <li>Anything else</li> </ul> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column to compare</p> required <code>distance_threshold_or_thresholds</code> <code>Union[int, list]</code> <p>The threshold(s) to use for the middle similarity level(s). Defaults to [0.9, 0.7].</p> <code>[0.9, 0.7]</code> <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>valid_string_regex</code> <code>str</code> <p>regular expression pattern that if not matched will result in column being treated as a null.</p> <code>None</code> <code>include_exact_match_level</code> <code>bool</code> <p>If True, include an exact match level. Defaults to True.</p> <code>True</code> <code>term_frequency_adjustments</code> <code>bool</code> <p>If True, apply term frequency adjustments to the exact match level. Defaults to False.</p> <code>False</code> <code>m_probability_exact_match</code> <code>float</code> <p>If provided, overrides the default m probability for the exact match level. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_jac</code> <code>Union[float, list]</code> <p>If provided, overrides the default m probabilities for the thresholds specified for given function. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>float</code> <p>If provided, overrides the default m probability for the 'anything else' level. Defaults to None.</p> <p>Examples:</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code> <p>A comparison for Jaccard similarity that can be included in the Splink settings dictionary.</p>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.JaroAtThresholdsComparisonBase","title":"<code>splink.comparison_library.JaroAtThresholdsComparisonBase</code>","text":"<p>         Bases: <code>DistanceFunctionAtThresholdsComparisonBase</code></p>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.JaroAtThresholdsComparisonBase.__init__","title":"<code>__init__(col_name, distance_threshold_or_thresholds=[0.9, 0.7], regex_extract=None, valid_string_regex=None, set_to_lowercase=False, include_exact_match_level=True, term_frequency_adjustments=False, m_probability_exact_match=None, m_probability_or_probabilities_jar=None, m_probability_else=None)</code>","text":"<p>A comparison of the data in <code>col_name</code> with the jaro distance used to assess middle similarity levels.</p> <p>An example of the output with default arguments and setting <code>distance_threshold_or_thresholds = [0.9, 0.7]</code> would be</p> <ul> <li>Exact match</li> <li>Jaro distance &lt;= 0.9</li> <li>Jaro distance &lt;= 0.7</li> <li>Anything else</li> </ul> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column to compare</p> required <code>distance_threshold_or_thresholds</code> <code>Union[int, list]</code> <p>The threshold(s) to use for the middle similarity level(s). Defaults to [0.9, 0.7].</p> <code>[0.9, 0.7]</code> <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>valid_string_regex</code> <code>str</code> <p>regular expression pattern that if not matched will result in column being treated as a null.</p> <code>None</code> <code>include_exact_match_level</code> <code>bool</code> <p>If True, include an exact match level. Defaults to True.</p> <code>True</code> <code>term_frequency_adjustments</code> <code>bool</code> <p>If True, apply term frequency adjustments to the exact match level. Defaults to False.</p> <code>False</code> <code>m_probability_exact_match</code> <code>float</code> <p>If provided, overrides the default m probability for the exact match level. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_jar</code> <code>Union[float, list]</code> <p>If provided, overrides the default m probabilities for the thresholds specified for given function. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>float</code> <p>If provided, overrides the default m probability for the 'anything else' level. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSpark <p>Create comparison with jaro match levels with similarity score &gt;=0.9 and &gt;=0.7 <pre><code>import splink.duckdb.duckdb_comparison_library as cl\ncl.jaro_at_thresholds(\"first_name\", [0.9, 0.7])\n</code></pre> Create comparison with jaro match levels with similarity score &gt;=0.9 and &gt;=0.7 on a substring of name column as determined by a regular expression <pre><code>import splink.duckdb.duckdb_comparison_library as cl\ncl.jaro_at_thresholds(\"first_name\", [0.9, 0.7], regex_extract=\"^[A-Z]\")\n</code></pre></p> <p>Create comparison with jaro match levels with similarity score &gt;=0.9 and &gt;=0.7 <pre><code>import splink.spark.spark_comparison_library as cl\ncl.jaro_at_thresholds(\"first_name\", [0.9, 0.7])\n</code></pre> Create comparison with jaro match levels with similarity score &gt;=0.9 and &gt;=0.7 on a substring of name column as determined by a regular expression ``` python import splink.spark.spark_comparison_library as cl cl.jaro_at_thresholds(\"first_name\", [0.9, 0.7], regex_extract=\"^[A-Z]\")</p> <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.JaroWinklerAtThresholdsComparisonBase","title":"<code>splink.comparison_library.JaroWinklerAtThresholdsComparisonBase</code>","text":"<p>         Bases: <code>DistanceFunctionAtThresholdsComparisonBase</code></p>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.JaroWinklerAtThresholdsComparisonBase.__init__","title":"<code>__init__(col_name, distance_threshold_or_thresholds=[0.9, 0.7], regex_extract=None, valid_string_regex=None, set_to_lowercase=False, include_exact_match_level=True, term_frequency_adjustments=False, m_probability_exact_match=None, m_probability_or_probabilities_jw=None, m_probability_else=None)</code>","text":"<p>A comparison of the data in <code>col_name</code> with the jaro_winkler distance used to assess middle similarity levels.</p> <p>An example of the output with default arguments and setting <code>distance_threshold_or_thresholds = [0.9, 0.7]</code> would be</p> <ul> <li>Exact match</li> <li>Jaro-Winkler distance &lt;= 0.9</li> <li>Jaro-Winkler distance &lt;= 0.7</li> <li>Anything else</li> </ul> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column to compare</p> required <code>distance_threshold_or_thresholds</code> <code>Union[int, list]</code> <p>The threshold(s) to use for the middle similarity level(s). Defaults to [0.9, 0.7].</p> <code>[0.9, 0.7]</code> <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>valid_string_regex</code> <code>str</code> <p>regular expression pattern that if not matched will result in column being treated as a null.</p> <code>None</code> <code>include_exact_match_level</code> <code>bool</code> <p>If True, include an exact match level. Defaults to True.</p> <code>True</code> <code>term_frequency_adjustments</code> <code>bool</code> <p>If True, apply term frequency adjustments to the exact match level. Defaults to False.</p> <code>False</code> <code>m_probability_exact_match</code> <code>float</code> <p>If provided, overrides the default m probability for the exact match level. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_jw</code> <code>Union[float, list]</code> <p>If provided, overrides the default m probabilities for the thresholds specified for given function. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>float</code> <p>If provided, overrides the default m probability for the 'anything else' level. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSpark <p>Create comparison with jaro_winkler match levels with similarity score</p> <p>=0.9 and &gt;=0.7 <pre><code>import splink.duckdb.duckdb_comparison_library as cl\ncl.jaro_winkler_at_thresholds(\"first_name\", [0.9, 0.7])\n</code></pre> Create comparison with jaro_winkler match levels with similarity score =0.9 and &gt;=0.7 on a substring of name column as determined by a regular expression <pre><code>import splink.duckdb.duckdb_comparison_library as cl\ncl.jaro_winkler_at_thresholds(\"first_name\",\n                              [0.9, 0.7],\n                              regex_extract=\"^[A-Z]\"\n                              )\n</code></pre></p> <p>Create comparison with jaro_winkler match levels with similarity score</p> <p>=0.9 and &gt;=0.7 <pre><code>import splink.spark.spark_comparison_library as cl\ncl.jaro_winkler_at_thresholds(\"first_name\", [0.9, 0.7])\n</code></pre> Create comparison with jaro_winkler match levels with similarity score =0.9 and &gt;=0.7 on a substring of name column as determined by a regular expression <pre><code>import splink.spark.spark_comparison_library as cl\ncl.jaro_winkler_at_thresholds(\"first_name\",\n                              [0.9, 0.7],\n                              regex_extract=\"^[A-Z]\"\n                              )\n</code></pre></p> <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code> <p>A comparison for Jaro Winkler similarity that can be included in the Splink settings dictionary.</p>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.ArrayIntersectAtSizesComparisonBase","title":"<code>splink.comparison_library.ArrayIntersectAtSizesComparisonBase</code>","text":"<p>         Bases: <code>Comparison</code></p>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.ArrayIntersectAtSizesComparisonBase.__init__","title":"<code>__init__(col_name, size_or_sizes=[1], m_probability_or_probabilities_sizes=None, m_probability_else=None)</code>","text":"<p>A comparison of the data in array column <code>col_name</code> with various intersection sizes to assess similarity levels.</p> <p>An example of the output with default arguments and setting <code>size_or_sizes = [3, 1]</code> would be</p> <ul> <li>Intersection has at least 3 elements</li> <li>Intersection has at least 1 element (i.e. 1 or 2)</li> <li>Anything else (i.e. empty intersection)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column to compare</p> required <code>size_or_sizes</code> <code>Union[int, list]</code> <p>The size(s) of intersection to use for the non-'else' similarity level(s). Should be in descending order. Defaults to [1].</p> <code>[1]</code> <code>m_probability_or_probabilities_sizes</code> <code>Union[float, list]</code> <p>If provided, overrides the default m probabilities for the sizes specified. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>float</code> <p>If provided, overrides the default m probability for the 'anything else' level. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSparkAthena <pre><code>import splink.duckdb.duckdb_comparison_library as cl\ncl.array_intersect_at_sizes(\"first_name\", [3, 1])\n</code></pre> <pre><code>import splink.spark.spark_comparison_library as cl\ncl.array_intersect_at_sizes(\"first_name\", [3, 1])\n</code></pre> <pre><code>import splink.athena.athena_comparison_library as cl\ncl.array_intersect_at_sizes(\"first_name\", [3, 1])\n</code></pre> <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code> <p>A comparison for the intersection of arrays that can be included in the Splink settings dictionary.</p>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.DateDiffAtThresholdsComparisonBase","title":"<code>splink.comparison_library.DateDiffAtThresholdsComparisonBase</code>","text":"<p>         Bases: <code>Comparison</code></p>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.DateDiffAtThresholdsComparisonBase.__init__","title":"<code>__init__(col_name, date_thresholds=[1], date_metrics=['year'], cast_strings_to_date=False, date_format=None, invalid_dates_as_null=False, include_exact_match_level=True, term_frequency_adjustments=False, m_probability_exact_match=None, m_probability_or_probabilities_dat=None, m_probability_else=None)</code>","text":"<p>A comparison of the data in the date column <code>col_name</code> with various date thresholds and metrics to assess similarity levels.</p> <p>An example of the output with default arguments and settings <code>date_thresholds = [1]</code> and <code>date_metrics = ['day']</code> would be - The two input dates are within 1 day of one another - Anything else (i.e. all other dates lie outside this range)</p> <p><code>date_thresholds</code> and <code>date_metrics</code> should be used in conjunction with one another. For example, <code>date_thresholds = [10, 12, 15]</code> with <code>date_metrics = ['day', 'month', 'year']</code> would result in the following checks:</p> <ul> <li>The two dates are within 10 days of one another</li> <li>The two dates are within 12 months of one another</li> <li>And the two dates are within 15 years of one another</li> </ul> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the date column to compare.</p> required <code>date_thresholds</code> <code>Union[int, list]</code> <p>The size(s) of given date thresholds, to assess whether two dates fall within a given time interval. These values can be any integer value and should be used in tandem with <code>date_metrics</code>.</p> <code>[1]</code> <code>date_metrics</code> <code>Union[str, list]</code> <p>The unit of time you wish your <code>date_thresholds</code> to be measured against. Metrics should be one of <code>day</code>, <code>month</code> or <code>year</code>.</p> <code>['year']</code> <code>cast_strings_to_date</code> <code>bool</code> <p>Set to True to enable date-casting when input dates are strings. Also adjust date_format if date-strings are not in (yyyy-mm-dd) format. Defaults to False.</p> <code>False</code> <code>date_format(str,</code> <code>optional</code> <p>Format of input dates if date-strings are given. Must be consistent across record pairs. If None (the default), downstream functions for each backend assign date_format to ISO 8601 format (yyyy-mm-dd). Set to \"yyyy-MM-dd\" for Spark and \"%Y-%m-%d\" for DuckDB when invalid_dates_as_null=True</p> required <code>invalid_dates_as_null</code> <code>bool</code> <p>assign any dates that do not adhere to date_format to the null level. Defaults to False.</p> <code>False</code> <code>include_exact_match_level</code> <code>bool</code> <p>If True, include an exact match level. Defaults to True.</p> <code>True</code> <code>term_frequency_adjustments</code> <code>bool</code> <p>If True, apply term frequency adjustments to the exact match level. Defaults to False.</p> <code>False</code> <code>m_probability_exact_match</code> <code>_type_</code> <p>If provided, overrides the default m probability for the exact match level. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_dat</code> <code>Union[float, list]</code> <p>If provided, overrides the default m probabilities for the sizes specified. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>_type_</code> <p>If provided, overrides the default m probability for the 'anything else' level. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSpark <p>Date Difference comparison at thresholds 10 days, 12 months and 15 years <pre><code>import splink.duckdb.duckdb_comparison_library as cl\ncl.datediff_at_thresholds(\"date\",\n                            date_thresholds = [10, 12, 15],\n                            date_metrics = ['day', 'month', 'year']\n                            )\n</code></pre></p> <p>Datediff comparison with date-casting and unspecified date_format (default = %Y-%m-%d) <pre><code>import splink.duckdb.duckdb_comparison_library as cl\ncl.datediff_at_thresholds(\"date\",\n                            date_thresholds=[1,5],\n                            date_metrics = [\"day\", \"year\"],\n                            cast_strings_to_date=True\n                            )\n</code></pre> Datediff comparison with date-casting and specified (non-default) date_format <pre><code>import splink.duckdb.duckdb_comparison_library as cl\ncl.datediff_at_thresholds(\"date\",\n                            date_thresholds=[1,5],\n                            date_metrics = [\"day\", \"year\"],\n                            cast_strings_to_date=True,\n                            date_format='%d/%m/%Y'\n                            )\n</code></pre></p> <p>Date Difference comparison at thresholds 10 days, 12 months and 15 years <pre><code>import splink.spark.spark_comparison_library as cl\ncl.datediff_at_thresholds(\"date\",\n                            date_thresholds = [10, 12, 15],\n                            date_metrics = ['day', 'month', 'year']\n                            )\n</code></pre></p> <p>Datediff comparison with date-casting and unspecified date_format (default = %Y-%m-%d) <pre><code>    import splink.spark.spark_comparison_library as cl\n    cl.datediff_at_thresholds(\"date\",\n                                date_thresholds=[1,5],\n                                date_metrics = [\"day\", \"year\"],\n                                cast_strings_to_date=True\n                                )\n</code></pre></p> <p>Datediff comparison with date-casting and specified (non-default) date_format <pre><code>import splink.spark.spark_comparison_library as cl\ncl.datediff_at_thresholds(\"date\",\n                            date_thresholds=[1,5],\n                            date_metrics = [\"day\", \"year\"],\n                            cast_strings_to_date=True,\n                            date_format='%d/%m/%Y'\n                            )\n</code></pre></p> <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code> <p>A comparison for Datediff that can be included in the Splink settings dictionary.</p>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.DistanceInKMAtThresholdsComparisonBase","title":"<code>splink.comparison_library.DistanceInKMAtThresholdsComparisonBase</code>","text":"<p>         Bases: <code>Comparison</code></p>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_library.html#splink.comparison_library.DistanceInKMAtThresholdsComparisonBase.__init__","title":"<code>__init__(lat_col, long_col, km_thresholds=[0.1, 1], include_exact_match_level=False, m_probability_exact_match=None, m_probability_or_probabilities_km=None, m_probability_else=None)</code>","text":"<p>A comparison of the coordinates defined in 'lat_col' and 'long col' giving the haversine distance between them in km.</p> <p>An example of the output with default arguments and settings <code>km_thresholds = [1]</code> would be</p> <ul> <li>The two coordinates within 1 km of one another</li> <li>Anything else (i.e.  the distance between all coordinate lie outside this range)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the date column to compare.</p> required <code>lat_col</code> <code>str</code> <p>The name of the column containing the lattitude of the coordinates.</p> required <code>long_col</code> <code>str</code> <p>The name of the column containing the longitude of the coordinates.</p> required <code>km_thresholds</code> <code>Union[int, list]</code> <p>The size(s) of given date thresholds, to assess whether two coordinates fall within a given distance.</p> <code>[0.1, 1]</code> <code>include_exact_match_level</code> <code>bool</code> <p>If True, include an exact match level. Defaults to True.</p> <code>False</code> <code>m_probability_exact_match</code> <code>float</code> <p>If provided, overrides the default m probability for the exact match level. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_km</code> <code>Union[float, list]</code> <p>If provided, overrides the default m probabilities for the sizes specified. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>float</code> <p>If provided, overrides the default m probability for the 'anything else' level. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSparkAthena <pre><code>import splink.duckdb.duckdb_comparison_library as cl\ncl.distance_in_km_at_thresholds(\"lat_col\",\n                           \"long_col\",\n                           km_thresholds = [0.1, 1, 10]\n                        )\n</code></pre> <pre><code>import splink.spark.spark_comparison_library as cl\ncl.distance_in_km_at_thresholds(\"lat_col\",\n                           \"long_col\",\n                           km_thresholds = [0.1, 1, 10]\n                        )\n</code></pre> <pre><code>import splink.athena.athena_comparison_library as cl\ncl.distance_in_km_at_thresholds(\"lat_col\",\n                           \"long_col\",\n                           km_thresholds = [0.1, 1, 10]\n                        )\n</code></pre> <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code> <p>A comparison for Distance in KM that can be included in the Splink settings dictionary.</p>","tags":["API","comparisons","Levenshtein","Jaro-Winkler","Jaccard","Distance In KM","Date Difference","Array Intersect"]},{"location":"comparison_template_library.html","title":"Documentation for <code>comparison_template_library</code>","text":"<p>The <code>comparison_template_library</code> contains pre-made comparisons with pre-defined parameters available for use directly as described in this topic guide. However, not every comparison is available for every Splink-compatible SQL backend. More detail on creating comparisons for specific data types is also included in the topic guide.</p> <p>The pre-made Splink comparison templates available for each SQL dialect are as given in this table:</p> duckdb spark athena sqlite <code>date_comparison</code> \u2713 \u2713 <code>forename_surname_comparison</code> \u2713 \u2713 <code>name_comparison</code> \u2713 \u2713 <code>postcode_comparison</code> \u2713 \u2713 \u2713 <p>The detailed API for each of these are outlined below.</p>","tags":["API","comparisons","Date Comparison"]},{"location":"comparison_template_library.html#library-comparison-apis","title":"Library comparison APIs","text":"","tags":["API","comparisons","Date Comparison"]},{"location":"comparison_template_library.html#splink.comparison_template_library.DateComparisonBase","title":"<code>splink.comparison_template_library.DateComparisonBase</code>","text":"<p>         Bases: <code>Comparison</code></p>","tags":["API","comparisons","Date Comparison"]},{"location":"comparison_template_library.html#splink.comparison_template_library.DateComparisonBase.__init__","title":"<code>__init__(col_name, cast_strings_to_date=False, date_format=None, invalid_dates_as_null=False, include_exact_match_level=True, term_frequency_adjustments=False, separate_1st_january=False, levenshtein_thresholds=[], damerau_levenshtein_thresholds=[1], datediff_thresholds=[1, 1, 10], datediff_metrics=['month', 'year', 'year'], m_probability_exact_match=None, m_probability_1st_january=None, m_probability_or_probabilities_lev=None, m_probability_or_probabilities_dl=None, m_probability_or_probabilities_datediff=None, m_probability_else=None)</code>","text":"<p>A wrapper to generate a comparison for a date column the data in <code>col_name</code> with preselected defaults.</p> <p>The default arguments will give a comparison with comparison levels:</p> <ul> <li> <p>Exact match (1st of January only)</p> </li> <li> <p>Exact match (all other dates)</p> </li> <li> <p>Damerau-Levenshtein distance &lt;= 1</p> </li> <li> <p>Date difference &lt;= 1 year</p> </li> <li> <p>Date difference &lt;= 10 years </p> </li> <li> <p>Anything else</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column to compare.</p> required <code>cast_strings_to_date</code> <code>bool</code> <p>Set to True to enable date-casting when input dates are strings. Also adjust date_format if date-strings are not in (yyyy-mm-dd) format. Defaults to False.</p> <code>False</code> <code>date_format</code> <code>str</code> <p>Format of input dates if date-strings are given. Must be consistent across record pairs. If None (the default), downstream functions for each backend assign date_format to ISO 8601 format (yyyy-mm-dd). Set to \"yyyy-MM-dd\" for Spark and \"%Y-%m-%d\" for DuckDB when invalid_dates_as_null=True</p> <code>None</code> <code>invalid_dates_as_null</code> <code>bool</code> <p>assign any dates that do not adhere to date_format to the null level. Defaults to False.</p> <code>False</code> <code>include_exact_match_level</code> <code>bool</code> <p>If True, include an exact match level. Defaults to True.</p> <code>True</code> <code>term_frequency_adjustments</code> <code>bool</code> <p>If True, apply term frequency adjustments to the exact match level. Defaults to False.</p> <code>False</code> <code>separate_1st_january</code> <code>bool</code> <p>If True, include a separate exact match comparison level when date is 1st January.</p> <code>False</code> <code>levenshtein_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for levenshtein similarity level(s). Defaults to []</p> <code>[]</code> <code>damerau_levenshtein_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for damerau-levenshtein similarity level(s). Defaults to [1]</p> <code>[1]</code> <code>datediff_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for datediff similarity level(s). Defaults to [1, 1].</p> <code>[1, 1, 10]</code> <code>datediff_metrics</code> <code>Union[str, list]</code> <p>The metrics to apply thresholds to for datediff similarity level(s). Defaults to [\"month\", \"year\"].</p> <code>['month', 'year', 'year']</code> <code>cast_strings_to_date</code> <code>bool</code> <p>Set to True to enable date-casting when input dates are strings. Also adjust date_format if date-strings are not in (yyyy-mm-dd) format. Defaults to False.</p> <code>False</code> <code>date_format</code> <code>str</code> <p>Format of input dates if date-strings are given. Must be consistent across record pairs. If None (the default), downstream functions for each backend assign date_format to ISO 8601 format (yyyy-mm-dd).</p> <code>None</code> <code>m_probability_exact_match</code> <code>float</code> <p>If provided, overrides the default m probability for the exact match level. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_lev</code> <code>Union[float, list]</code> <p>If provided, overrides the default m probabilities for the levenshtein thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_dl</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the damerau-levenshtein thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_datediff</code> <code>Union[float, list]</code> <p>If provided, overrides the default m probabilities for the datediff thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>float</code> <p>If provided, overrides the default m probability for the 'anything else' level. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSpark <p>Basic Date Comparison <pre><code>import splink.duckdb.duckdb_comparison_template_library as ctl\nctl.date_comparison(\"date_of_birth\")\n</code></pre> Bespoke Date Comparison <pre><code>import splink.duckdb.duckdb_comparison_template_library as ctl\nctl.date_comparison(\"date_of_birth\",\n                    damerau_levenshtein_thresholds=[],\n                    levenshtein_thresholds=[2],\n                    datediff_thresholds=[1, 1],\n                    datediff_metrics=[\"month\", \"year\"])\n</code></pre> Date Comparison casting columns date and assigning values that do not match the date_format to the null level <pre><code>import splink.duckdb.duckdb_comparison_template_library as ctl\nctl.date_comparison(\"date_of_birth\",\n                    cast_strings_to_date=True,\n                    date_format='%d/%m/%Y',\n                    invalid_dates_as_null=True)\n</code></pre></p> <p>Basic Date Comparison <pre><code>import splink.spark.spark_comparison_template_library as ctl\nctl.date_comparison(\"date_of_birth\")\n</code></pre> Bespoke Date Comparison <pre><code>import splink.spark.spark_comparison_template_library as ctl\nctl.date_comparison(\"date_of_birth\",\n                    damerau_levenshtein_thresholds=[],\n                    levenshtein_thresholds=[2],\n                    datediff_thresholds=[1, 1],\n                    datediff_metrics=[\"month\", \"year\"])\n</code></pre> Date Comparison casting columns date and assigning values that do not match the date_format to the null level <pre><code>import splink.spark.spark_comparison_template_library as ctl\nctl.date_comparison(\"date_of_birth\",\n                    cast_strings_to_date=True,\n                    date_format='dd/mm/yyyy',\n                    invalid_dates_as_null=True)\n</code></pre></p> <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code> <p>A comparison that can be inclued in the Splink settings dictionary.</p>","tags":["API","comparisons","Date Comparison"]},{"location":"comparison_template_library.html#splink.comparison_template_library.NameComparisonBase","title":"<code>splink.comparison_template_library.NameComparisonBase</code>","text":"<p>         Bases: <code>Comparison</code></p>","tags":["API","comparisons","Date Comparison"]},{"location":"comparison_template_library.html#splink.comparison_template_library.NameComparisonBase.__init__","title":"<code>__init__(col_name, regex_extract=None, set_to_lowercase=False, include_exact_match_level=True, phonetic_col_name=None, term_frequency_adjustments=False, levenshtein_thresholds=[], damerau_levenshtein_thresholds=[1], jaro_thresholds=[], jaro_winkler_thresholds=[0.9, 0.8], jaccard_thresholds=[], m_probability_exact_match_name=None, m_probability_exact_match_phonetic_name=None, m_probability_or_probabilities_lev=None, m_probability_or_probabilities_dl=None, m_probability_or_probabilities_jar=None, m_probability_or_probabilities_jw=None, m_probability_or_probabilities_jac=None, m_probability_else=None)</code>","text":"<p>A wrapper to generate a comparison for a name column the data in <code>col_name</code> with preselected defaults.</p> <p>The default arguments will give a comparison with comparison levels:</p> <ul> <li> <p>Exact match </p> </li> <li> <p>Damerau-Levenshtein Distance &lt;= 1</p> </li> <li> <p>Jaro Winkler similarity &gt;= 0.9</p> </li> <li> <p>Jaro Winkler similarity &gt;= 0.8</p> </li> <li> <p>Anything else</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column to compare.</p> required <code>regex_extract</code> <code>str</code> <p>Regular expression pattern to evaluate a match on.</p> <code>None</code> <code>set_to_lowercase</code> <code>bool</code> <p>If True, all names are set to lowercase during the pairwise comparisons. Defaults to False</p> <code>False</code> <code>include_exact_match_level</code> <code>bool</code> <p>If True, include an exact match level for col_name. Defaults to True.</p> <code>True</code> <code>phonetic_col_name</code> <code>str</code> <p>The name of the column with phonetic reduction (such as dmetaphone) of col_name. Including parameter will create an exact match level for  phonetic_col_name. The phonetic column must be present in the dataset to use this parameter. Defaults to None</p> <code>None</code> <code>term_frequency_adjustments</code> <code>bool</code> <p>If True, apply term frequency adjustments to the exact match level for \"col_name\". Defaults to False.</p> <code>False</code> <code>term_frequency_adjustments_phonetic_name</code> <code>bool</code> <p>If True, apply term frequency adjustments to the exact match level for \"phonetic_col_name\". Defaults to False.</p> required <code>levenshtein_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for levenshtein similarity level(s). Defaults to []</p> <code>[]</code> <code>damerau_levenshtein_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for damerau-levenshtein similarity level(s). Defaults to [1]</p> <code>[1]</code> <code>jaro_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for jaro similarity level(s). Defaults to []</p> <code>[]</code> <code>jaro_winkler_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for jaro_winkler similarity level(s). Defaults to [0.9, 0.8]</p> <code>[0.9, 0.8]</code> <code>jaccard_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for jaccard similarity level(s). Defaults to []</p> <code>[]</code> <code>m_probability_exact_match_name</code> <code>_type_</code> <p>Starting m probability for exact match level. Defaults to None.</p> <code>None</code> <code>m_probability_exact_match_phonetic_name</code> <code>_type_</code> <p>Starting m probability for exact match level for phonetic_col_name. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_lev</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_dl</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_datediff</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> required <code>m_probability_or_probabilities_jar</code> <code>Union[float, list]</code> <p>Starting m probabilities for the jaro thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_jw</code> <code>Union[float, list]</code> <p>Starting m probabilities for the jaro winkler thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_jac</code> <code>Union[float, list]</code> <p>Starting m probabilities for the jaccard thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>_type_</code> <p>Starting m probability for the 'everything else' level. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSpark <p>Basic Name Comparison <pre><code>import splink.duckdb.duckdb_comparison_template_library as ctl\nctl.name_comparison(\"name\")\n</code></pre> Bespoke Name Comparison <pre><code>import splink.duckdb.duckdb_comparison_template_library as ctl\nctl.name_comparison(\"name\",\n                    phonetic_col_name = \"name_dm\",\n                    term_frequency_adjustments = True,\n                    levenshtein_thresholds=[2],\n                    damerau_levenshtein_thresholds=[],\n                    jaro_winkler_thresholds=[],\n                    jaccard_thresholds=[1]\n                    )\n</code></pre></p> <p>Basic Name Comparison <pre><code>import splink.spark.spark_comparison_template_library as ctl\nctl.name_comparison(\"name\")\n</code></pre> Bespoke Name Comparison <pre><code>import splink.spark.spark_comparison_template_library as ctl\nctl.name_comparison(\"name\",\n                    phonetic_col_name = \"name_dm\",\n                    term_frequency_adjustments = True,\n                    levenshtein_thresholds=[2],\n                    damerau_levenshtein_thresholds=[],\n                    jaro_winkler_thresholds=[],\n                    jaccard_thresholds=[1]\n                    )\n</code></pre></p> <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code> <p>A comparison that can be included in the Splink settings dictionary.</p>","tags":["API","comparisons","Date Comparison"]},{"location":"comparison_template_library.html#splink.comparison_template_library.ForenameSurnameComparisonBase","title":"<code>splink.comparison_template_library.ForenameSurnameComparisonBase</code>","text":"<p>         Bases: <code>Comparison</code></p>","tags":["API","comparisons","Date Comparison"]},{"location":"comparison_template_library.html#splink.comparison_template_library.ForenameSurnameComparisonBase.__init__","title":"<code>__init__(forename_col_name, surname_col_name, set_to_lowercase=False, include_exact_match_level=True, include_columns_reversed=True, term_frequency_adjustments=False, tf_adjustment_col_forename_and_surname=None, phonetic_forename_col_name=None, phonetic_surname_col_name=None, levenshtein_thresholds=[], damerau_levenshtein_thresholds=[], jaro_winkler_thresholds=[0.88], jaro_thresholds=[], jaccard_thresholds=[], m_probability_exact_match_forename_surname=None, m_probability_exact_match_phonetic_forename_surname=None, m_probability_columns_reversed_forename_surname=None, m_probability_exact_match_surname=None, m_probability_exact_match_forename=None, m_probability_exact_match_phonetic_surname=None, m_probability_exact_match_phonetic_forename=None, m_probability_or_probabilities_surname_lev=None, m_probability_or_probabilities_surname_dl=None, m_probability_or_probabilities_surname_jw=None, m_probability_or_probabilities_surname_jac=None, m_probability_or_probabilities_forename_lev=None, m_probability_or_probabilities_forename_dl=None, m_probability_or_probabilities_forename_jw=None, m_probability_or_probabilities_forename_jac=None, m_probability_else=None)</code>","text":"<p>A wrapper to generate a comparison for a name column the data in <code>col_name</code> with preselected defaults.</p> <p>The default arguments will give a comparison with comparison levels:</p> <ul> <li> <p>Exact match forename and surname</p> </li> <li> <p>Macth of forename and surname reversed</p> </li> <li> <p>Exact match surname</p> </li> <li> <p>Exact match forename</p> </li> <li> <p>Fuzzy match surname jaro-winkler &gt;= 0.88</p> </li> <li> <p>Fuzzy match forename jaro-winkler&gt;=  0.88</p> </li> <li> <p>Anything else</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>forename_col_name</code> <code>str</code> <p>The name of the forename column to compare</p> required <code>surname_col_name</code> <code>str</code> <p>The name of the surname column to compare</p> required <code>set_to_lowercase</code> <code>bool</code> <p>If True, all names are set to lowercase during the pairwise comparisons. Defaults to False</p> <code>False</code> <code>include_exact_match_level</code> <code>bool</code> <p>If True, include an exact match level for col_name. Defaults to True.</p> <code>True</code> <code>include_columns_reversed</code> <code>bool</code> <p>If True, include a comparison level for forename and surname being swapped. Defaults to True</p> <code>True</code> <code>term_frequency_adjustments</code> <code>bool</code> <p>If True, apply term frequency adjustments to the exact match level for forename_col_name and surname_col_name. Applies term frequency adjustments to full name exact match level and columns reversed exact match level if tf_adjustment_col_forename_and_surname is provided. Applies term frequency adjustments to phonetic_forename_col_name and phonetic_surname_col_name exact match levels, if they are provided. Defaults to False.</p> <code>False</code> <code>tf_adjustment_col_forename_and_surname</code> <code>str</code> <p>The name of a combined forename surname column. This column is used to provide term frequency adjustments for forename surname exact match and columns reversed levels. Defaults to None</p> <code>None</code> <code>set_to_lowercase</code> <code>bool</code> <p>If True, all postcodes are set to lowercase during the pairwise comparisons. Defaults to True</p> <code>False</code> <code>phonetic_forename_col_name</code> <code>str</code> <p>The name of the column with phonetic reduction (such as dmetaphone) of forename_col_name. Including parameter will create an exact match level for phonetic_forename_col_name. The phonetic column must be present in the dataset to use this parameter. Defaults to None</p> <code>None</code> <code>phonetic_surname_col_name</code> <code>str</code> <p>The name of the column with phonetic reduction (such as dmetaphone) of surname_col_name. Including this parameter will create an exact match level for phonetic_surname_col_name. The phonetic column must be present in the dataset to use this parameter. Defaults to None</p> <code>None</code> <code>levenshtein_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for levenshtein similarity level(s) for surname_col_name and forename_col_name. Defaults to []</p> <code>[]</code> <code>damerau_levenshtein_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for damerau-levenshtein similarity level(s). Defaults to []</p> <code>[]</code> <code>jaro_winkler_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for jaro_winkler similarity level(s) for surname_col_name and forename_col_name. Defaults to [0.88]</p> <code>[0.88]</code> <code>jaro_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for jaro similarity level(s) for surname_col_name and forename_col_name. Defaults to []</p> <code>[]</code> <code>jaccard_thresholds</code> <code>Union[int, list]</code> <p>The thresholds to use for jaccard similarity level(s) for surname_col_name and forename_col_name. Defaults to []</p> <code>[]</code> <code>m_probability_exact_match_forename_surname</code> <code>_type_</code> <p>If provided, overrides the default m probability for the exact match level for forename and surname. Defaults to None.</p> <code>None</code> <code>m_probability_exact_match_phonetic_forename_surname</code> <code>_type_</code> <p>If provided, overrides the default m probability for the phonetic match level for forename and surname. Defaults to None.</p> <code>None</code> <code>m_probability_columns_reversed_forename_surname</code> <code>_type_</code> <p>If provided, overrides the default m probability for the columns reversed level for forename and surname. Defaults to None.</p> <code>None</code> <code>m_probability_columns_reversed_forename_surname</code> <code>_type_</code> <p>If provided, overrides the default m probability for the columns reversed level for forename and surname. Defaults to None.</p> <code>None</code> <code>m_probability_exact_match_surname</code> <code>_type_</code> <p>If provided, overrides the default m probability for the surname exact match level for forename and surname. Defaults to None.</p> <code>None</code> <code>m_probability_exact_match_forename</code> <code>_type_</code> <p>If provided, overrides the default m probability for the forename exact match level for forename and forename. Defaults to None.</p> <code>None</code> <code>m_probability_phonetic_match_surname</code> <code>_type_</code> <p>If provided, overrides the default m probability for the surname phonetic match level for forename and surname. Defaults to None.</p> required <code>m_probability_phonetic_match_forename</code> <code>_type_</code> <p>If provided, overrides the default m probability for the forename phonetic match level for forename and forename. Defaults to None.</p> required <code>m_probability_or_probabilities_surname_lev</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_surname_dl</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_surname_jw</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_surname_jac</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_forename_lev</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_forename_dl</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_forename_jw</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_forename_jac</code> <code>Union[float, list]</code> <p>description. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>_type_</code> <p>If provided, overrides the default m probability for the 'anything else' level. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSpark <p>Basic Forename Surname Comparison <pre><code>import splink.duckdb.duckdb_comparison_template_library as ctl\nctl.forename_surname_comparison(\"first_name\", \"surname)\n</code></pre></p> <p>Bespoke Forename Surname Comparison <pre><code>import splink.duckdb.duckdb_comparison_template_library as ctl\nctl.forename_surname_comparison(\n        \"forename\",\n        \"surname\",\n        term_frequency_adjustments=True,\n        tf_adjustment_col_forename_and_surname=\"full_name\",\n        phonetic_forename_col_name=\"forename_dm\",\n        phonetic_surname_col_name=\"surname_dm\",\n        levenshtein_thresholds=[2],\n        jaro_winkler_thresholds=[],\n        jaccard_thresholds=[1],\n    )\n</code></pre></p> <p>Basic Forename Surname Comparison <pre><code>import splink.spark.spark_comparison_template_library as ctl\nctl.forename_surname_comparison(\"first_name\", \"surname)\n</code></pre></p> <p>Bespoke Forename Surname Comparison <pre><code>import splink.spark.spark_comparison_template_library as ctl\nctl.forename_surname_comparison(\n        \"forename\",\n        \"surname\",\n        term_frequency_adjustments=True,\n        tf_adjustment_col_forename_and_surname=\"full_name\",\n        phonetic_forename_col_name=\"forename_dm\",\n        phonetic_surname_col_name=\"surname_dm\",\n        levenshtein_thresholds=[2],\n        jaro_winkler_thresholds=[],\n        jaccard_thresholds=[1],\n    )\n</code></pre></p> <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code> <p>A comparison that can be included in the Splink settings dictionary.</p>","tags":["API","comparisons","Date Comparison"]},{"location":"comparison_template_library.html#splink.comparison_template_library.PostcodeComparisonBase","title":"<code>splink.comparison_template_library.PostcodeComparisonBase</code>","text":"<p>         Bases: <code>Comparison</code></p>","tags":["API","comparisons","Date Comparison"]},{"location":"comparison_template_library.html#splink.comparison_template_library.PostcodeComparisonBase.__init__","title":"<code>__init__(col_name, invalid_postcodes_as_null=False, set_to_lowercase=True, valid_postcode_regex='^[A-Za-z]{1,2}[0-9][A-Za-z0-9]? [0-9][A-Za-z]{2}$', term_frequency_adjustments_full=False, include_full_match_level=True, include_sector_match_level=True, include_district_match_level=True, include_area_match_level=True, lat_col=None, long_col=None, km_thresholds=[], m_probability_full_match=None, m_probability_sector_match=None, m_probability_district_match=None, m_probability_area_match=None, m_probability_or_probabilities_km_distance=None, m_probability_else=None)</code>","text":"<p>A wrapper to generate a comparison for a poscode column 'col_name'     with preselected defaults.</p> <p>The default arguments will give a comparison with levels:</p> <ul> <li> <p>Exact match on full postcode</p> </li> <li> <p>Exact match on sector</p> </li> <li> <p>Exact match on district</p> </li> <li> <p>Exact match on area</p> </li> <li> <p>All other comparisons</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The name of the column to compare.</p> required <code>invalid_postcodes_as_null</code> <code>bool</code> <p>If True, postcodes that do not adhere to valid_postcode_regex will be included in the null level. Defaults to False</p> <code>False</code> <code>set_to_lowercase</code> <code>bool</code> <p>If True, all postcodes are set to lowercase during the pairwise comparisons. Defaults to True</p> <code>True</code> <code>valid_postcode_regex</code> <code>str</code> <p>regular expression pattern that is used to validate postcodes. If invalid_postcodes_as_null is True, postcodes that do not adhere to valid_postcode_regex will be included  in the null level.  Defaults to \"^[A-Za-z]{1,2}0-9? 0-9$\"</p> <code>'^[A-Za-z]{1,2}[0-9][A-Za-z0-9]? [0-9][A-Za-z]{2}$'</code> <code>term_frequency_adjustments_full</code> <code>bool</code> <p>If True, apply term frequency adjustments to the full postcode exact match level. Defaults to False.</p> <code>False</code> <code>include_full_match_level</code> <code>bool</code> <p>If True, include an exact match on full postcode level. Defaults to True.</p> <code>True</code> <code>include_sector_match_level</code> <code>bool</code> <p>If True, include an exact match on sector level. Defaults to True.</p> <code>True</code> <code>include_district_match_level</code> <code>bool</code> <p>If True, include an exact match on district level. Defaults to True.</p> <code>True</code> <code>include_area_match_level</code> <code>bool</code> <p>If True, include an exact match on area level. Defaults to True.</p> <code>True</code> <code>include_distance_in_km_level</code> <code>bool</code> <p>If True, include a comparison of distance between postcodes as measured in kilometers. Defaults to False.</p> required <code>lat_col</code> <code>str</code> <p>The name of a latitude column or the respective array or struct column column containing the information, plus an index. For example: lat, long_lat['lat'] or long_lat[0].</p> <code>None</code> <code>long_col</code> <code>str</code> <p>The name of a longitudinal column or the respective array or struct column column containing the information, plus an index. For example: long, long_lat['long'] or long_lat[1].</p> <code>None</code> <code>km_thresholds</code> <code>int, float, list</code> <p>The total distance in kilometers to evaluate the distance_in_km_level comparison against.</p> <code>[]</code> <code>m_probability_full_match</code> <code>float</code> <p>Starting m probability for full match level. Defaults to None.</p> <code>None</code> <code>m_probability_sector_match</code> <code>float</code> <p>Starting m probability for sector match level. Defaults to None.</p> <code>None</code> <code>m_probability_district_match</code> <code>float</code> <p>Starting m probability for district match level. Defaults to None.</p> <code>None</code> <code>m_probability_area_match</code> <code>float</code> <p>Starting m probability for area match level. Defaults to None.</p> <code>None</code> <code>m_probability_or_probabilities_km_distance</code> <code>float</code> <p>Starting m probability for 'distance in km' level. Defaults to None.</p> <code>None</code> <code>m_probability_else</code> <code>float</code> <p>Starting m probability for the 'everything else' level. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSparkAthena <p>Basic Postcode Comparison <pre><code>import splink.duckdb.duckdb_comparison_template_library as ctl\nctl.postcode_comparison(\"postcode\")\n</code></pre> Bespoke Postcode Comparison <pre><code>import splink.duckdb.duckdb_comparison_template_library as ctl\nctl.postcode_comparison(\"postcode\",\n                    invalid_postcodes_as_null=True,\n                    include_distance_in_km_level=True,\n                    lat_col=\"lat\",\n                    long_col=\"long\",\n                    km_thresholds=[10, 100]\n                    )\n</code></pre></p> <p>Basic Postcode Comparison <pre><code>import splink.spark.spark_comparison_template_library as ctl\nctl.postcode_comparison(\"postcode\")\n</code></pre> Bespoke Postcode Comparison <pre><code>import splink.spark.spark_comparison_template_library as ctl\nctl.postcode_comparison(\"postcode\",\n                    invalid_postcodes_as_null=True,\n                    include_distance_in_km_level=True,\n                    lat_col=\"lat\",\n                    long_col=\"long\",\n                    km_thresholds=[10, 100]\n                    )\n</code></pre></p> <p>Basic Postcode Comparison <pre><code>import splink.athean.athena_comparison_template_library as ctl\nctl.postcode_comparison(\"postcode\")\n</code></pre> Bespoke Postcode Comparison <pre><code>import splink.athena.athena_comparison_template_library as ctl\nctl.postcode_comparison(\"postcode\",\n                    invalid_postcodes_as_null=True,\n                    include_distance_in_km_level=True,\n                    lat_col=\"lat\",\n                    long_col=\"long\",\n                    km_thresholds=[10, 100]\n                    )\n</code></pre></p> <p>Returns:</p> Name Type Description <code>Comparison</code> <code>Comparison</code> <p>A comparison that can be inclued in the Splink settings dictionary.</p>","tags":["API","comparisons","Date Comparison"]},{"location":"em_training_session.html","title":"Documentation for <code>EMTrainingSession</code> object","text":"<p>Manages training models using the Expectation Maximisation algorithm, and holds statistics on the evolution of parameter estimates.  Plots diagnostic charts</p> Source code in <code>splink/em_training_session.py</code> <pre><code>class EMTrainingSession:\n\"\"\"Manages training models using the Expectation Maximisation algorithm, and\n    holds statistics on the evolution of parameter estimates.  Plots diagnostic charts\n    \"\"\"\n\n    def __init__(\n        self,\n        linker: Linker,\n        blocking_rule_for_training: str,\n        fix_u_probabilities: bool = False,\n        fix_m_probabilities: bool = False,\n        fix_probability_two_random_records_match: bool = False,\n        comparisons_to_deactivate: list[Comparison] = None,\n        comparison_levels_to_reverse_blocking_rule: list[ComparisonLevel] = None,\n    ):\n        logger.info(\"\\n----- Starting EM training session -----\\n\")\n\n        self._original_settings_obj = linker._settings_obj\n        self._original_linker = linker\n        self._training_linker = deepcopy(linker)\n\n        self._settings_obj = self._training_linker._settings_obj\n        self._settings_obj._retain_matching_columns = False\n        self._settings_obj._retain_intermediate_calculation_columns = False\n        self._settings_obj._training_mode = True\n\n        if not isinstance(blocking_rule_for_training, BlockingRule):\n            blocking_rule = BlockingRule(blocking_rule_for_training)\n\n        self._settings_obj._blocking_rule_for_training = blocking_rule\n        self._blocking_rule_for_training = blocking_rule\n\n        if comparison_levels_to_reverse_blocking_rule:\n            self._comparison_levels_to_reverse_blocking_rule = (\n                comparison_levels_to_reverse_blocking_rule\n            )\n        else:\n            self._comparison_levels_to_reverse_blocking_rule = self._original_settings_obj._get_comparison_levels_corresponding_to_training_blocking_rule(  # noqa\n                blocking_rule_for_training\n            )\n\n        self._settings_obj._probability_two_random_records_match = (\n            self._blocking_adjusted_probability_two_random_records_match\n        )\n\n        self._training_fix_u_probabilities = fix_u_probabilities\n        self._training_fix_m_probabilities = fix_m_probabilities\n        self._training_fix_probability_two_random_records_match = (\n            fix_probability_two_random_records_match\n        )\n\n        # Remove comparison columns which are either 'used up' by the blocking rules\n        # or alternatively, if the user has manually provided a list to remove,\n        # use this instead\n        if not comparisons_to_deactivate:\n            comparisons_to_deactivate = []\n            br_cols = get_columns_used_from_sql(\n                blocking_rule_for_training, self._settings_obj._sql_dialect\n            )\n            for cc in self._settings_obj.comparisons:\n                cc_cols = cc._input_columns_used_by_case_statement\n                cc_cols = [c.input_name for c in cc_cols]\n                if set(br_cols).intersection(cc_cols):\n                    comparisons_to_deactivate.append(cc)\n        cc_names_to_deactivate = [\n            cc._output_column_name for cc in comparisons_to_deactivate\n        ]\n        self._comparisons_that_cannot_be_estimated: list[\n            Comparison\n        ] = comparisons_to_deactivate\n\n        filtered_ccs = [\n            cc\n            for cc in self._settings_obj.comparisons\n            if cc._output_column_name not in cc_names_to_deactivate\n        ]\n\n        self._settings_obj.comparisons = filtered_ccs\n        self._comparisons_that_can_be_estimated = filtered_ccs\n\n        self._settings_obj_history = []\n\n        # Add iteration 0 i.e. the starting parameters\n        self._add_iteration()\n\n    def _training_log_message(self):\n        not_estimated = [\n            cc._output_column_name for cc in self._comparisons_that_cannot_be_estimated\n        ]\n        not_estimated = \"\".join([f\"\\n    - {cc}\" for cc in not_estimated])\n\n        estimated = [\n            cc._output_column_name for cc in self._comparisons_that_can_be_estimated\n        ]\n        estimated = \"\".join([f\"\\n    - {cc}\" for cc in estimated])\n\n        if self._training_fix_m_probabilities and self._training_fix_u_probabilities:\n            raise ValueError(\"Can't train model if you fix both m and u probabilites\")\n        elif self._training_fix_u_probabilities:\n            mu = \"m probabilities\"\n        elif self._training_fix_m_probabilities:\n            mu = \"u probabilities\"\n        else:\n            mu = \"m and u probabilities\"\n\n        blocking_rule = self._blocking_rule_for_training.blocking_rule\n\n        logger.info(\n            f\"Estimating the {mu} of the model by blocking on:\\n\"\n            f\"{blocking_rule}\\n\\n\"\n            \"Parameter estimates will be made for the following comparison(s):\"\n            f\"{estimated}\\n\"\n            \"\\nParameter estimates cannot be made for the following comparison(s)\"\n            f\" since they are used in the blocking rules: {not_estimated}\"\n        )\n\n    def _comparison_vectors(self):\n        self._training_log_message()\n\n        nodes_with_tf = self._original_linker._initialise_df_concat_with_tf()\n\n        sql = block_using_rules_sql(self._training_linker)\n        self._training_linker._enqueue_sql(sql, \"__splink__df_blocked\")\n\n        # repartition after blocking only exists on the SparkLinker\n        repartition_after_blocking = getattr(\n            self._original_linker, \"repartition_after_blocking\", False\n        )\n\n        if repartition_after_blocking:\n            df_blocked = self._training_linker._execute_sql_pipeline([nodes_with_tf])\n            input_dataframes = [nodes_with_tf, df_blocked]\n        else:\n            input_dataframes = [nodes_with_tf]\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._training_linker._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n        return self._training_linker._execute_sql_pipeline(input_dataframes)\n\n    def _train(self):\n        cvv = self._comparison_vectors()\n\n        # check that the blocking rule actually generates _some_ record pairs,\n        # if not give the user a helpful message\n        if not cvv.as_record_dict(limit=1):\n            br_sql = f\"`{self._blocking_rule_for_training.blocking_rule}`\"\n            raise EMTrainingException(\n                f\"Training rule {br_sql} resulted in no record pairs.  \"\n                \"This means that in the supplied data set \"\n                f\"there were no pairs of records for which {br_sql} was `true`.\\n\"\n                \"Expectation maximisation requires a substantial number of record \"\n                \"comparisons to produce accurate parameter estimates - usually \"\n                \"at least a few hundred, but preferably at least a few thousand.\\n\"\n                \"You must revise your training blocking rule so that the set of \"\n                \"generated comparisons is not empty.  You can use \"\n                \"`linker.count_num_comparisons_from_blocking_rule()` to compute \"\n                \"the number of comparisons that will be generated by a blocking rule.\"\n            )\n\n        # Compute the new params, populating the paramters in the copied settings object\n        # At this stage, we do not overwrite any of the parameters\n        # in the original (main) setting object\n        expectation_maximisation(self, cvv)\n\n        rule = self._blocking_rule_for_training.blocking_rule\n        training_desc = f\"EM, blocked on: {rule}\"\n\n        # Add m and u values to original settings\n        for cc in self._settings_obj.comparisons:\n            orig_cc = self._original_settings_obj._get_comparison_by_output_column_name(\n                cc._output_column_name\n            )\n            for cl in cc._comparison_levels_excluding_null:\n                orig_cl = orig_cc._get_comparison_level_by_comparison_vector_value(\n                    cl._comparison_vector_value\n                )\n\n                if not self._training_fix_m_probabilities:\n                    not_observed = LEVEL_NOT_OBSERVED_TEXT\n                    if cl._m_probability == not_observed:\n                        orig_cl._add_trained_m_probability(not_observed, training_desc)\n                        logger.info(\n                            f\"m probability not trained for {cc._output_column_name} - \"\n                            f\"{cl.label_for_charts} (comparison vector value: \"\n                            f\"{cl._comparison_vector_value}). This usually means the \"\n                            \"comparison level was never observed in the training data.\"\n                        )\n                    else:\n                        orig_cl._add_trained_m_probability(\n                            cl.m_probability, training_desc\n                        )\n\n                if not self._training_fix_u_probabilities:\n                    not_observed = LEVEL_NOT_OBSERVED_TEXT\n                    if cl._u_probability == not_observed:\n                        orig_cl._add_trained_u_probability(not_observed, training_desc)\n                        logger.info(\n                            f\"u probability not trained for {cc._output_column_name} - \"\n                            f\"{cl.label_for_charts} (comparison vector value: \"\n                            f\"{cl._comparison_vector_value}). This usually means the \"\n                            \"comparison level was never observed in the training data.\"\n                        )\n                    else:\n                        orig_cl._add_trained_u_probability(\n                            cl.u_probability, training_desc\n                        )\n\n        self._original_linker._em_training_sessions.append(self)\n\n    def _add_iteration(self):\n        self._settings_obj_history.append(deepcopy(self._settings_obj))\n\n    @property\n    def _blocking_adjusted_probability_two_random_records_match(self):\n        orig_prop_m = self._original_settings_obj._probability_two_random_records_match\n\n        adj_bayes_factor = prob_to_bayes_factor(orig_prop_m)\n\n        logger.log(15, f\"Original prob two random records match: {orig_prop_m:.3f}\")\n\n        comp_levels = self._comparison_levels_to_reverse_blocking_rule\n        if not comp_levels:\n            comp_levels = self._original_settings_obj._get_comparison_levels_corresponding_to_training_blocking_rule(  # noqa\n                self._blocking_rule_for_training.blocking_rule\n            )\n\n        for cl in comp_levels:\n            adj_bayes_factor = cl._bayes_factor * adj_bayes_factor\n\n            logger.log(\n                15,\n                f\"Increasing prob two random records match using \"\n                f\"{cl.comparison._output_column_name} - {cl.label_for_charts}\"\n                f\" using bayes factor {cl._bayes_factor:,.3f}\",\n            )\n\n        adjusted_prop_m = bayes_factor_to_prob(adj_bayes_factor)\n        logger.log(\n            15,\n            f\"\\nProb two random records match adjusted for blocking on \"\n            f\"{self._blocking_rule_for_training.blocking_rule}: \"\n            f\"{adjusted_prop_m:.3f}\",\n        )\n        return adjusted_prop_m\n\n    @property\n    def _iteration_history_records(self):\n        output_records = []\n\n        for iteration, settings_obj in enumerate(self._settings_obj_history):\n            records = settings_obj._parameters_as_detailed_records\n\n            for r in records:\n                r[\"iteration\"] = iteration\n                r[\n                    \"probability_two_random_records_match\"\n                ] = self._settings_obj._probability_two_random_records_match\n\n            output_records.extend(records)\n        return output_records\n\n    @property\n    def _lambda_history_records(self):\n        output_records = []\n        for i, s in enumerate(self._settings_obj_history):\n            lam = s._probability_two_random_records_match\n            r = {\n                \"probability_two_random_records_match\": lam,\n                \"probability_two_random_records_match_reciprocal\": 1 / lam,\n                \"iteration\": i,\n            }\n\n            output_records.append(r)\n        return output_records\n\n    def probability_two_random_records_match_iteration_chart(self):\n        records = self._lambda_history_records\n        return probability_two_random_records_match_iteration_chart(records)\n\n    def match_weights_interactive_history_chart(self):\n        records = self._iteration_history_records\n        return match_weights_interactive_history_chart(\n            records, blocking_rule=self._blocking_rule_for_training\n        )\n\n    def m_u_values_interactive_history_chart(self):\n        records = self._iteration_history_records\n        return m_u_parameters_interactive_history_chart(records)\n\n    def _max_change_message(self, max_change_dict):\n        message = \"Largest change in params was\"\n\n        if max_change_dict[\"max_change_type\"] == \"probability_two_random_records_match\":\n            message = (\n                f\"{message} {max_change_dict['max_change_value']:,.3g} in \"\n                \"probability_two_random_records_match\"\n            )\n        else:\n            cl = max_change_dict[\"current_comparison_level\"]\n            m_u = max_change_dict[\"max_change_type\"]\n            cc_name = cl.comparison._output_column_name\n\n            cl_label = cl.label_for_charts\n            level_text = f\"{cc_name}, level `{cl_label}`\"\n\n            message = (\n                f\"{message} {max_change_dict['max_change_value']:,.3g} in \"\n                f\"the {m_u} of {level_text}\"\n            )\n\n        return message\n\n    def _max_change_in_parameters_comparison_levels(self):\n        previous_iteration = self._settings_obj_history[-2]\n        this_iteration = self._settings_obj_history[-1]\n        max_change = -0.1\n\n        max_change_levels = {\n            \"previous_iteration\": None,\n            \"this_iteration\": None,\n            \"max_change_type\": None,\n            \"max_change_value\": None,\n        }\n        comparisons = zip(previous_iteration.comparisons, this_iteration.comparisons)\n        for comparison in comparisons:\n            prev_cc = comparison[0]\n            this_cc = comparison[1]\n            z_cls = zip(prev_cc.comparison_levels, this_cc.comparison_levels)\n            for z_cl in z_cls:\n                if z_cl[0].is_null_level:\n                    continue\n                prev_cl = z_cl[0]\n                this_cl = z_cl[1]\n                change_m = this_cl.m_probability - prev_cl.m_probability\n                change_u = this_cl.u_probability - prev_cl.u_probability\n                change = max(abs(change_m), abs(change_u))\n                change_type = (\n                    \"m_probability\"\n                    if abs(change_m) &gt; abs(change_u)\n                    else \"u_probability\"\n                )\n                change_value = change_m if abs(change_m) &gt; abs(change_u) else change_u\n                if change &gt; max_change:\n                    max_change = change\n                    max_change_levels[\"prev_comparison_level\"] = prev_cl\n                    max_change_levels[\"current_comparison_level\"] = this_cl\n                    max_change_levels[\"max_change_type\"] = change_type\n                    max_change_levels[\"max_change_value\"] = change_value\n                    max_change_levels[\"max_abs_change_value\"] = abs(change_value)\n\n        change_probability_two_random_records_match = (\n            this_iteration._probability_two_random_records_match\n            - previous_iteration._probability_two_random_records_match\n        )\n\n        if abs(change_probability_two_random_records_match) &gt; max_change:\n            max_change = abs(change_probability_two_random_records_match)\n            max_change_levels[\"prev_comparison_level\"] = None\n            max_change_levels[\"current_comparison_level\"] = None\n            max_change_levels[\n                \"max_change_type\"\n            ] = \"probability_two_random_records_match\"\n            max_change_levels[\n                \"max_change_value\"\n            ] = change_probability_two_random_records_match\n            max_change_levels[\"max_abs_change_value\"] = abs(\n                change_probability_two_random_records_match\n            )\n\n        max_change_levels[\"message\"] = self._max_change_message(max_change_levels)\n\n        return max_change_levels\n\n    def __repr__(self):\n        deactivated_cols = \", \".join(\n            [\n                cc._output_column_name\n                for cc in self._comparisons_that_cannot_be_estimated\n            ]\n        )\n        blocking_rule = self._blocking_rule_for_training.blocking_rule\n        return (\n            f\"&lt;EMTrainingSession, blocking on {blocking_rule}, \"\n            f\"deactivating comparisons {deactivated_cols}&gt;\"\n        )\n</code></pre>","tags":["API","Expectation Maximisation","Model Training"]},{"location":"em_training_session.html#splink.em_training_session.EMTrainingSession.match_weights_interactive_history_chart","title":"<code>match_weights_interactive_history_chart()</code>","text":"Source code in <code>splink/em_training_session.py</code> <pre><code>def match_weights_interactive_history_chart(self):\n    records = self._iteration_history_records\n    return match_weights_interactive_history_chart(\n        records, blocking_rule=self._blocking_rule_for_training\n    )\n</code></pre>","tags":["API","Expectation Maximisation","Model Training"]},{"location":"em_training_session.html#splink.em_training_session.EMTrainingSession.m_u_values_interactive_history_chart","title":"<code>m_u_values_interactive_history_chart()</code>","text":"Source code in <code>splink/em_training_session.py</code> <pre><code>def m_u_values_interactive_history_chart(self):\n    records = self._iteration_history_records\n    return m_u_parameters_interactive_history_chart(records)\n</code></pre>","tags":["API","Expectation Maximisation","Model Training"]},{"location":"examples_index.html","title":"Examples index","text":"","tags":["Examples","DuckDB","Spark"]},{"location":"examples_index.html#examples","title":"Examples","text":"<p>This page provides a series of examples to help you get started with splink. You can find the underlying notebooks at the splink_demos repo.</p> <p>You can try these demos live in your web browser using the following link:</p> <p></p>","tags":["Examples","DuckDB","Spark"]},{"location":"examples_index.html#duckdb-examples","title":"DuckDB examples","text":"","tags":["Examples","DuckDB","Spark"]},{"location":"examples_index.html#entity-type-persons","title":"Entity type: Persons","text":"<p>Deduplicating 50,000 records of realistic data based on historical persons</p> <p>Using the <code>link_only</code> setting to link, but not dedupe, two datasets</p> <p>Real time record linkage</p> <p>Accuracy analysis and ROC charts using a ground truth (cluster) column</p> <p>Estimating m probabilities from pairwise labels</p> <p>Deduplicating the febrl3 dataset. Note this dataset comes from febrl, as referenced in A.2 here and replicated here.</p> <p>Linking the febrl4 datasets. As above, these datasets are from febrl, replicated here.</p>","tags":["Examples","DuckDB","Spark"]},{"location":"examples_index.html#entity-type-financial-transactions","title":"Entity type: Financial transactions","text":"<p>Linking financial transactions</p>","tags":["Examples","DuckDB","Spark"]},{"location":"examples_index.html#pyspark-examples","title":"PySpark examples","text":"<p>Deduplication of a small dataset using Pyspark. Entity type is persons.</p>","tags":["Examples","DuckDB","Spark"]},{"location":"installations.html","title":"Install","text":"<p>Splink supports python 3.7+.</p> <p>To obtain the latest released version of splink you can install from PyPI using pip: <pre><code>pip install splink\n</code></pre></p> <p>or if you prefer, you can instead install splink using conda: <pre><code>conda install -c conda-forge splink\n</code></pre></p>"},{"location":"installations.html#duckdb-less-installation","title":"DuckDB-less Installation","text":"<p>Should you be unable to install <code>DuckDB</code> to your local machine, you can still run <code>Splink</code> without the <code>DuckDB</code> dependency using a small workaround.</p> <p>To start, install the latest released version of splink from PyPI without any dependencies using: <pre><code>pip install splink --no-deps\n</code></pre></p> <p>Then, to install the remaining requirements, download the following <code>requirements.txt</code> from our github repository using: <pre><code>github_url=\"https://raw.githubusercontent.com/moj-analytical-services/splink/master/scripts/duckdbless_requirements.txt\"\noutput_file=\"splink_requirements.txt\"\n\n# Download the file from GitHub using curl\ncurl -o \"$output_file\" \"$github_url\"\n</code></pre></p> <p>Or, if you're either unable to download it directly from github or you'd rather create the file manually, simply:</p> <ol> <li>Create a file called <code>splink_requirements.txt</code></li> <li>Copy and paste the contents from our duckdbless requirements file into your file.</li> </ol> <p>Finally, run the following command within your virtual environment to install the remaining splink dependencies: <pre><code>pip install -r splink_requirements.txt\n</code></pre></p>"},{"location":"linker.html","title":"Documentation for <code>Linker</code> object","text":"<p>The Linker object manages the data linkage process and holds the data linkage model.</p> <p>Most of Splink's functionality can  be accessed by calling methods (functions) on the linker, such as <code>linker.predict()</code>, <code>linker.profile_columns()</code> etc.</p> <p>The Linker class is intended for subclassing for specific backends, e.g. a <code>DuckDBLinker</code>.</p> Source code in <code>splink/linker.py</code> <pre><code>class Linker:\n\"\"\"The Linker object manages the data linkage process and holds the data linkage\n    model.\n\n    Most of Splink's functionality can  be accessed by calling methods (functions)\n    on the linker, such as `linker.predict()`, `linker.profile_columns()` etc.\n\n    The Linker class is intended for subclassing for specific backends, e.g.\n    a `DuckDBLinker`.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_table_or_tables: str | list,\n        settings_dict: dict | Path,\n        accepted_df_dtypes,\n        set_up_basic_logging: bool = True,\n        input_table_aliases: str | list = None,\n    ):\n\"\"\"Initialise the linker object, which manages the data linkage process and\n        holds the data linkage model.\n\n        Examples:\n            === \"DuckDB\"\n                Dedupe\n                ```py\n                df = pd.read_csv(\"data_to_dedupe.csv\")\n                linker = DuckDBLinker(df, settings_dict)\n                ```\n                Link\n                ```py\n                df_1 = pd.read_parquet(\"table_1/\")\n                df_2 = pd.read_parquet(\"table_2/\")\n                linker = DuckDBLinker(\n                    [df_1, df_2],\n                    settings_dict,\n                    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                    )\n                ```\n                Dedupe with a pre-trained model read from a json file\n                ```py\n                df = pd.read_csv(\"data_to_dedupe.csv\")\n                linker = DuckDBLinker(df, \"model.json\")\n                ```\n            === \"Spark\"\n                Dedupe\n                ```py\n                df = spark.read.csv(\"data_to_dedupe.csv\")\n                linker = SparkLinker(df, settings_dict)\n                ```\n                Link\n                ```py\n                df_1 = spark.read.parquet(\"table_1/\")\n                df_2 = spark.read.parquet(\"table_2/\")\n                linker = SparkLinker(\n                    [df_1, df_2],\n                    settings_dict,\n                    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                    )\n                ```\n                Dedupe with a pre-trained model read from a json file\n                ```py\n                df = spark.read.csv(\"data_to_dedupe.csv\")\n                linker = SparkLinker(df, \"model.json\")\n                ```\n\n        Args:\n            input_table_or_tables (Union[str, list]): Input data into the linkage model.\n                Either a single string (the name of a table in a database) for\n                deduplication jobs, or a list of strings  (the name of tables in a\n                database) for link_only or link_and_dedupe.  For some linkers, such as\n                the DuckDBLinker and the SparkLinker, it's also possible to pass in\n                dataframes (Pandas and Spark respectively) rather than strings.\n            settings_dict (dict | Path, optional): A Splink settings dictionary, or a\n                path to a json defining a settingss dictionary or pre-trained model.\n                If not provided when the object is created, can later be added using\n                `linker.load_settings()` or `linker.load_model()` Defaults to None.\n            set_up_basic_logging (bool, optional): If true, sets ups up basic logging\n                so that Splink sends messages at INFO level to stdout. Defaults to True.\n            input_table_aliases (Union[str, list], optional): Labels assigned to\n                input tables in Splink outputs.  If the names of the tables in the\n                input database are long or unspecific, this argument can be used\n                to attach more easily readable/interpretable names. Defaults to None.\n        \"\"\"\n\n        if set_up_basic_logging:\n            logging.basicConfig(\n                format=\"%(message)s\",\n            )\n            splink_logger = logging.getLogger(\"splink\")\n            splink_logger.setLevel(logging.INFO)\n\n        self._pipeline = SQLPipeline()\n\n        self._names_of_tables_created_by_splink: set = set()\n        self._intermediate_table_cache: dict = CacheDictWithLogging()\n\n        if not isinstance(settings_dict, (dict, type(None))):\n            # Run if you've entered a filepath\n            # feed it a blank settings dictionary\n            self._setup_settings_objs(None)\n            self.load_settings(settings_dict)\n        else:\n            settings_dict = deepcopy(settings_dict)\n            self._setup_settings_objs(settings_dict)\n\n        homogenised_tables, homogenised_aliases = self._register_input_tables(\n            input_table_or_tables,\n            input_table_aliases,\n            accepted_df_dtypes,\n        )\n\n        self._input_tables_dict = self._get_input_tables_dict(\n            homogenised_tables, homogenised_aliases\n        )\n\n        self._validate_input_dfs()\n        self._em_training_sessions = []\n\n        self._find_new_matches_mode = False\n        self._train_u_using_random_sample_mode = False\n        self._compare_two_records_mode = False\n        self._self_link_mode = False\n        self._analyse_blocking_mode = False\n        self._deterministic_link_mode = False\n\n        self.debug_mode = False\n\n    @property\n    def _cache_uid(self):\n        if self._settings_dict:\n            return self._settings_obj._cache_uid\n        else:\n            return self._cache_uid_no_settings\n\n    @_cache_uid.setter\n    def _cache_uid(self, value):\n        if self._settings_dict:\n            self._settings_obj._cache_uid = value\n        else:\n            self._cache_uid_no_settings = value\n\n    @property\n    def _settings_obj(self) -&gt; Settings:\n        if self._settings_obj_ is None:\n            raise ValueError(\n                \"You did not provide a settings dictionary when you \"\n                \"created the linker.  To continue, you need to provide a settings \"\n                \"dictionary using the `load_settings()` method on your linker \"\n                \"object. i.e. linker.load_settings(settings_dict)\"\n            )\n        return self._settings_obj_\n\n    @property\n    def _input_tablename_l(self):\n        if self._find_new_matches_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._self_link_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._compare_two_records_mode:\n            return \"__splink__compare_two_records_left_with_tf\"\n\n        if self._train_u_using_random_sample_mode:\n            return \"__splink__df_concat_with_tf_sample\"\n\n        if self._analyse_blocking_mode:\n            return \"__splink__df_concat\"\n\n        if self._two_dataset_link_only:\n            return \"__splink__df_concat_with_tf_left\"\n\n        return \"__splink__df_concat_with_tf\"\n\n    @property\n    def _input_tablename_r(self):\n        if self._find_new_matches_mode:\n            return \"__splink__df_new_records_with_tf\"\n\n        if self._self_link_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._compare_two_records_mode:\n            return \"__splink__compare_two_records_right_with_tf\"\n\n        if self._train_u_using_random_sample_mode:\n            return \"__splink__df_concat_with_tf_sample\"\n\n        if self._analyse_blocking_mode:\n            return \"__splink__df_concat\"\n\n        if self._two_dataset_link_only:\n            return \"__splink_df_concat_with_tf_right\"\n        return \"__splink__df_concat_with_tf\"\n\n    @property\n    def _source_dataset_column_name(self):\n        if self._settings_obj_ is None:\n            return None\n\n        # Used throughout the scripts to feed our SQL\n        if self._settings_obj._source_dataset_column_name_is_required:\n            df_obj = next(iter(self._input_tables_dict.values()))\n            columns = df_obj.columns_escaped\n\n            input_column, src_ds_col = self._settings_obj_._source_dataset_col\n            return \"__splink_source_dataset\" if src_ds_col in columns else input_column\n        else:\n            return None\n\n    @property\n    def _two_dataset_link_only(self):\n        # Two dataset link only join is a special case where an inner join of the\n        # two datasets is much more efficient than self-joining the vertically\n        # concatenation of all input datasets\n        if self._find_new_matches_mode:\n            return True\n\n        if self._compare_two_records_mode:\n            return True\n\n        # in u-train sample mode we are joining the concatenated table mixing\n        # both data sets - hence if we inner join on True we will end up with\n        # samples which both originate from the same dataset\n        if self._train_u_using_random_sample_mode:\n            return False\n\n        if self._analyse_blocking_mode:\n            return False\n\n        if (\n            len(self._input_tables_dict) == 2\n            and self._settings_obj._link_type == \"link_only\"\n        ):\n            return True\n        else:\n            return False\n\n    @property\n    def _sql_dialect(self):\n        if self._sql_dialect_ is None:\n            raise NotImplementedError(\n                f\"No SQL dialect set on object of type {type(self)}. \"\n                \"Did you make sure to create a dialect-specific Linker?\"\n            )\n        return self._sql_dialect_\n\n    @property\n    def _infinity_expression(self):\n        raise NotImplementedError(\n            f\"infinity sql expression not available for {type(self)}\"\n        )\n\n    @property\n    def _verify_link_only_job(self):\n        cache = self._intermediate_table_cache\n        if \"__splink__df_concat_with_tf\" not in cache:\n            return\n\n        if self._settings_obj._link_type == \"link_only\":\n            # if input datasets &gt; 1 then skip\n            if len(self._input_tables_dict) &gt; 1:\n                return\n\n            # else, check if source dataset column is populated...\n            src_ds = self._source_dataset_column_name\n            if src_ds == \"__splink_source_dataset\":\n                _, src_ds = self._settings_obj_._source_dataset_col\n\n            sql = find_unique_source_dataset(src_ds)\n            self._enqueue_sql(sql, \"source_ds_distinct\")\n            src_ds_distinct = self._execute_sql_pipeline(\n                [cache[\"__splink__df_concat_with_tf\"]]\n            )\n            if len(src_ds_distinct.as_record_dict()) == 1:\n                raise SplinkException(\n                    \"if `link_type` is `link_only`, it should have at least two \"\n                    \"input dataframes, or one dataframe with a `source_dataset` \"\n                    \"column outlining which dataset each record belongs to.\"\n                )\n\n    def _register_input_tables(self, input_tables, input_aliases, accepted_df_dtypes):\n        # 'homogenised' means all entries are strings representing tables\n        homogenised_tables = []\n        homogenised_aliases = []\n        accepted_df_dtypes = ensure_is_tuple(accepted_df_dtypes)\n\n        existing_tables = []\n        for alias in input_aliases:\n            # Check if alias is a string (indicating a table name) and that it is not\n            # a file path.\n            if not isinstance(alias, str) or re.match(pattern=r\".*\", string=alias):\n                continue\n            exists = self._table_exists_in_database(alias)\n            if exists:\n                existing_tables.append(f\"'{alias}'\")\n        if existing_tables:\n            input_tables = \", \".join(existing_tables)\n            raise ValueError(\n                f\"Table(s): {input_tables} already exists in database. \"\n                \"Please remove or rename it/them before retrying\"\n            )\n\n        for i, (table, alias) in enumerate(zip(input_tables, input_aliases)):\n            if isinstance(alias, accepted_df_dtypes):\n                alias = f\"__splink__input_table_{i}\"\n\n            if isinstance(table, accepted_df_dtypes):\n                self._table_registration(table, alias)\n                table = alias\n\n            homogenised_tables.append(table)\n            homogenised_aliases.append(alias)\n\n        return homogenised_tables, homogenised_aliases\n\n    def _setup_settings_objs(self, settings_dict):\n        # Setup the linker class's required settings\n        self._settings_dict = settings_dict\n\n        # if settings_dict is passed, set sql_dialect on it if missing, and make sure\n        # incompatible dialect not passed\n        if settings_dict is not None and settings_dict.get(\"sql_dialect\", None) is None:\n            settings_dict[\"sql_dialect\"] = self._sql_dialect\n\n        if settings_dict is None:\n            self._cache_uid_no_settings = ascii_uid(8)\n        else:\n            uid = settings_dict.get(\"linker_uid\", ascii_uid(8))\n            settings_dict[\"linker_uid\"] = uid\n\n        if settings_dict is None:\n            self._settings_obj_ = None\n        else:\n            self._settings_obj_ = Settings(settings_dict)\n\n            self._validate_dialect()\n\n    def _initialise_df_concat(self, materialise=False):\n        cache = self._intermediate_table_cache\n        concat_df = None\n        if \"__splink__df_concat\" in cache:\n            concat_df = cache[\"__splink__df_concat\"]\n        elif \"__splink__df_concat_with_tf\" in cache:\n            concat_df = cache[\"__splink__df_concat_with_tf\"]\n            concat_df.templated_name = \"__splink__df_concat\"\n        else:\n            if materialise:\n                # Clear the pipeline if we are materialising\n                # There's no reason not to do this, since when\n                # we execute the pipeline, it'll get cleared anyway\n                self._pipeline.reset()\n            sql = vertically_concatenate_sql(self)\n            self._enqueue_sql(sql, \"__splink__df_concat\")\n            if materialise:\n                concat_df = self._execute_sql_pipeline()\n                cache[\"__splink__df_concat\"] = concat_df\n\n        return concat_df\n\n    def _initialise_df_concat_with_tf(self, materialise=True):\n        cache = self._intermediate_table_cache\n        nodes_with_tf = None\n        if \"__splink__df_concat_with_tf\" in cache:\n            nodes_with_tf = cache[\"__splink__df_concat_with_tf\"]\n\n        else:\n            if materialise:\n                # Clear the pipeline if we are materialising\n                # There's no reason not to do this, since when\n                # we execute the pipeline, it'll get cleared anyway\n                self._pipeline.reset()\n\n            sql = vertically_concatenate_sql(self)\n            self._enqueue_sql(sql, \"__splink__df_concat\")\n\n            sqls = compute_all_term_frequencies_sqls(self)\n            for sql in sqls:\n                self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n            if materialise:\n                nodes_with_tf = self._execute_sql_pipeline()\n                cache[\"__splink__df_concat_with_tf\"] = nodes_with_tf\n\n        # verify the link job\n        if self._settings_obj_ is not None:\n            self._verify_link_only_job\n\n        return nodes_with_tf\n\n    def _table_to_splink_dataframe(\n        self, templated_name, physical_name\n    ) -&gt; SplinkDataFrame:\n\"\"\"Create a SplinkDataframe from a table in the underlying database called\n        `physical_name`.\n\n        Associate a `templated_name` with this table, which signifies the purpose\n        or 'meaning' of this table to splink. (e.g. `__splink__df_blocked`)\n\n        Args:\n            templated_name (str): The purpose of the table to Splink\n            physical_name (str): The name of the table in the underlying databse\n        \"\"\"\n        raise NotImplementedError(\n            \"_table_to_splink_dataframe not implemented on this linker\"\n        )\n\n    def _enqueue_sql(self, sql, output_table_name):\n\"\"\"Add sql to the current pipeline, but do not execute the pipeline.\"\"\"\n        self._pipeline.enqueue_sql(sql, output_table_name)\n\n    def _execute_sql_pipeline(\n        self,\n        input_dataframes: list[SplinkDataFrame] = [],\n        materialise_as_hash=True,\n        use_cache=True,\n    ) -&gt; SplinkDataFrame:\n\"\"\"Execute the SQL queued in the current pipeline as a single statement\n        e.g. `with a as (), b as , c as (), select ... from c`, then execute the\n        pipeline, returning the resultant table as a SplinkDataFrame\n\n        Args:\n            input_dataframes (List[SplinkDataFrame], optional): A 'starting point' of\n                SplinkDataFrames if needed. Defaults to [].\n            materialise_as_hash (bool, optional): If true, the output tablename will end\n                in a unique identifer. Defaults to True.\n            use_cache (bool, optional): If true, look at whether the SQL pipeline has\n                been executed before, and if so, use the existing result. Defaults to\n                True.\n\n        Returns:\n            SplinkDataFrame: An abstraction representing the table created by the sql\n                pipeline\n        \"\"\"\n\n        if not self.debug_mode:\n            sql_gen = self._pipeline._generate_pipeline(input_dataframes)\n\n            output_tablename_templated = self._pipeline.queue[-1].output_table_name\n\n            try:\n                dataframe = self._sql_to_splink_dataframe_checking_cache(\n                    sql_gen,\n                    output_tablename_templated,\n                    materialise_as_hash,\n                    use_cache,\n                )\n            except Exception as e:\n                raise e\n            finally:\n                self._pipeline.reset()\n\n            return dataframe\n        else:\n            # In debug mode, we do not pipeline the sql and print the\n            # results of each part of the pipeline\n            for task in self._pipeline._generate_pipeline_parts(input_dataframes):\n                output_tablename = task.output_table_name\n                sql = task.sql\n                print(\"------\")\n                print(f\"--------Creating table: {output_tablename}--------\")\n\n                dataframe = self._sql_to_splink_dataframe_checking_cache(\n                    sql,\n                    output_tablename,\n                    materialise_as_hash=False,\n                    use_cache=False,\n                )\n            self._pipeline.reset()\n            return dataframe\n\n    def _execute_sql_against_backend(\n        self, sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n\"\"\"Execute a single sql SELECT statement, returning a SplinkDataFrame.\n\n        Subclasses should implement this, using _log_and_run_sql_execution() within\n        their implementation, maybe doing some SQL translation or other prep/cleanup\n        work before/after.\n        \"\"\"\n        raise NotImplementedError(\n            f\"_execute_sql_against_backend not implemented for {type(self)}\"\n        )\n\n    def _run_sql_execution(\n        self, final_sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n\"\"\"**Actually** execute the sql against the backend database.\n\n        This is intended to be implemented by a subclass, but not actually called\n        directly. Instead, call _log_and_run_sql_execution, and that will call\n        this method.\n\n        This could return something, or not. It's up to the Linker subclass to decide.\n        \"\"\"\n        raise NotImplementedError(\n            f\"_run_sql_execution not implemented for {type(self)}\"\n        )\n\n    def _log_and_run_sql_execution(\n        self, final_sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n\"\"\"Log the sql, then call _run_sql_execution(), wrapping any errors\"\"\"\n        logger.debug(execute_sql_logging_message_info(templated_name, physical_name))\n        logger.log(5, log_sql(final_sql))\n        try:\n            return self._run_sql_execution(final_sql, templated_name, physical_name)\n        except Exception as e:\n            # Parse our SQL through sqlglot to pretty print\n            try:\n                final_sql = sqlglot.parse_one(\n                    final_sql,\n                    read=self._sql_dialect,\n                ).sql(pretty=True)\n                # if sqlglot produces any errors, just report the raw SQL\n            except Exception:\n                pass\n\n            raise SplinkException(\n                f\"Error executing the following sql for table \"\n                f\"`{templated_name}` ({physical_name}):\\n{final_sql}\"\n            ) from e\n\n    def register_table(self, input, table_name, overwrite=False):\n\"\"\"\n        Register a table to your backend database, to be used in one of the\n        splink methods, or simply to allow querying.\n\n        Tables can be of type: dictionary, record level dictionary,\n        pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n        Examples:\n            ```py\n            test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]}\n            linker.register_table(test_dict, \"test_dict\")\n            linker.query_sql(\"select * from test_dict\")\n            ```\n\n        Args:\n            input: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            table_name (str): The name you wish to assign to the table.\n            overwrite (bool): Overwrite the table in the underlying database if it\n                exists\n\n        Returns:\n            SplinkDataFrame: An abstraction representing the table created by the sql\n                pipeline\n        \"\"\"\n\n        raise NotImplementedError(f\"register_table not implemented for {type(self)}\")\n\n    def _table_registration(self, input, table_name):\n\"\"\"\n        Register a table to your backend database, to be used in one of the\n        splink methods, or simply to allow querying.\n\n        Tables can be of type: dictionary, record level dictionary,\n        pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n        This function is contains no overwrite functionality, so it can be used\n        where we don't want to allow for overwriting.\n\n        Args:\n            input: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            table_name (str): The name you wish to assign to the table.\n\n        Returns:\n            None\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"_table_registration not implemented for {type(self)}\"\n        )\n\n    def query_sql(self, sql, output_type=\"pandas\"):\n\"\"\"\n        Run a SQL query against your backend database and return\n        the resulting output.\n\n        Examples:\n            === \"DuckDB\"\n                ```py\n                linker = DuckDBLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \"Spark\"\n                ```py\n                linker = SparkLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \"Athena\"\n                ```py\n                linker = AthenaLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \"SQLite\"\n                ```py\n                linker = SQLiteLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n\n        Args:\n            sql (str): The SQL to be queried.\n            output_type (str): One of splink_df/splinkdf or pandas.\n                This determines the type of table that your results are output in.\n        \"\"\"\n\n        output_tablename_templated = \"__splink__df_sql_query\"\n\n        splink_dataframe = self._sql_to_splink_dataframe_checking_cache(\n            sql,\n            output_tablename_templated,\n            materialise_as_hash=False,\n            use_cache=False,\n        )\n\n        if output_type in (\"splink_df\", \"splinkdf\"):\n            return splink_dataframe\n        elif output_type == \"pandas\":\n            out = splink_dataframe.as_pandas_dataframe()\n            # If pandas, drop the table to cleanup the db\n            splink_dataframe.drop_table_from_database()\n            return out\n        else:\n            raise ValueError(\n                f\"output_type '{output_type}' is not supported.\",\n                \"Must be one of 'splink_df'/'splinkdf' or 'pandas'\",\n            )\n\n    def _sql_to_splink_dataframe_checking_cache(\n        self,\n        sql,\n        output_tablename_templated,\n        materialise_as_hash=True,\n        use_cache=True,\n    ) -&gt; SplinkDataFrame:\n\"\"\"Execute sql, or if identical sql has been run before, return cached results.\n\n        This function\n            - is used by _execute_sql_pipeline to to execute SQL\n            - or can be used directly if you have a single SQL statement that's\n              not in a pipeline\n\n        Return a SplinkDataFrame representing the results of the SQL\n        \"\"\"\n\n        to_hash = (sql + self._cache_uid).encode(\"utf-8\")\n        hash = hashlib.sha256(to_hash).hexdigest()[:9]\n        # Ensure hash is valid sql table name\n        table_name_hash = f\"{output_tablename_templated}_{hash}\"\n\n        if use_cache:\n            if self._table_exists_in_database(output_tablename_templated):\n                logger.debug(f\"Using existing table {output_tablename_templated}\")\n                return self._table_to_splink_dataframe(\n                    output_tablename_templated, output_tablename_templated\n                )\n\n            if self._table_exists_in_database(table_name_hash):\n                logger.debug(\n                    f\"Using cache for {output_tablename_templated}\"\n                    f\" with physical name {table_name_hash}\"\n                )\n                return self._table_to_splink_dataframe(\n                    output_tablename_templated, table_name_hash\n                )\n\n        if self.debug_mode:\n            print(sql)\n\n        if materialise_as_hash:\n            splink_dataframe = self._execute_sql_against_backend(\n                sql, output_tablename_templated, table_name_hash\n            )\n        else:\n            splink_dataframe = self._execute_sql_against_backend(\n                sql,\n                output_tablename_templated,\n                output_tablename_templated,\n            )\n\n        self._names_of_tables_created_by_splink.add(splink_dataframe.physical_name)\n\n        if self.debug_mode:\n            df_pd = splink_dataframe.as_pandas_dataframe()\n            try:\n                from IPython.display import display\n\n                display(df_pd)\n            except ModuleNotFoundError:\n                print(df_pd)\n\n        return splink_dataframe\n\n    def __deepcopy__(self, memo):\n\"\"\"When we do EM training, we need a copy of the linker which is independent\n        of the main linker e.g. setting parameters on the copy will not affect the\n        main linker.  This method implements ensures linker can be deepcopied.\n        \"\"\"\n        new_linker = copy(self)\n        new_linker._em_training_sessions = []\n        new_settings = deepcopy(self._settings_obj_)\n        new_linker._settings_obj_ = new_settings\n        return new_linker\n\n    def _ensure_aliases_populated_and_is_list(\n        self, input_table_or_tables, input_table_aliases\n    ):\n        if input_table_aliases is None:\n            input_table_aliases = input_table_or_tables\n\n        input_table_aliases = ensure_is_list(input_table_aliases)\n\n        return input_table_aliases\n\n    def _get_input_tables_dict(self, input_table_or_tables, input_table_aliases):\n        input_table_or_tables = ensure_is_list(input_table_or_tables)\n\n        input_table_aliases = self._ensure_aliases_populated_and_is_list(\n            input_table_or_tables, input_table_aliases\n        )\n\n        d = {}\n        for table_name, table_alias in zip(input_table_or_tables, input_table_aliases):\n            d[table_alias] = self._table_to_splink_dataframe(table_alias, table_name)\n        return d\n\n    def _get_input_tf_dict(self, df_dict):\n        d = {}\n        for df_name, df_value in df_dict.items():\n            renamed = colname_to_tf_tablename(df_name)\n            d[renamed] = self._table_to_splink_dataframe(renamed, df_value)\n        return d\n\n    def _predict_warning(self):\n        if not self._settings_obj._is_fully_trained:\n            msg = (\n                \"\\n -- WARNING --\\n\"\n                \"You have called predict(), but there are some parameter \"\n                \"estimates which have neither been estimated or specified in your \"\n                \"settings dictionary.  To produce predictions the following\"\n                \" untrained trained parameters will use default values.\"\n            )\n            messages = self._settings_obj._not_trained_messages()\n\n            warn_message = \"\\n\".join([msg] + messages)\n\n            logger.warning(warn_message)\n\n    def _table_exists_in_database(self, table_name):\n        raise NotImplementedError(\n            f\"table_exists_in_database not implemented for {type(self)}\"\n        )\n\n    def _validate_input_dfs(self):\n        if not hasattr(self, \"_input_tables_dict\"):\n            # This is only triggered where a user loads a settings dict from a\n            # given file path.\n            return\n\n        for df in self._input_tables_dict.values():\n            df.validate()\n\n        if self._settings_obj_ is not None:\n            if self._settings_obj._link_type == \"dedupe_only\":\n                if len(self._input_tables_dict) &gt; 1:\n                    raise ValueError(\n                        'If link_type = \"dedupe only\" then input tables must contain '\n                        \"only a single input table\",\n                    )\n\n    def _validate_dialect(self):\n        settings_dialect = self._settings_obj._sql_dialect\n        if settings_dialect != self._sql_dialect:\n            raise ValueError(\n                f\"Incompatible SQL dialect! `settings` dictionary uses \"\n                f\"dialect {settings_dialect}, but expecting \"\n                f\"'{self._sql_dialect}' for Linker of type {type(self)}\"\n            )\n\n    def _populate_probability_two_random_records_match_from_trained_values(self):\n        recip_prop_matches_estimates = []\n\n        logger.log(\n            15,\n            (\n                \"---- Using training sessions to compute \"\n                \"probability two random records match ----\"\n            ),\n        )\n        for em_training_session in self._em_training_sessions:\n            training_lambda = (\n                em_training_session._settings_obj._probability_two_random_records_match\n            )\n            training_lambda_bf = prob_to_bayes_factor(training_lambda)\n            reverse_levels = (\n                em_training_session._comparison_levels_to_reverse_blocking_rule\n            )\n\n            logger.log(\n                15,\n                \"\\n\"\n                f\"Probability two random records match from trained model blocking on \"\n                f\"{em_training_session._blocking_rule_for_training.blocking_rule}: \"\n                f\"{training_lambda:,.3f}\",\n            )\n\n            for reverse_level in reverse_levels:\n                # Get comparison level on current settings obj\n                cc = self._settings_obj._get_comparison_by_output_column_name(\n                    reverse_level.comparison._output_column_name\n                )\n\n                cl = cc._get_comparison_level_by_comparison_vector_value(\n                    reverse_level._comparison_vector_value\n                )\n\n                if cl._has_estimated_values:\n                    bf = cl._trained_m_median / cl._trained_u_median\n                else:\n                    bf = cl._bayes_factor\n\n                logger.log(\n                    15,\n                    f\"Reversing comparison level {cc._output_column_name}\"\n                    f\" using bayes factor {bf:,.3f}\",\n                )\n\n                training_lambda_bf = training_lambda_bf / bf\n\n                as_prob = bayes_factor_to_prob(training_lambda_bf)\n\n                logger.log(\n                    15,\n                    (\n                        \"This estimate of probability two random records match now: \"\n                        f\" {as_prob:,.3f} \"\n                        f\"with reciprocal {(1/as_prob):,.3f}\"\n                    ),\n                )\n            logger.log(15, \"\\n---------\")\n            p = bayes_factor_to_prob(training_lambda_bf)\n            recip_prop_matches_estimates.append(1 / p)\n\n        prop_matches_estimate = 1 / median(recip_prop_matches_estimates)\n\n        self._settings_obj._probability_two_random_records_match = prop_matches_estimate\n        logger.log(\n            15,\n            \"\\nMedian of prop of matches estimates: \"\n            f\"{self._settings_obj._probability_two_random_records_match:,.3f} \"\n            \"reciprocal \"\n            f\"{1/self._settings_obj._probability_two_random_records_match:,.3f}\",\n        )\n\n    def _populate_m_u_from_trained_values(self):\n        ccs = self._settings_obj.comparisons\n\n        for cc in ccs:\n            for cl in cc._comparison_levels_excluding_null:\n                if cl._has_estimated_u_values:\n                    cl.u_probability = cl._trained_u_median\n                if cl._has_estimated_m_values:\n                    cl.m_probability = cl._trained_m_median\n\n    def _delete_tables_created_by_splink_from_db(\n        self, retain_term_frequency=True, retain_df_concat_with_tf=True\n    ):\n        to_remove = set()\n        for name in self._names_of_tables_created_by_splink:\n            # Only delete tables explicitly marked as having been created by splink\n            if \"__splink__\" not in name:\n                continue\n            if name == \"__splink__df_concat_with_tf\":\n                if not retain_df_concat_with_tf:\n                    self._delete_table_from_database(name)\n                    to_remove.add(name)\n            elif name.startswith(\"__splink__df_tf_\"):\n                if not retain_term_frequency:\n                    self._delete_table_from_database(name)\n                    to_remove.add(name)\n            else:\n                self._delete_table_from_database(name)\n                to_remove.add(name)\n\n        self._names_of_tables_created_by_splink = (\n            self._names_of_tables_created_by_splink - to_remove\n        )\n\n    def _raise_error_if_necessary_waterfall_columns_not_computed(self):\n        ricc = self._settings_obj._retain_intermediate_calculation_columns\n        rmc = self._settings_obj._retain_matching_columns\n        if not (ricc and rmc):\n            raise ValueError(\n                \"retain_intermediate_calculation_columns and \"\n                \"retain_matching_columns must both be set to True in your settings\"\n                \" dictionary to use this function, because otherwise the necessary \"\n                \"columns will not be available in the input records.\"\n                f\" Their current values are {ricc} and {rmc}, respectively. \"\n                \"Please re-run your linkage with them both set to True.\"\n            )\n\n    def _raise_error_if_necessary_accuracy_columns_not_computed(self):\n        rmc = self._settings_obj._retain_matching_columns\n        if not (rmc):\n            raise ValueError(\n                \"retain_matching_columns must be set to True in your settings\"\n                \" dictionary to use this function, because otherwise the necessary \"\n                \"columns will not be available in the input records.\"\n                f\" Its current value is {rmc}. \"\n                \"Please re-run your linkage with it set to True.\"\n            )\n\n    def load_settings(self, settings_dict: dict | str | Path):\n\"\"\"Initialise settings for the linker.  To be used if settings were\n        not passed to the linker on creation. This can either be in the form\n        of a settings dictionary or a filepath to a json file containing a\n        valid settings dictionary.\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.load_settings(settings_dict)\n            ```\n\n        Args:\n            settings_dict (dict | str | Path): A Splink settings dictionary or\n                the path to your settings json file.\n        \"\"\"\n\n        if not isinstance(settings_dict, dict):\n            p = Path(settings_dict)\n            if not p.is_file():  # check if it's a valid file/filepath\n                raise FileNotFoundError(\n                    \"The filepath you have provided is either not a valid file \"\n                    \"or doesn't exist along the path provided.\"\n                )\n            settings_dict = json.loads(p.read_text())\n\n        # Store the cache ID so it can be reloaded after cache invalidation\n        cache_id = self._cache_uid\n        # So we don't run into any issues with generated tables having\n        # invalid columns as settings have been tweaked, invalidate\n        # the cache and allow these tables to be recomputed.\n\n        # This is less efficient, but triggers infrequently and ensures we don't\n        # run into issues where the defaults used conflict with the actual values\n        # supplied in settings.\n\n        # This is particularly relevant with `source_dataset`, which appears within\n        # concat_with_tf.\n        self.invalidate_cache()\n\n        # If a uid already exists in your settings object, prioritise this\n        settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", cache_id)\n        settings_dict[\"sql_dialect\"] = settings_dict.get(\n            \"sql_dialect\", self._sql_dialect\n        )\n        self._settings_dict = settings_dict\n        self._settings_obj_ = Settings(settings_dict)\n        self._validate_input_dfs()\n        self._validate_dialect()\n\n    def load_model(self, model_path: Path):\n\"\"\"\n        Load a pre-defined model from a json file into the linker.\n        This is intended to be used with the output of\n        `save_model_to_json()`.\n\n        Examples:\n            ```py\n            linker.load_model(\"my_settings.json\")\n            ```\n\n        Args:\n            model_path (Path): A path to your model settings json file.\n        \"\"\"\n\n        return self.load_settings(model_path)\n\n    def initialise_settings(self, settings_dict: dict):\n\"\"\"*This method is now deprecated. Please use `load_settings`\n        when loading existing settings or `load_model` when loading\n         a pre-trained model.*\n\n        Initialise settings for the linker.  To be used if settings were\n        not passed to the linker on creation.\n        Examples:\n            === \"DuckDB\"\n                ```py\n                linker = DuckDBLinker(df\")\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \"Spark\"\n                ```py\n                linker = SparkLinker(df\")\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \"Athena\"\n                ```py\n                linker = AthenaLinker(df\")\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \"SQLite\"\n                ```py\n                linker = SQLiteLinker(df\")\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n        Args:\n            settings_dict (dict): A Splink settings dictionary\n        \"\"\"\n        # If a uid already exists in your settings object, prioritise this\n        settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", self._cache_uid)\n        settings_dict[\"sql_dialect\"] = settings_dict.get(\n            \"sql_dialect\", self._sql_dialect\n        )\n        self._settings_dict = settings_dict\n        self._settings_obj_ = Settings(settings_dict)\n        self._validate_input_dfs()\n        self._validate_dialect()\n\n        warnings.warn(\n            \"`initialise_settings` is deprecated. We advise you use \"\n            \"`linker.load_settings()` when loading in your settings or a previously \"\n            \"trained model.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    def load_settings_from_json(self, in_path: str | Path):\n\"\"\"*This method is now deprecated. Please use `load_settings`\n        when loading existing settings or `load_model` when loading\n         a pre-trained model.*\n\n        Load settings from a `.json` file.\n        This `.json` file would usually be the output of\n        `linker.save_model_to_json()`\n        Examples:\n            ```py\n            linker.load_settings_from_json(\"my_settings.json\")\n            ```\n        Args:\n            in_path (str): Path to settings json file\n        \"\"\"\n        self.load_settings(in_path)\n\n        warnings.warn(\n            \"`load_settings_from_json` is deprecated. We advise you use \"\n            \"`linker.load_settings()` when loading in your settings or a previously \"\n            \"trained model.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    def compute_tf_table(self, column_name: str) -&gt; SplinkDataFrame:\n\"\"\"Compute a term frequency table for a given column and persist to the database\n\n        This method is useful if you want to pre-compute term frequency tables e.g.\n        so that real time linkage executes faster, or so that you can estimate\n        various models without having to recompute term frequency tables each time\n\n        Examples:\n            === \"DuckDB\"\n                Real time linkage\n                ```py\n                linker = DuckDBLinker(df)\n                linker.load_settings(\"saved_settings.json\")\n                linker.compute_tf_table(\"surname\")\n                linker.compare_two_records(record_left, record_right)\n                ```\n                Pre-computed term frequency tables\n                ```py\n                linker = DuckDBLinker(df)\n                df_first_name_tf = linker.compute_tf_table(\"first_name\")\n                df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n                &gt;&gt;&gt;\n                # On subsequent data linking job, read this table rather than recompute\n                df_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\n                df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n                ```\n            === \"Spark\"\n                Real time linkage\n                ```py\n                linker = SparkLinker(df)\n                linker.load_settings(\"saved_settings.json\")\n                linker.compute_tf_table(\"surname\")\n                linker.compare_two_records(record_left, record_right)\n                ```\n                Pre-computed term frequency tables\n                ```py\n                linker = SparkLinker(df)\n                df_first_name_tf = linker.compute_tf_table(\"first_name\")\n                df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n                &gt;&gt;&gt;\n                # On subsequent data linking job, read this table rather than recompute\n                df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\n                df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n                ```\n\n        Args:\n            column_name (str): The column name in the input table\n\n        Returns:\n            SplinkDataFrame: The resultant table as a splink data frame\n        \"\"\"\n\n        input_col = InputColumn(column_name, settings_obj=self._settings_obj)\n        tf_tablename = colname_to_tf_tablename(input_col)\n        cache = self._intermediate_table_cache\n        concat_tf_tables = [\n            remove_quotes_from_identifiers(tf_col.input_name_as_tree).sql()\n            for tf_col in self._settings_obj._term_frequency_columns\n        ]\n\n        if tf_tablename in cache:\n            tf_df = cache[tf_tablename]\n        elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables:\n            self._pipeline.reset()\n            # If our df_concat_with_tf table already exists, use backwards inference to\n            # find a given tf table\n            colname = InputColumn(column_name)\n            sql = term_frequencies_from_concat_with_tf(colname)\n            self._enqueue_sql(sql, colname_to_tf_tablename(colname))\n            tf_df = self._execute_sql_pipeline(\n                [cache[\"__splink__df_concat_with_tf\"]], materialise_as_hash=True\n            )\n            self._intermediate_table_cache[tf_tablename] = tf_df\n        else:\n            # Clear the pipeline if we are materialising\n            self._pipeline.reset()\n            df_concat = self._initialise_df_concat()\n            input_dfs = []\n            if df_concat:\n                input_dfs.append(df_concat)\n            sql = term_frequencies_for_single_column_sql(input_col)\n            self._enqueue_sql(sql, tf_tablename)\n            tf_df = self._execute_sql_pipeline(input_dfs, materialise_as_hash=True)\n            self._intermediate_table_cache[tf_tablename] = tf_df\n\n        return tf_df\n\n    def deterministic_link(self) -&gt; SplinkDataFrame:\n\"\"\"Uses the blocking rules specified by\n        `blocking_rules_to_generate_predictions` in the settings dictionary to\n        generate pairwise record comparisons.\n\n        For deterministic linkage, this should be a list of blocking rules which\n        are strict enough to generate only true links.\n\n        Deterministic linkage, however, is likely to result in missed links\n        (false negatives).\n\n        Examples:\n            === \"DuckDB\"\n            ```py\n            from splink.duckdb.duckdb_linker import DuckDBLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = DuckDBLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n            === \"Spark\"\n            ```py\n            from splink.spark.spark_linker import SparkLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = SparkLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n            === \"Athena\"\n            ```py\n            from splink.athena.athena_linker import AthenaLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = AthenaLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n            === \"SQLite\"\n            ```py\n            from splink.sqlite.sqlite_linker import SQLiteLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = SQLiteLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n                represents a table materialised in the database. Methods on the\n                SplinkDataFrame allow you to access the underlying data.\n        \"\"\"\n\n        # Allows clustering during a deterministic linkage.\n        # This is used in `cluster_pairwise_predictions_at_threshold`\n        # to set the cluster threshold to 1\n        self._deterministic_link_mode = True\n\n        concat_with_tf = self._initialise_df_concat_with_tf()\n        sql = block_using_rules_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_blocked\")\n        return self._execute_sql_pipeline([concat_with_tf])\n\n    def estimate_u_using_random_sampling(\n        self, max_pairs: int = None, seed: int = None, *, target_rows=None\n    ):\n\"\"\"Estimate the u parameters of the linkage model using random sampling.\n\n        The u parameters represent the proportion of record comparisons that fall\n        into each comparison level amongst truly non-matching records.\n\n        This procedure takes a sample of the data and generates the cartesian\n        product of pairwise record comparisons amongst the sampled records.\n        The validity of the u values rests on the assumption that the resultant\n        pairwise comparisons are non-matches (or at least, they are very unlikely to be\n        matches). For large datasets, this is typically true.\n\n        The results of estimate_u_using_random_sampling, and therefore an entire splink\n        model, can be made reproducible by setting the seed parameter. Setting the seed\n        will have performance implications as additional processing is required.\n\n        Args:\n            max_pairs (int): The maximum number of pairwise record comparisons to\n            sample. Larger will give more accurate estimates\n            but lead to longer runtimes.  In our experience at least 1e9 (one billion)\n            gives best results but can take a long time to compute. 1e7 (ten million)\n            is often adequate whilst testing different model specifications, before\n            the final model is estimated.\n            seed (int): Seed for random sampling. Assign to get reproducible u\n            probabilities. Note, seed for random sampling is only supported for\n            DuckDB and Spark, for Athena and SQLite set to None.\n\n        Examples:\n            ```py\n            linker.estimate_u_using_random_sampling(1e8)\n            ```\n\n        Returns:\n            None: Updates the estimated u parameters within the linker object\n            and returns nothing.\n        \"\"\"\n        # TODO: Remove this compatibility code in a future release once we drop\n        # support for \"target_rows\". Deprecation warning added in 3.7.0\n        if max_pairs is not None and target_rows is not None:\n            # user supplied both\n            raise TypeError(\"Just use max_pairs\")\n        elif max_pairs is not None:\n            # user is doing it correctly\n            pass\n        elif target_rows is not None:\n            # user is using deprecated argument\n            warnings.warn(\n                \"target_rows is deprecated; use max_pairs\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            max_pairs = target_rows\n        else:\n            raise TypeError(\"Missing argument max_pairs\")\n\n        estimate_u_values(self, max_pairs, seed)\n        self._populate_m_u_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n    def estimate_m_from_label_column(self, label_colname: str):\n\"\"\"Estimate the m parameters of the linkage model from a label (ground truth)\n        column in the input dataframe(s).\n\n        The m parameters represent the proportion of record comparisons that fall\n        into each comparison level amongst truly matching records.\n\n        The ground truth column is used to generate pairwise record comparisons\n        which are then assumed to be matches.\n\n        For example, if the entity being matched is persons, and your input dataset(s)\n        contain social security number, this could be used to estimate the m values\n        for the model.\n\n        Note that this column does not need to be fully populated.  A common case is\n        where a unique identifier such as social security number is only partially\n        populated.\n\n        Args:\n            label_colname (str): The name of the column containing the ground truth\n                label in the input data.\n\n        Examples:\n            ```py\n            linker.estimate_m_from_label_column(\"social_security_number\")\n            ```\n\n        Returns:\n            Updates the estimated m parameters within the linker object\n            and returns nothing.\n        \"\"\"\n\n        # Ensure this has been run on the main linker so that it can be used by\n        # training linked when it checks the cache\n        self._initialise_df_concat_with_tf()\n        estimate_m_values_from_label_column(\n            self,\n            self._input_tables_dict,\n            label_colname,\n        )\n        self._populate_m_u_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n    def estimate_parameters_using_expectation_maximisation(\n        self,\n        blocking_rule: str,\n        comparisons_to_deactivate: list[str | Comparison] = None,\n        comparison_levels_to_reverse_blocking_rule: list[ComparisonLevel] = None,\n        fix_probability_two_random_records_match: bool = False,\n        fix_m_probabilities=False,\n        fix_u_probabilities=True,\n        populate_probability_two_random_records_match_from_trained_values=False,\n    ) -&gt; EMTrainingSession:\n\"\"\"Estimate the parameters of the linkage model using expectation maximisation.\n\n        By default, the m probabilities are estimated, but not the u probabilities,\n        because good estimates for the u probabilities can be obtained from\n        `linker.estimate_u_using_random_sampling()`.  You can change this by setting\n        `fix_u_probabilities` to False.\n\n        The blocking rule provided is used to generate pairwise record comparisons.\n        Usually, this should be a blocking rule that results in a dataframe where\n        matches are between about 1% and 99% of the comparisons.\n\n        By default, m parameters are estimated for all comparisons except those which\n        are included in the blocking rule.\n\n        For example, if the blocking rule is `l.first_name = r.first_name`, then\n        parameter esimates will be made for all comparison except those which use\n        `first_name` in their sql_condition\n\n        By default, the probability two random records match is estimated for the\n        blocked data, and then the m and u parameters for the columns specified in the\n        blocking rules are used to estiamte the global probability two random records\n        match.\n\n        To control which comparisons should have their parameter estimated, and the\n        process of 'reversing out' the global probability two random records match, the\n        user may specify `comparisons_to_deactivate` and\n        `comparison_levels_to_reverse_blocking_rule`.   This is useful, for example\n        if you block on the dmetaphone of a column but match on the original column.\n\n        Examples:\n            Default behaviour\n            ```py\n            br_training = \"l.first_name = r.first_name and l.dob = r.dob\"\n            linker.estimate_parameters_using_expectation_maximisation(br_training)\n            ```\n            Specify which comparisons to deactivate\n            ```py\n            br_training = \"l.dmeta_first_name = r.dmeta_first_name\"\n            settings_obj = linker._settings_obj\n            comp = settings_obj._get_comparison_by_output_column_name(\"first_name\")\n            dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1)\n            linker.estimate_parameters_using_expectation_maximisation(\n                br_training,\n                comparisons_to_deactivate=[\"first_name\"],\n                comparison_levels_to_reverse_blocking_rule=[dmeta_level],\n            )\n            ```\n\n        Args:\n            blocking_rule (str): The blocking rule used to generate pairwise record\n                comparisons.\n            comparisons_to_deactivate (list, optional): By default, splink will\n                analyse the blocking rule provided and estimate the m parameters for\n                all comaprisons except those included in the blocking rule.  If\n                comparisons_to_deactivate are provided, spink will instead\n                estimate m parameters for all comparison except those specified\n                in the comparisons_to_deactivate list.  This list can either contain\n                the output_column_name of the Comparison as a string, or Comparison\n                objects.  Defaults to None.\n            comparison_levels_to_reverse_blocking_rule (list, optional): By default,\n                splink will analyse the blocking rule provided and adjust the\n                global probability two random records match to account for the matches\n                specified in the blocking rule. If provided, this argument will overrule\n                this default behaviour. The user must provide a list of ComparisonLevel\n                objects.  Defaults to None.\n            fix_probability_two_random_records_match (bool, optional): If True, do not\n                update the probability two random records match after each iteration.\n                Defaults to False.\n            fix_m_probabilities (bool, optional): If True, do not update the m\n                probabilities after each iteration. Defaults to False.\n            fix_u_probabilities (bool, optional): If True, do not update the u\n                probabilities after each iteration. Defaults to True.\n            populate_probability_two_random_records_match_from_trained_values\n                (bool, optional): If True, derive this parameter from\n                the blocked value. Defaults to False.\n\n        Examples:\n            ```py\n            blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\"\n            linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n            ```\n\n        Returns:\n            EMTrainingSession:  An object containing information about the training\n                session such as how parameters changed during the iteration history\n\n        \"\"\"\n        # Ensure this has been run on the main linker so that it's in the cache\n        # to be used by the training linkers\n        self._initialise_df_concat_with_tf()\n\n        if comparisons_to_deactivate:\n            # If user provided a string, convert to Comparison object\n            comparisons_to_deactivate = [\n                self._settings_obj._get_comparison_by_output_column_name(n)\n                if isinstance(n, str)\n                else n\n                for n in comparisons_to_deactivate\n            ]\n            if comparison_levels_to_reverse_blocking_rule is None:\n                logger.warning(\n                    \"\\nWARNING: \\n\"\n                    \"You have provided comparisons_to_deactivate but not \"\n                    \"comparison_levels_to_reverse_blocking_rule.\\n\"\n                    \"If comparisons_to_deactivate is provided, then \"\n                    \"you usually need to provide corresponding \"\n                    \"comparison_levels_to_reverse_blocking_rule \"\n                    \"because each comparison to deactivate is effectively treated \"\n                    \"as an exact match.\"\n                )\n\n        em_training_session = EMTrainingSession(\n            self,\n            blocking_rule,\n            fix_u_probabilities=fix_u_probabilities,\n            fix_m_probabilities=fix_m_probabilities,\n            fix_probability_two_random_records_match=fix_probability_two_random_records_match,  # noqa 501\n            comparisons_to_deactivate=comparisons_to_deactivate,\n            comparison_levels_to_reverse_blocking_rule=comparison_levels_to_reverse_blocking_rule,  # noqa 501\n        )\n\n        em_training_session._train()\n\n        self._populate_m_u_from_trained_values()\n\n        if populate_probability_two_random_records_match_from_trained_values:\n            self._populate_probability_two_random_records_match_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n        return em_training_session\n\n    def predict(\n        self,\n        threshold_match_probability: float = None,\n        threshold_match_weight: float = None,\n        materialise_after_computing_term_frequencies=True,\n    ) -&gt; SplinkDataFrame:\n\"\"\"Create a dataframe of scored pairwise comparisons using the parameters\n        of the linkage model.\n\n        Uses the blocking rules specified in the\n        `blocking_rules_to_generate_predictions` of the settings dictionary to\n        generate the pairwise comparisons.\n\n        Args:\n            threshold_match_probability (float, optional): If specified,\n                filter the results to include only pairwise comparisons with a\n                match_probability above this threshold. Defaults to None.\n            threshold_match_weight (float, optional): If specified,\n                filter the results to include only pairwise comparisons with a\n                match_weight above this threshold. Defaults to None.\n            materialise_after_computing_term_frequencies (bool): If true, Splink\n                will materialise the table containing the input nodes (rows)\n                joined to any term frequencies which have been asked\n                for in the settings object.  If False, this will be\n                computed as part of one possibly gigantic CTE\n                pipeline.   Defaults to True\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            df = linker.predict(threshold_match_probability=0.95)\n            df.as_pandas_dataframe(limit=5)\n            ```\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n                represents a table materialised in the database. Methods on the\n                SplinkDataFrame allow you to access the underlying data.\n\n        \"\"\"\n\n        # If materialise_after_computing_term_frequencies=False and the user only\n        # calls predict, it runs as a single pipeline with no materialisation\n        # of anything.\n\n        # _initialise_df_concat_with_tf returns None if the table doesn't exist\n        # and only SQL is queued in this step.\n        nodes_with_tf = self._initialise_df_concat_with_tf(\n            materialise=materialise_after_computing_term_frequencies\n        )\n\n        input_dataframes = []\n        if nodes_with_tf:\n            input_dataframes.append(nodes_with_tf)\n\n        sql = block_using_rules_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_blocked\")\n\n        repartition_after_blocking = getattr(self, \"repartition_after_blocking\", False)\n\n        # repartition after blocking only exists on the SparkLinker\n        if repartition_after_blocking:\n            df_blocked = self._execute_sql_pipeline(input_dataframes)\n            input_dataframes.append(df_blocked)\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            threshold_match_probability,\n            threshold_match_weight,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        predictions = self._execute_sql_pipeline(input_dataframes)\n        self._predict_warning()\n        return predictions\n\n    def find_matches_to_new_records(\n        self,\n        records_or_tablename,\n        blocking_rules=[],\n        match_weight_threshold=-4,\n    ) -&gt; SplinkDataFrame:\n\"\"\"Given one or more records, find records in the input dataset(s) which match\n        and return in order of the splink prediction score.\n\n        This effectively provides a way of searching the input datasets\n        for given record(s)\n\n        Args:\n            records_or_tablename (List[dict]): Input search record(s) as list of dict,\n                or a table registered to the database.\n            blocking_rules (list, optional): Blocking rules to select\n                which records to find and score. If [], do not use a blocking\n                rule - meaning the input records will be compared to all records\n                provided to the linker when it was instantiated. Defaults to [].\n            match_weight_threshold (int, optional): Return matches with a match weight\n                above this threshold. Defaults to -4.\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            # Pre-compute tf tables for any tables with\n            # term frequency adjustments\n            linker.compute_tf_table(\"first_name\")\n            record = {'unique_id': 1,\n                'first_name': \"John\",\n                'surname': \"Smith\",\n                'dob': \"1971-05-24\",\n                'city': \"London\",\n                'email': \"john@smith.net\"\n                }\n            df = linker.find_matches_to_new_records([record], blocking_rules=[])\n            ```\n\n        Returns:\n            SplinkDataFrame: The pairwise comparisons.\n        \"\"\"\n\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        if not isinstance(records_or_tablename, str):\n            uid = ascii_uid(8)\n            self.register_table(\n                records_or_tablename, f\"__splink__df_new_records_{uid}\", overwrite=True\n            )\n            new_records_tablename = f\"__splink__df_new_records_{uid}\"\n        else:\n            new_records_tablename = records_or_tablename\n\n        cache = self._intermediate_table_cache\n        input_dfs = []\n        # If our df_concat_with_tf table already exists, use backwards inference to\n        # find all underlying term frequency tables.\n        if \"__splink__df_concat_with_tf\" in cache:\n            concat_with_tf = cache[\"__splink__df_concat_with_tf\"]\n            tf_tables = compute_term_frequencies_from_concat_with_tf(self)\n            # This queues up our tf tables, rather materialising them\n            for tf in tf_tables:\n                # if tf is a SplinkDataFrame, then the table already exists\n                if isinstance(tf, SplinkDataFrame):\n                    input_dfs.append(tf)\n                else:\n                    self._enqueue_sql(tf[\"sql\"], tf[\"output_table_name\"])\n        else:\n            # This queues up our cols_with_tf and df_concat_with_tf tables.\n            concat_with_tf = self._initialise_df_concat_with_tf(materialise=False)\n\n        if concat_with_tf:\n            input_dfs.append(concat_with_tf)\n\n        rules = []\n        for r in blocking_rules:\n            br_as_obj = BlockingRule(r) if not isinstance(r, BlockingRule) else r\n            br_as_obj.preceding_rules = rules.copy()\n            rules.append(br_as_obj)\n        blocking_rules = rules\n\n        self._settings_obj._blocking_rules_to_generate_predictions = blocking_rules\n\n        self._settings_obj._link_type = \"link_only_find_matches_to_new_records\"\n        self._find_new_matches_mode = True\n\n        sql = _join_tf_to_input_df_sql(self)\n        sql = sql.replace(\"__splink__df_concat\", new_records_tablename)\n        self._enqueue_sql(sql, \"__splink__df_new_records_with_tf\")\n\n        sql = block_using_rules_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_blocked\")\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = f\"\"\"\n        select * from __splink__df_predict\n        where match_weight &gt; {match_weight_threshold}\n        \"\"\"\n\n        self._enqueue_sql(sql, \"__splink__find_matches_predictions\")\n\n        predictions = self._execute_sql_pipeline(\n            input_dataframes=input_dfs, use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._find_new_matches_mode = False\n\n        return predictions\n\n    def compare_two_records(self, record_1: dict, record_2: dict):\n\"\"\"Use the linkage model to compare and score a pairwise record comparison\n        based on the two input records provided\n\n        Args:\n            record_1 (dict): dictionary representing the first record.  Columns names\n                and data types must be the same as the columns in the settings object\n            record_2 (dict): dictionary representing the second record.  Columns names\n                and data types must be the same as the columns in the settings object\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n\n        Returns:\n            SplinkDataFrame: Pairwise comparison with scored prediction\n        \"\"\"\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        self._compare_two_records_mode = True\n        self._settings_obj._blocking_rules_to_generate_predictions = []\n\n        uid = ascii_uid(8)\n        df_records_left = self.register_table(\n            [record_1], f\"__splink__compare_two_records_left_{uid}\", overwrite=True\n        )\n        df_records_left.templated_name = \"__splink__compare_two_records_left\"\n\n        df_records_right = self.register_table(\n            [record_2], f\"__splink__compare_two_records_right_{uid}\", overwrite=True\n        )\n        df_records_right.templated_name = \"__splink__compare_two_records_right\"\n\n        sql_join_tf = _join_tf_to_input_df_sql(self)\n\n        sql_join_tf = sql_join_tf.replace(\n            \"__splink__df_concat\", \"__splink__compare_two_records_left\"\n        )\n        self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_left_with_tf\")\n\n        sql_join_tf = sql_join_tf.replace(\n            \"__splink__compare_two_records_left\", \"__splink__compare_two_records_right\"\n        )\n\n        self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_right_with_tf\")\n\n        sql = block_using_rules_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_blocked\")\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        predictions = self._execute_sql_pipeline(\n            [df_records_left, df_records_right], use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._compare_two_records_mode = False\n\n        return predictions\n\n    def _self_link(self) -&gt; SplinkDataFrame:\n\"\"\"Use the linkage model to compare and score all records in our input df with\n            themselves.\n\n        Returns:\n            SplinkDataFrame: Scored pairwise comparisons of the input records to\n                themselves.\n        \"\"\"\n\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        # Changes our sql to allow for a self link.\n        # This is used in `_sql_gen_where_condition` in blocking.py\n        # to remove any 'where' clauses when blocking (normally when blocking\n        # we want to *remove* self links!)\n        self._self_link_mode = True\n\n        # Block on uid i.e. create pairwise record comparisons where the uid matches\n        uid_cols = self._settings_obj._unique_id_input_columns\n        uid_l = _composite_unique_id_from_edges_sql(uid_cols, None, \"l\")\n        uid_r = _composite_unique_id_from_edges_sql(uid_cols, None, \"r\")\n\n        self._settings_obj._blocking_rules_to_generate_predictions = [\n            BlockingRule(f\"{uid_l} = {uid_r}\")\n        ]\n\n        nodes_with_tf = self._initialise_df_concat_with_tf()\n\n        sql = block_using_rules_sql(self)\n\n        self._enqueue_sql(sql, \"__splink__df_blocked\")\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            output_table_name = sql[\"output_table_name\"]\n            output_table_name = output_table_name.replace(\"predict\", \"self_link\")\n            self._enqueue_sql(sql[\"sql\"], output_table_name)\n\n        predictions = self._execute_sql_pipeline(\n            input_dataframes=[nodes_with_tf], use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._self_link_mode = False\n\n        return predictions\n\n    def cluster_pairwise_predictions_at_threshold(\n        self,\n        df_predict: SplinkDataFrame,\n        threshold_match_probability: float = None,\n        pairwise_formatting: bool = False,\n        filter_pairwise_format_for_clusters: bool = True,\n    ) -&gt; SplinkDataFrame:\n\"\"\"Clusters the pairwise match predictions that result from `linker.predict()`\n        into groups of connected record using the connected components graph clustering\n        algorithm\n\n        Records with an estimated `match_probability` above\n        `threshold_match_probability` are considered to be a match (i.e. they represent\n        the same entity).\n\n        Args:\n            df_predict (SplinkDataFrame): The results of `linker.predict()`\n            threshold_match_probability (float): Filter the pairwise match predictions\n                to include only pairwise comparisons with a match_probability above this\n                threshold. This dataframe is then fed into the clustering\n                algorithm.\n            pairwise_formatting (bool): Whether to output the pairwise match predictions\n                from linker.predict() with cluster IDs.\n                If this is set to false, the output will be a list of all IDs, clustered\n                into groups based on the desired match threshold.\n            filter_pairwise_format_for_clusters (bool): If pairwise formatting has been\n                selected, whether to output all columns found within linker.predict(),\n                or just return clusters.\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered\n                into groups based on the desired match threshold.\n\n        \"\"\"\n\n        # Feeding in df_predict forces materiailisation, if it exists in your database\n        concat_with_tf = self._initialise_df_concat_with_tf(df_predict)\n\n        edges_table = _cc_create_unique_id_cols(\n            self,\n            concat_with_tf.physical_name,\n            df_predict.physical_name,\n            threshold_match_probability,\n        )\n\n        cc = solve_connected_components(\n            self,\n            edges_table,\n            df_predict,\n            concat_with_tf,\n            pairwise_formatting,\n            filter_pairwise_format_for_clusters,\n        )\n\n        return cc\n\n    def profile_columns(\n        self, column_expressions: str | list[str], top_n=10, bottom_n=10\n    ):\n        return profile_columns(self, column_expressions, top_n=top_n, bottom_n=bottom_n)\n\n    def _get_labels_tablename_from_input(\n        self, labels_splinkdataframe_or_table_name: str | SplinkDataFrame\n    ):\n        if isinstance(labels_splinkdataframe_or_table_name, SplinkDataFrame):\n            labels_tablename = labels_splinkdataframe_or_table_name.physical_name\n        elif isinstance(labels_splinkdataframe_or_table_name, str):\n            labels_tablename = labels_splinkdataframe_or_table_name\n        else:\n            raise ValueError(\n                \"The 'labels_splinkdataframe_or_table_name' argument\"\n                \" must be of type SplinkDataframe or a string representing a tablename\"\n                \" in the input database\"\n            )\n        return labels_tablename\n\n    def estimate_m_from_pairwise_labels(self, labels_splinkdataframe_or_table_name):\n\"\"\"Estimate the m parameters of the linkage model from a dataframe of pairwise\n        labels.\n\n        The table of labels should be in the following format, and should\n        be registered with your database:\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|\n        |----------------|-----------|----------------|-----------|\n        |df_1            |1          |df_2            |2          |\n        |df_1            |1          |df_2            |3          |\n\n        Note that `source_dataset` and `unique_id` should correspond to the\n        values specified in the settings dict, and the `input_table_aliases`\n        passed to the `linker` object. Note that at the moment, this method does\n        not respect values in a `clerical_match_score` column.  If provided, these\n        are ignored and it is assumed that every row in the table of labels is a score\n        of 1, i.e. a perfect match.\n\n        Args:\n          labels_splinkdataframe_or_table_name (str): Name of table containing labels\n            in the database or SplinkDataframe\n\n        Examples:\n            ```py\n            pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\")\n            linker.register_table(pairwise_labels, \"labels\", overwrite=True)\n            linker.estimate_m_from_pairwise_labels(\"labels\")\n            ```\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        estimate_m_from_pairwise_labels(self, labels_tablename)\n\n    def truth_space_table_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ) -&gt; SplinkDataFrame:\n\"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n        match_probability, suitable for plotting a ROC chart.\n\n        The table of labels should be in the following format, and should be registered\n        with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            === \"DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.truth_space_table_from_labels_table(\"labels\")\n                ```\n            === \"Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.truth_space_table_from_labels_table(\"labels\")\n                ```\n        Returns:\n            SplinkDataFrame:  Table of truth statistics\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        return truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n\n    def roc_chart_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name: str | SplinkDataFrame,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n\"\"\"Generate a ROC chart from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            === \"DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.roc_chart_from_labels_table(\"labels\")\n                ```\n            === \"Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.roc_chart_from_labels_table(\"labels\")\n                ```\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return roc_chart(recs)\n\n    def precision_recall_chart_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n\"\"\"Generate a precision-recall chart from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        as a table with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n        Examples:\n            === \"DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.precision_recall_chart_from_labels_table(\"labels\")\n                ```\n            === \"Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.precision_recall_chart_from_labels_table(\"labels\")\n                ```\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return precision_recall_chart(recs)\n\n    def prediction_errors_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        include_false_positives=True,\n        include_false_negatives=True,\n        threshold=0.5,\n    ):\n\"\"\"Generate a dataframe containing false positives and false negatives\n        based on the comparison between the clerical_match_score in the labels\n        table compared with the splink predicted match probability\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            include_false_positives (bool, optional): Defaults to True.\n            include_false_negatives (bool, optional): Defaults to True.\n            threshold (float, optional): Threshold above which a score is considered\n                to be a match. Defaults to 0.5.\n\n        Returns:\n            SplinkDataFrame:  Table containing false positives and negatives\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        return prediction_errors_from_labels_table(\n            self,\n            labels_tablename,\n            include_false_positives,\n            include_false_negatives,\n            threshold,\n        )\n\n    def truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n\"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n        match_probability, suitable for plotting a ROC chart.\n\n        Your labels_column_name should include the ground truth cluster (unique\n        identifier) that groups entities which are the same\n\n        Args:\n            labels_tablename (str): Name of table containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            ```py\n            linker.truth_space_table_from_labels_column(\"cluster\")\n            ```\n\n        Returns:\n            SplinkDataFrame:  Table of truth statistics\n        \"\"\"\n\n        return truth_space_table_from_labels_column(\n            self, labels_column_name, threshold_actual, match_weight_round_to_nearest\n        )\n\n    def roc_chart_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n\"\"\"Generate a ROC chart from ground truth data, whereby the ground truth\n        is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            ```py\n            linker.roc_chart_from_labels_column(\"labels\")\n            ```\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n        \"\"\"\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return roc_chart(recs)\n\n    def precision_recall_chart_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n\"\"\"Generate a precision-recall chart from ground truth data, whereby the ground\n        truth is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n        Examples:\n            ```py\n            linker.precision_recall_chart_from_labels_column(\"ground_truth\")\n            ```\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n        \"\"\"\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return precision_recall_chart(recs)\n\n    def prediction_errors_from_labels_column(\n        self,\n        label_colname,\n        include_false_positives=True,\n        include_false_negatives=True,\n        threshold=0.5,\n    ):\n\"\"\"Generate a dataframe containing false positives and false negatives\n        based on the comparison between the splink match probability and the\n        labels column.  A label column is a column in the input dataset that contains\n        the 'ground truth' cluster to which the record belongs\n\n        Args:\n            label_colname (str): Name of labels column in input data\n            include_false_positives (bool, optional): Defaults to True.\n            include_false_negatives (bool, optional): Defaults to True.\n            threshold (float, optional): Threshold above which a score is considered\n                to be a match. Defaults to 0.5.\n\n        Returns:\n            SplinkDataFrame:  Table containing false positives and negatives\n        \"\"\"\n        return prediction_errors_from_label_column(\n            self,\n            label_colname,\n            include_false_positives,\n            include_false_negatives,\n            threshold,\n        )\n\n    def match_weights_histogram(\n        self, df_predict: SplinkDataFrame, target_bins: int = 30, width=600, height=250\n    ):\n\"\"\"Generate a histogram that shows the distribution of match weights in\n        `df_predict`\n\n        Args:\n            df_predict (SplinkDataFrame): Output of `linker.predict()`\n            target_bins (int, optional): Target number of bins in histogram. Defaults to\n                30.\n            width (int, optional): Width of output. Defaults to 600.\n            height (int, optional): Height of output chart. Defaults to 250.\n\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n\n        \"\"\"\n        df = histogram_data(self, df_predict, target_bins)\n        recs = df.as_record_dict()\n        return match_weights_histogram(recs, width=width, height=height)\n\n    def waterfall_chart(self, records: list[dict], filter_nulls=True):\n\"\"\"Visualise how the final match weight is computed for the provided pairwise\n        record comparisons.\n\n        Records must be provided as a list of dictionaries. This would usually be\n        obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame.\n\n        Examples:\n            ```py\n            df = linker.predict(threshold_match_weight=2)\n            records = df.as_record_dict(limit=10)\n            linker.waterfall_chart(records)\n            ```\n\n        Args:\n            records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)`\n                where `df` is a SplinkDataFrame.\n            filter_nulls (bool, optional): Whether the visualiation shows null\n                comparisons, which have no effect on final match weight. Defaults to\n                True.\n\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        return waterfall_chart(records, self._settings_obj, filter_nulls)\n\n    def unlinkables_chart(\n        self,\n        x_col=\"match_weight\",\n        source_dataset=None,\n        as_dict=False,\n    ):\n\"\"\"Generate an interactive chart displaying the proportion of records that\n        are \"unlinkable\" for a given splink score threshold and model parameters.\n\n        Unlinkable records are those that, even when compared with themselves, do not\n        contain enough information to confirm a match.\n\n        Args:\n            x_col (str, optional): Column to use for the x-axis.\n                Defaults to \"match_weight\".\n            source_dataset (str, optional): Name of the source dataset to use for\n                the title of the output chart.\n            as_dict (bool, optional): If True, return a dict version of the chart.\n\n        Examples:\n            For the simplest code pipeline, load a pre-trained model\n            and run this against the test data.\n            ```py\n            df = pd.read_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\")\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.unlinkables_chart()\n            ```\n            For more complex code pipelines, you can run an entire pipeline\n            that estimates your m and u values, before `unlinkables_chart().\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n        \"\"\"\n\n        # Link our initial df on itself and calculate the % of unlinkable entries\n        records = unlinkables_data(self)\n        return unlinkables_chart(records, x_col, source_dataset, as_dict)\n\n    def comparison_viewer_dashboard(\n        self,\n        df_predict: SplinkDataFrame,\n        out_path: str,\n        overwrite=False,\n        num_example_rows=2,\n        return_html_as_string=False,\n    ):\n\"\"\"Generate an interactive html visualization of the linker's predictions and\n        save to `out_path`.  For more information see\n        [this video](https://www.youtube.com/watch?v=DNvCMqjipis)\n\n\n        Args:\n            df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n            out_path (str): The path (including filename) to save the html file to.\n            overwrite (bool, optional): Overwrite the html file if it already exists?\n                Defaults to False.\n            num_example_rows (int, optional): Number of example rows per comparison\n                vector. Defaults to 2.\n            return_html_as_string: If True, return the html as a string\n\n        Examples:\n            ```py\n            df_predictions = linker.predict()\n            linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n            ```\n\n            Optionally, in Jupyter, you can display the results inline\n            Otherwise you can just load the html file in your browser\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n            ```\n\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        sql = comparison_vector_distribution_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vector_distribution\")\n\n        sqls = comparison_viewer_table_sqls(self, num_example_rows)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        df = self._execute_sql_pipeline([df_predict])\n\n        rendered = render_splink_comparison_viewer_html(\n            df.as_record_dict(),\n            self._settings_obj._as_completed_dict(),\n            out_path,\n            overwrite,\n        )\n        if return_html_as_string:\n            return rendered\n\n    def parameter_estimate_comparisons_chart(self, include_m=True, include_u=True):\n\"\"\"Show a chart that shows how parameter estimates have differed across\n        the different estimation methods you have used.\n\n        For example, if you have run two EM estimation sessions, blocking on\n        different variables, and both result in parameter estimates for\n        first_name, this chart will enable easy comparison of the different\n        estimates\n\n        Args:\n            include_m (bool, optional): Show different estimates of m values. Defaults\n                to True.\n            include_u (bool, optional): Show different estimates of u values. Defaults\n                to True.\n\n        \"\"\"\n        records = self._settings_obj._parameter_estimates_as_records\n\n        to_retain = []\n        if include_m:\n            to_retain.append(\"m\")\n        if include_u:\n            to_retain.append(\"u\")\n\n        records = [r for r in records if r[\"m_or_u\"] in to_retain]\n\n        return parameter_estimate_comparisons(records)\n\n    def missingness_chart(self, input_dataset: str = None):\n\"\"\"Generate a summary chart of the missingness (prevalence of nulls) of\n        columns in the input datasets.  By default, missingness is assessed across\n        all input datasets\n\n        Args:\n            input_dataset (str, optional): Name of one of the input tables in the\n            database.  If provided, missingness will be computed for this table alone.\n            Defaults to None.\n\n        Examples:\n            ```py\n            linker.missingness_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.missingness_chart()\n            save_offline_chart(c.spec, \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500\n            ```\n        \"\"\"\n        records = missingness_data(self, input_dataset)\n        return missingness_chart(records)\n\n    def completeness_chart(self, input_dataset: str = None, cols: list[str] = None):\n\"\"\"Generate a summary chart of the completeness (proportion of non-nulls) of\n        columns in each of the input datasets. By default, completeness is assessed for\n        all column in the input data.\n\n        Args:\n            input_dataset (str, optional): Name of one of the input tables in the\n                database.  If provided, completeness will be computed for this table\n                alone. Defaults to None.\n            cols (List[str], optional): List of column names to calculate completeness.\n                Default to None.\n\n        Examples:\n            ```py\n            linker.completeness_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.completeness_chart()\n            save_offline_chart(c.spec, \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500\n            ```\n        \"\"\"\n        records = completeness_data(self, input_dataset, cols)\n        return completeness_chart(records)\n\n    def count_num_comparisons_from_blocking_rule(\n        self,\n        blocking_rule: str,\n    ) -&gt; int:\n\"\"\"Compute the number of pairwise record comparisons that would be generated by\n        a blocking rule\n\n        Args:\n            blocking_rule (str): The blocking rule to analyse\n            link_type (str, optional): The link type.  This is needed only if the\n                linker has not yet been provided with a settings dictionary.  Defaults\n                to None.\n            unique_id_column_name (str, optional):  This is needed only if the\n                linker has not yet been provided with a settings dictionary.  Defaults\n                to None.\n\n        Examples:\n            ```py\n            br = \"l.first_name = r.first_name\"\n            linker.count_num_comparisons_from_blocking_rule(br)\n            ```\n            &gt; 19387\n            ```py\n            br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            linker.count_num_comparisons_from_blocking_rule(br)\n            ```\n            &gt; 394\n\n        Returns:\n            int: The number of comparisons generated by the blocking rule\n        \"\"\"\n\n        sql = vertically_concatenate_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_concat\")\n\n        sql = number_of_comparisons_generated_by_blocking_rule_sql(self, blocking_rule)\n        self._enqueue_sql(sql, \"__splink__analyse_blocking_rule\")\n        res = self._execute_sql_pipeline().as_record_dict()[0]\n        return res[\"count_of_pairwise_comparisons_generated\"]\n\n    def cumulative_comparisons_from_blocking_rules_records(\n        self,\n        blocking_rules: str or list = None,\n    ):\n\"\"\"Output the number of comparisons generated by each successive blocking rule.\n\n        This is equivalent to the output size of df_predict and details how many\n        comparisons each of your individual blocking rules will contribute to the\n        total.\n\n        Args:\n            blocking_rules (str or list): The blocking rule(s) to compute comparisons\n                for. If null, the rules set out in your settings object will be used.\n\n        Examples:\n            ```py\n            linker_settings = DuckDBLinker(df, settings)\n            # Compute the cumulative number of comparisons generated by the rules\n            # in your settings object.\n            linker_settings.cumulative_comparisons_from_blocking_rules_records()\n            &gt;&gt;&gt;\n            # Generate total comparisons with custom blocking rules.\n            blocking_rules = [\n               \"l.surname = r.surname\",\n               \"l.first_name = r.first_name\n                and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            ]\n            &gt;&gt;&gt;\n            linker_settings.cumulative_comparisons_from_blocking_rules_records(\n                blocking_rules\n             )\n            ```\n\n        Returns:\n            List: A list of blocking rules and the corresponding number of\n                comparisons it is forecast to generate.\n        \"\"\"\n        if blocking_rules:\n            blocking_rules = ensure_is_list(blocking_rules)\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self, blocking_rules, output_chart=False\n        )\n\n        return records\n\n    def cumulative_num_comparisons_from_blocking_rules_chart(\n        self,\n        blocking_rules: str or list = None,\n    ):\n\"\"\"Display a chart with the cumulative number of comparisons generated by a\n        selection of blocking rules.\n\n        This is equivalent to the output size of df_predict and details how many\n        comparisons each of your individual blocking rules will contribute to the\n        total.\n\n        Args:\n            blocking_rules (str or list): The blocking rule(s) to compute comparisons\n                for. If null, the rules set out in your settings object will be used.\n\n        Examples:\n            ```py\n            linker_settings = DuckDBLinker(df, settings)\n            # Compute the cumulative number of comparisons generated by the rules\n            # in your settings object.\n            linker_settings.cumulative_num_comparisons_from_blocking_rules_chart()\n            &gt;&gt;&gt;\n            # Generate total comparisons with custom blocking rules.\n            blocking_rules = [\n               \"l.surname = r.surname\",\n               \"l.first_name = r.first_name\n                and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            ]\n            &gt;&gt;&gt;\n            linker_settings.cumulative_num_comparisons_from_blocking_rules_chart(\n                blocking_rules\n             )\n            ```\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n        \"\"\"\n\n        if blocking_rules:\n            blocking_rules = ensure_is_list(blocking_rules)\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self, blocking_rules, output_chart=True\n        )\n\n        return cumulative_blocking_rule_comparisons_generated(records)\n\n    def count_num_comparisons_from_blocking_rules_for_prediction(self, df_predict):\n\"\"\"Counts the maginal number of edges created from each of the blocking rules\n        in `blocking_rules_to_generate_predictions`\n\n        This is different to `count_num_comparisons_from_blocking_rule`\n        because it (a) analyses multiple blocking rules rather than a single rule, and\n        (b) deduplicates any comparisons that are generated, to tell you the\n        marginal effect of each entry in `blocking_rules_to_generate_predictions`\n\n        Args:\n            df_predict (SplinkDataFrame): SplinkDataFrame with match weights\n            and probabilities of rows matching\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df, connection=\":memory:\")\n            linker.load_settings(\"saved_settings.json\")\n            df_predict = linker.predict(threshold_match_probability=0.95)\n            count_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict)\n            count_pairwise.as_pandas_dataframe(limit=5)\n            ```\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons and\n                estimated pairwise comparisons generated by the blocking rules.\n        \"\"\"  # noqa: E501\n        sql = count_num_comparisons_from_blocking_rules_for_prediction_sql(\n            self, df_predict\n        )\n        match_key_analysis = self._sql_to_splink_dataframe_checking_cache(\n            sql, \"__splink__match_key_analysis\"\n        )\n        return match_key_analysis\n\n    def match_weights_chart(self):\n\"\"\"Display a chart of the (partial) match weights of the linkage model\n\n        Examples:\n            ```py\n            linker.match_weights_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.match_weights_chart()\n            save_offline_chart(c.spec, \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500)\n            ```\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n        \"\"\"\n        return self._settings_obj.match_weights_chart()\n\n    def tf_adjustment_chart(\n        self,\n        output_column_name: str,\n        n_most_freq: int = 10,\n        n_least_freq: int = 10,\n        vals_to_include: str | list = None,\n        as_dict: bool = False,\n    ):\n\"\"\"Display a chart showing the impact of term frequency adjustments on a\n        specific comparison level.\n        Each value\n\n        Args:\n            output_column_name (str): Name of an output column for which term frequency\n                 adjustment has been applied.\n            n_most_freq (int, optional): Number of most frequent values to show. If this\n                 or `n_least_freq` set to None, all values will be shown.\n                Default to 10.\n            n_least_freq (int, optional): Number of least frequent values to show. If\n                this or `n_most_freq` set to None, all values will be shown.\n                Default to 10.\n            vals_to_include (list, optional): Specific values for which to show term\n                sfrequency adjustments.\n                Defaults to None.\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n        \"\"\"\n\n        # Comparisons with TF adjustments\n        tf_comparisons = [\n            c._output_column_name\n            for c in self._settings_obj.comparisons\n            if any([cl._has_tf_adjustments for cl in c.comparison_levels])\n        ]\n        if output_column_name not in tf_comparisons:\n            raise ValueError(\n                f\"{output_column_name} is not a valid comparison column, or does not\"\n                f\" have term frequency adjustment activated\"\n            )\n\n        vals_to_include = ensure_is_list(vals_to_include)\n\n        return tf_adjustment_chart(\n            self,\n            output_column_name,\n            n_most_freq,\n            n_least_freq,\n            vals_to_include,\n            as_dict,\n        )\n\n    def m_u_parameters_chart(self):\n\"\"\"Display a chart of the m and u parameters of the linkage model\n\n        Examples:\n            ```py\n            linker.m_u_parameters_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.match_weights_chart()\n            save_offline_chart(c.spec, \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500)\n            ```\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n        \"\"\"\n\n        return self._settings_obj.m_u_parameters_chart()\n\n    def cluster_studio_dashboard(\n        self,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        out_path: str,\n        sampling_method=\"random\",\n        sample_size: int = 10,\n        cluster_ids: list = None,\n        cluster_names: list = None,\n        overwrite: bool = False,\n        return_html_as_string=False,\n    ):\n\"\"\"Generate an interactive html visualization of the predicted cluster and\n        save to `out_path`.\n\n        Args:\n            df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n            df_clustered (SplinkDataFrame): The outputs of\n                `linker.cluster_pairwise_predictions_at_threshold()`\n            out_path (str): The path (including filename) to save the html file to.\n            sampling_method (str, optional): `random` or `by_cluster_size`. Defaults to\n                `random`.\n            sample_size (int, optional): Number of clusters to show in the dahboard.\n                Defaults to 10.\n            cluster_ids (list): The IDs of the clusters that will be displayed in the\n                dashboard.  If provided, ignore the `sampling_method` and `sample_size`\n                arguments. Defaults to None.\n            overwrite (bool, optional): Overwrite the html file if it already exists?\n                Defaults to False.\n            cluster_names (list, optional): If provided, the dashboard will display\n                these names in the selection box. Ony works in conjunction with\n                `cluster_ids`.  Defaults to None.\n            return_html_as_string: If True, return the html as a string\n\n        Examples:\n            ```py\n            df_p = linker.predict()\n            df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\n            linker.cluster_studio_dashboard(\n                df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n            )\n            ```\n            Optionally, in Jupyter, you can display the results inline\n            Otherwise you can just load the html file in your browser\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n            ```\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        rendered = render_splink_cluster_studio_html(\n            self,\n            df_predict,\n            df_clustered,\n            out_path,\n            sampling_method=sampling_method,\n            sample_size=sample_size,\n            cluster_ids=cluster_ids,\n            overwrite=overwrite,\n            cluster_names=cluster_names,\n        )\n\n        if return_html_as_string:\n            return rendered\n\n    def save_model_to_json(\n        self, out_path: str | None = None, overwrite: bool = False\n    ) -&gt; dict:\n\"\"\"Save the configuration and parameters of the linkage model to a `.json` file.\n\n        The model can later be loaded back in using `linker.load_model()`.\n        The settings dict is also returned in case you want to save it a different way.\n\n        Examples:\n            ```py\n            linker.save_model_to_json(\"my_settings.json\", overwrite=True)\n            ```\n        Args:\n            out_path (str, optional): File path for json file. If None, don't save to\n                file. Defaults to None.\n            overwrite (bool, optional): Overwrite if already exists? Defaults to False.\n\n        Returns:\n            dict: The settings as a dictionary.\n        \"\"\"\n        model_dict = self._settings_obj.as_dict()\n        if out_path:\n            if os.path.isfile(out_path) and not overwrite:\n                raise ValueError(\n                    f\"The path {out_path} already exists. Please provide a different \"\n                    \"path or set overwrite=True\"\n                )\n            with open(out_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(model_dict, f, indent=4)\n        return model_dict\n\n    def save_settings_to_json(\n        self, out_path: str | None = None, overwrite: bool = False\n    ) -&gt; dict:\n\"\"\"\n        This function is deprecated. Use save_model_to_json() instead.\n        \"\"\"\n        warnings.warn(\n            \"This function is deprecated. Use save_model_to_json() instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return self.save_model_to_json(out_path, overwrite)\n\n    def estimate_probability_two_random_records_match(\n        self, deterministic_matching_rules, recall\n    ):\n\"\"\"Estimate the model parameter `probability_two_random_records_match` using\n        a direct estimation approach.\n\n        See [here](https://github.com/moj-analytical-services/splink/issues/462)\n        for discussion of methodology\n\n        Args:\n            deterministic_matching_rules (list): A list of deterministic matching\n                rules that should be designed to admit very few (none if possible)\n                false positives\n            recall (float): A guess at the recall the deterministic matching rules\n                will attain.  i.e. what proportion of true matches will be recovered\n                by these deterministic rules\n        \"\"\"\n\n        if (recall &gt; 1) or (recall &lt;= 0):\n            raise ValueError(\n                f\"Estimated recall must be greater than 0 \"\n                f\"and no more than 1. Supplied value {recall}.\"\n            )\n\n        # If user, by error, provides a single rule as a string\n        if isinstance(deterministic_matching_rules, str):\n            deterministic_matching_rules = [deterministic_matching_rules]\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self,\n            deterministic_matching_rules,\n        )\n\n        summary_record = records[-1]\n        num_observed_matches = summary_record[\"cumulative_rows\"]\n        num_total_comparisons = summary_record[\"cartesian\"]\n\n        if num_observed_matches &gt; num_total_comparisons * recall:\n            raise ValueError(\n                f\"Deterministic matching rules led to more \"\n                f\"observed matches than is consistent with supplied recall. \"\n                f\"With these rules, recall must be at least \"\n                f\"{num_observed_matches/num_total_comparisons:,.2f}.\"\n            )\n\n        num_expected_matches = num_observed_matches / recall\n        prob = num_expected_matches / num_total_comparisons\n\n        # warn about boundary values, as these will usually be in error\n        if num_observed_matches == 0:\n            logger.warning(\n                f\"WARNING: Deterministic matching rules led to no observed matches! \"\n                f\"This means that no possible record pairs are matches, \"\n                f\"and no records are linked to one another.\\n\"\n                f\"If this is truly the case then you do not need \"\n                f\"to run the linkage model.\\n\"\n                f\"However this is usually in error; \"\n                f\"expected rules to have recall of {100*recall:,.0f}%. \"\n                f\"Consider revising rules as they may have an error.\"\n            )\n        if prob == 1:\n            logger.warning(\n                \"WARNING: Probability two random records match is estimated to be 1.\\n\"\n                \"This means that all possible record pairs are matches, \"\n                \"and all records are linked to one another.\\n\"\n                \"If this is truly the case then you do not need \"\n                \"to run the linkage model.\\n\"\n                \"However, it is more likely that this estimate is faulty. \"\n                \"Perhaps your deterministic matching rules include \"\n                \"too many false positives?\"\n            )\n\n        self._settings_obj._probability_two_random_records_match = prob\n\n        reciprocal_prob = \"Infinity\" if prob == 0 else f\"{1/prob:,.2f}\"\n        logger.info(\n            f\"Probability two random records match is estimated to be  {prob:.3g}.\\n\"\n            f\"This means that amongst all possible pairwise record comparisons, one in \"\n            f\"{reciprocal_prob} are expected to match.  \"\n            f\"With {num_total_comparisons:,.0f} total\"\n            \" possible comparisons, we expect a total of around \"\n            f\"{num_expected_matches:,.2f} matching pairs\"\n        )\n\n    def invalidate_cache(self):\n\"\"\"Invalidate the Splink cache.  Any previously-computed tables\n        will be recomputed.\n        This is useful, for example, if the input data tables have changed.\n        \"\"\"\n        # Before Splink executes a SQL command, it checks the cache to see\n        # whether a table already exists with the name of the output table\n\n        # This function has the effect of changing the names of the output tables\n        # to include a different unique id\n\n        # As a result, any previously cached tables will not be found\n        self._cache_uid = ascii_uid(8)\n\n        # As a result, any previously cached tables will not be found\n        self._intermediate_table_cache.invalidate_cache()\n\n        # Also drop any existing splink tables from the database\n        # Note, this is not actually necessary, it's just good housekeeping\n        self._delete_tables_created_by_splink_from_db()\n\n    def register_table_input_nodes_concat_with_tf(self, input_data, overwrite=False):\n\"\"\"Register a pre-computed version of the input_nodes_concat_with_tf table that\n        you want to re-use e.g. that you created in a previous run\n\n        This method allowed you to register this table in the Splink cache\n        so it will be used rather than Splink computing this table anew.\n\n        Args:\n            input_data: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            overwrite (bool): Overwrite the table in the underlying database if it\n                exists\n        \"\"\"\n\n        table_name_physical = \"__splink__df_concat_with_tf_\" + self._cache_uid\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        self._intermediate_table_cache[\"__splink__df_concat_with_tf\"] = splink_dataframe\n        return splink_dataframe\n\n    def register_table_predict(self, input_data, overwrite=False):\n        table_name_physical = \"__splink__df_predict_\" + self._cache_uid\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        self._intermediate_table_cache[\"__splink__df_predict\"] = splink_dataframe\n        return splink_dataframe\n\n    def register_term_frequency_lookup(self, input_data, col_name, overwrite=False):\n        input_col = InputColumn(col_name, settings_obj=self._settings_obj)\n        table_name_templated = colname_to_tf_tablename(input_col)\n        table_name_physical = f\"{table_name_templated}_{self._cache_uid}\"\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        self._intermediate_table_cache[table_name_templated] = splink_dataframe\n        return splink_dataframe\n\n    def register_labels_table(self, input_data, overwrite=False):\n        table_name_physical = \"__splink__df_labels_\" + ascii_uid(8)\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        return splink_dataframe\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.__init__","title":"<code>__init__(input_table_or_tables, settings_dict, accepted_df_dtypes, set_up_basic_logging=True, input_table_aliases=None)</code>","text":"<p>Initialise the linker object, which manages the data linkage process and holds the data linkage model.</p> <p>Examples:</p> DuckDBSpark <p>Dedupe <pre><code>df = pd.read_csv(\"data_to_dedupe.csv\")\nlinker = DuckDBLinker(df, settings_dict)\n</code></pre> Link <pre><code>df_1 = pd.read_parquet(\"table_1/\")\ndf_2 = pd.read_parquet(\"table_2/\")\nlinker = DuckDBLinker(\n    [df_1, df_2],\n    settings_dict,\n    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n    )\n</code></pre> Dedupe with a pre-trained model read from a json file <pre><code>df = pd.read_csv(\"data_to_dedupe.csv\")\nlinker = DuckDBLinker(df, \"model.json\")\n</code></pre></p> <p>Dedupe <pre><code>df = spark.read.csv(\"data_to_dedupe.csv\")\nlinker = SparkLinker(df, settings_dict)\n</code></pre> Link <pre><code>df_1 = spark.read.parquet(\"table_1/\")\ndf_2 = spark.read.parquet(\"table_2/\")\nlinker = SparkLinker(\n    [df_1, df_2],\n    settings_dict,\n    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n    )\n</code></pre> Dedupe with a pre-trained model read from a json file <pre><code>df = spark.read.csv(\"data_to_dedupe.csv\")\nlinker = SparkLinker(df, \"model.json\")\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>input_table_or_tables</code> <code>Union[str, list]</code> <p>Input data into the linkage model. Either a single string (the name of a table in a database) for deduplication jobs, or a list of strings  (the name of tables in a database) for link_only or link_and_dedupe.  For some linkers, such as the DuckDBLinker and the SparkLinker, it's also possible to pass in dataframes (Pandas and Spark respectively) rather than strings.</p> required <code>settings_dict</code> <code>dict | Path</code> <p>A Splink settings dictionary, or a path to a json defining a settingss dictionary or pre-trained model. If not provided when the object is created, can later be added using <code>linker.load_settings()</code> or <code>linker.load_model()</code> Defaults to None.</p> required <code>set_up_basic_logging</code> <code>bool</code> <p>If true, sets ups up basic logging so that Splink sends messages at INFO level to stdout. Defaults to True.</p> <code>True</code> <code>input_table_aliases</code> <code>Union[str, list]</code> <p>Labels assigned to input tables in Splink outputs.  If the names of the tables in the input database are long or unspecific, this argument can be used to attach more easily readable/interpretable names. Defaults to None.</p> <code>None</code> Source code in <code>splink/linker.py</code> <pre><code>def __init__(\n    self,\n    input_table_or_tables: str | list,\n    settings_dict: dict | Path,\n    accepted_df_dtypes,\n    set_up_basic_logging: bool = True,\n    input_table_aliases: str | list = None,\n):\n\"\"\"Initialise the linker object, which manages the data linkage process and\n    holds the data linkage model.\n\n    Examples:\n        === \"DuckDB\"\n            Dedupe\n            ```py\n            df = pd.read_csv(\"data_to_dedupe.csv\")\n            linker = DuckDBLinker(df, settings_dict)\n            ```\n            Link\n            ```py\n            df_1 = pd.read_parquet(\"table_1/\")\n            df_2 = pd.read_parquet(\"table_2/\")\n            linker = DuckDBLinker(\n                [df_1, df_2],\n                settings_dict,\n                input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                )\n            ```\n            Dedupe with a pre-trained model read from a json file\n            ```py\n            df = pd.read_csv(\"data_to_dedupe.csv\")\n            linker = DuckDBLinker(df, \"model.json\")\n            ```\n        === \"Spark\"\n            Dedupe\n            ```py\n            df = spark.read.csv(\"data_to_dedupe.csv\")\n            linker = SparkLinker(df, settings_dict)\n            ```\n            Link\n            ```py\n            df_1 = spark.read.parquet(\"table_1/\")\n            df_2 = spark.read.parquet(\"table_2/\")\n            linker = SparkLinker(\n                [df_1, df_2],\n                settings_dict,\n                input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                )\n            ```\n            Dedupe with a pre-trained model read from a json file\n            ```py\n            df = spark.read.csv(\"data_to_dedupe.csv\")\n            linker = SparkLinker(df, \"model.json\")\n            ```\n\n    Args:\n        input_table_or_tables (Union[str, list]): Input data into the linkage model.\n            Either a single string (the name of a table in a database) for\n            deduplication jobs, or a list of strings  (the name of tables in a\n            database) for link_only or link_and_dedupe.  For some linkers, such as\n            the DuckDBLinker and the SparkLinker, it's also possible to pass in\n            dataframes (Pandas and Spark respectively) rather than strings.\n        settings_dict (dict | Path, optional): A Splink settings dictionary, or a\n            path to a json defining a settingss dictionary or pre-trained model.\n            If not provided when the object is created, can later be added using\n            `linker.load_settings()` or `linker.load_model()` Defaults to None.\n        set_up_basic_logging (bool, optional): If true, sets ups up basic logging\n            so that Splink sends messages at INFO level to stdout. Defaults to True.\n        input_table_aliases (Union[str, list], optional): Labels assigned to\n            input tables in Splink outputs.  If the names of the tables in the\n            input database are long or unspecific, this argument can be used\n            to attach more easily readable/interpretable names. Defaults to None.\n    \"\"\"\n\n    if set_up_basic_logging:\n        logging.basicConfig(\n            format=\"%(message)s\",\n        )\n        splink_logger = logging.getLogger(\"splink\")\n        splink_logger.setLevel(logging.INFO)\n\n    self._pipeline = SQLPipeline()\n\n    self._names_of_tables_created_by_splink: set = set()\n    self._intermediate_table_cache: dict = CacheDictWithLogging()\n\n    if not isinstance(settings_dict, (dict, type(None))):\n        # Run if you've entered a filepath\n        # feed it a blank settings dictionary\n        self._setup_settings_objs(None)\n        self.load_settings(settings_dict)\n    else:\n        settings_dict = deepcopy(settings_dict)\n        self._setup_settings_objs(settings_dict)\n\n    homogenised_tables, homogenised_aliases = self._register_input_tables(\n        input_table_or_tables,\n        input_table_aliases,\n        accepted_df_dtypes,\n    )\n\n    self._input_tables_dict = self._get_input_tables_dict(\n        homogenised_tables, homogenised_aliases\n    )\n\n    self._validate_input_dfs()\n    self._em_training_sessions = []\n\n    self._find_new_matches_mode = False\n    self._train_u_using_random_sample_mode = False\n    self._compare_two_records_mode = False\n    self._self_link_mode = False\n    self._analyse_blocking_mode = False\n    self._deterministic_link_mode = False\n\n    self.debug_mode = False\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.cluster_pairwise_predictions_at_threshold","title":"<code>cluster_pairwise_predictions_at_threshold(df_predict, threshold_match_probability=None, pairwise_formatting=False, filter_pairwise_format_for_clusters=True)</code>","text":"<p>Clusters the pairwise match predictions that result from <code>linker.predict()</code> into groups of connected record using the connected components graph clustering algorithm</p> <p>Records with an estimated <code>match_probability</code> above <code>threshold_match_probability</code> are considered to be a match (i.e. they represent the same entity).</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The results of <code>linker.predict()</code></p> required <code>threshold_match_probability</code> <code>float</code> <p>Filter the pairwise match predictions to include only pairwise comparisons with a match_probability above this threshold. This dataframe is then fed into the clustering algorithm.</p> <code>None</code> <code>pairwise_formatting</code> <code>bool</code> <p>Whether to output the pairwise match predictions from linker.predict() with cluster IDs. If this is set to false, the output will be a list of all IDs, clustered into groups based on the desired match threshold.</p> <code>False</code> <code>filter_pairwise_format_for_clusters</code> <code>bool</code> <p>If pairwise formatting has been selected, whether to output all columns found within linker.predict(), or just return clusters.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold.</p> Source code in <code>splink/linker.py</code> <pre><code>def cluster_pairwise_predictions_at_threshold(\n    self,\n    df_predict: SplinkDataFrame,\n    threshold_match_probability: float = None,\n    pairwise_formatting: bool = False,\n    filter_pairwise_format_for_clusters: bool = True,\n) -&gt; SplinkDataFrame:\n\"\"\"Clusters the pairwise match predictions that result from `linker.predict()`\n    into groups of connected record using the connected components graph clustering\n    algorithm\n\n    Records with an estimated `match_probability` above\n    `threshold_match_probability` are considered to be a match (i.e. they represent\n    the same entity).\n\n    Args:\n        df_predict (SplinkDataFrame): The results of `linker.predict()`\n        threshold_match_probability (float): Filter the pairwise match predictions\n            to include only pairwise comparisons with a match_probability above this\n            threshold. This dataframe is then fed into the clustering\n            algorithm.\n        pairwise_formatting (bool): Whether to output the pairwise match predictions\n            from linker.predict() with cluster IDs.\n            If this is set to false, the output will be a list of all IDs, clustered\n            into groups based on the desired match threshold.\n        filter_pairwise_format_for_clusters (bool): If pairwise formatting has been\n            selected, whether to output all columns found within linker.predict(),\n            or just return clusters.\n\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered\n            into groups based on the desired match threshold.\n\n    \"\"\"\n\n    # Feeding in df_predict forces materiailisation, if it exists in your database\n    concat_with_tf = self._initialise_df_concat_with_tf(df_predict)\n\n    edges_table = _cc_create_unique_id_cols(\n        self,\n        concat_with_tf.physical_name,\n        df_predict.physical_name,\n        threshold_match_probability,\n    )\n\n    cc = solve_connected_components(\n        self,\n        edges_table,\n        df_predict,\n        concat_with_tf,\n        pairwise_formatting,\n        filter_pairwise_format_for_clusters,\n    )\n\n    return cc\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.cluster_studio_dashboard","title":"<code>cluster_studio_dashboard(df_predict, df_clustered, out_path, sampling_method='random', sample_size=10, cluster_ids=None, cluster_names=None, overwrite=False, return_html_as_string=False)</code>","text":"<p>Generate an interactive html visualization of the predicted cluster and save to <code>out_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.predict()</code></p> required <code>df_clustered</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.cluster_pairwise_predictions_at_threshold()</code></p> required <code>out_path</code> <code>str</code> <p>The path (including filename) to save the html file to.</p> required <code>sampling_method</code> <code>str</code> <p><code>random</code> or <code>by_cluster_size</code>. Defaults to <code>random</code>.</p> <code>'random'</code> <code>sample_size</code> <code>int</code> <p>Number of clusters to show in the dahboard. Defaults to 10.</p> <code>10</code> <code>cluster_ids</code> <code>list</code> <p>The IDs of the clusters that will be displayed in the dashboard.  If provided, ignore the <code>sampling_method</code> and <code>sample_size</code> arguments. Defaults to None.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Overwrite the html file if it already exists? Defaults to False.</p> <code>False</code> <code>cluster_names</code> <code>list</code> <p>If provided, the dashboard will display these names in the selection box. Ony works in conjunction with <code>cluster_ids</code>.  Defaults to None.</p> <code>None</code> <code>return_html_as_string</code> <p>If True, return the html as a string</p> <code>False</code> <p>Examples:</p> <p><pre><code>df_p = linker.predict()\ndf_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\nlinker.cluster_studio_dashboard(\n    df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n)\n</code></pre> Optionally, in Jupyter, you can display the results inline Otherwise you can just load the html file in your browser <pre><code>from IPython.display import IFrame\nIFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n</code></pre></p> Source code in <code>splink/linker.py</code> <pre><code>def cluster_studio_dashboard(\n    self,\n    df_predict: SplinkDataFrame,\n    df_clustered: SplinkDataFrame,\n    out_path: str,\n    sampling_method=\"random\",\n    sample_size: int = 10,\n    cluster_ids: list = None,\n    cluster_names: list = None,\n    overwrite: bool = False,\n    return_html_as_string=False,\n):\n\"\"\"Generate an interactive html visualization of the predicted cluster and\n    save to `out_path`.\n\n    Args:\n        df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n        df_clustered (SplinkDataFrame): The outputs of\n            `linker.cluster_pairwise_predictions_at_threshold()`\n        out_path (str): The path (including filename) to save the html file to.\n        sampling_method (str, optional): `random` or `by_cluster_size`. Defaults to\n            `random`.\n        sample_size (int, optional): Number of clusters to show in the dahboard.\n            Defaults to 10.\n        cluster_ids (list): The IDs of the clusters that will be displayed in the\n            dashboard.  If provided, ignore the `sampling_method` and `sample_size`\n            arguments. Defaults to None.\n        overwrite (bool, optional): Overwrite the html file if it already exists?\n            Defaults to False.\n        cluster_names (list, optional): If provided, the dashboard will display\n            these names in the selection box. Ony works in conjunction with\n            `cluster_ids`.  Defaults to None.\n        return_html_as_string: If True, return the html as a string\n\n    Examples:\n        ```py\n        df_p = linker.predict()\n        df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\n        linker.cluster_studio_dashboard(\n            df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n        )\n        ```\n        Optionally, in Jupyter, you can display the results inline\n        Otherwise you can just load the html file in your browser\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n        ```\n    \"\"\"\n    self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n    rendered = render_splink_cluster_studio_html(\n        self,\n        df_predict,\n        df_clustered,\n        out_path,\n        sampling_method=sampling_method,\n        sample_size=sample_size,\n        cluster_ids=cluster_ids,\n        overwrite=overwrite,\n        cluster_names=cluster_names,\n    )\n\n    if return_html_as_string:\n        return rendered\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.compare_two_records","title":"<code>compare_two_records(record_1, record_2)</code>","text":"<p>Use the linkage model to compare and score a pairwise record comparison based on the two input records provided</p> <p>Parameters:</p> Name Type Description Default <code>record_1</code> <code>dict</code> <p>dictionary representing the first record.  Columns names and data types must be the same as the columns in the settings object</p> required <code>record_2</code> <code>dict</code> <p>dictionary representing the second record.  Columns names and data types must be the same as the columns in the settings object</p> required <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.compare_two_records(record_left, record_right)\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Pairwise comparison with scored prediction</p> Source code in <code>splink/linker.py</code> <pre><code>def compare_two_records(self, record_1: dict, record_2: dict):\n\"\"\"Use the linkage model to compare and score a pairwise record comparison\n    based on the two input records provided\n\n    Args:\n        record_1 (dict): dictionary representing the first record.  Columns names\n            and data types must be the same as the columns in the settings object\n        record_2 (dict): dictionary representing the second record.  Columns names\n            and data types must be the same as the columns in the settings object\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        linker.compare_two_records(record_left, record_right)\n        ```\n\n    Returns:\n        SplinkDataFrame: Pairwise comparison with scored prediction\n    \"\"\"\n    original_blocking_rules = (\n        self._settings_obj._blocking_rules_to_generate_predictions\n    )\n    original_link_type = self._settings_obj._link_type\n\n    self._compare_two_records_mode = True\n    self._settings_obj._blocking_rules_to_generate_predictions = []\n\n    uid = ascii_uid(8)\n    df_records_left = self.register_table(\n        [record_1], f\"__splink__compare_two_records_left_{uid}\", overwrite=True\n    )\n    df_records_left.templated_name = \"__splink__compare_two_records_left\"\n\n    df_records_right = self.register_table(\n        [record_2], f\"__splink__compare_two_records_right_{uid}\", overwrite=True\n    )\n    df_records_right.templated_name = \"__splink__compare_two_records_right\"\n\n    sql_join_tf = _join_tf_to_input_df_sql(self)\n\n    sql_join_tf = sql_join_tf.replace(\n        \"__splink__df_concat\", \"__splink__compare_two_records_left\"\n    )\n    self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_left_with_tf\")\n\n    sql_join_tf = sql_join_tf.replace(\n        \"__splink__compare_two_records_left\", \"__splink__compare_two_records_right\"\n    )\n\n    self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_right_with_tf\")\n\n    sql = block_using_rules_sql(self)\n    self._enqueue_sql(sql, \"__splink__df_blocked\")\n\n    sql = compute_comparison_vector_values_sql(self._settings_obj)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n    sqls = predict_from_comparison_vectors_sqls(\n        self._settings_obj,\n        sql_infinity_expression=self._infinity_expression,\n    )\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    predictions = self._execute_sql_pipeline(\n        [df_records_left, df_records_right], use_cache=False\n    )\n\n    self._settings_obj._blocking_rules_to_generate_predictions = (\n        original_blocking_rules\n    )\n    self._settings_obj._link_type = original_link_type\n    self._compare_two_records_mode = False\n\n    return predictions\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.comparison_viewer_dashboard","title":"<code>comparison_viewer_dashboard(df_predict, out_path, overwrite=False, num_example_rows=2, return_html_as_string=False)</code>","text":"<p>Generate an interactive html visualization of the linker's predictions and save to <code>out_path</code>.  For more information see this video</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.predict()</code></p> required <code>out_path</code> <code>str</code> <p>The path (including filename) to save the html file to.</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite the html file if it already exists? Defaults to False.</p> <code>False</code> <code>num_example_rows</code> <code>int</code> <p>Number of example rows per comparison vector. Defaults to 2.</p> <code>2</code> <code>return_html_as_string</code> <p>If True, return the html as a string</p> <code>False</code> <p>Examples:</p> <pre><code>df_predictions = linker.predict()\nlinker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n</code></pre> <p>Optionally, in Jupyter, you can display the results inline Otherwise you can just load the html file in your browser <pre><code>from IPython.display import IFrame\nIFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n</code></pre></p> Source code in <code>splink/linker.py</code> <pre><code>def comparison_viewer_dashboard(\n    self,\n    df_predict: SplinkDataFrame,\n    out_path: str,\n    overwrite=False,\n    num_example_rows=2,\n    return_html_as_string=False,\n):\n\"\"\"Generate an interactive html visualization of the linker's predictions and\n    save to `out_path`.  For more information see\n    [this video](https://www.youtube.com/watch?v=DNvCMqjipis)\n\n\n    Args:\n        df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n        out_path (str): The path (including filename) to save the html file to.\n        overwrite (bool, optional): Overwrite the html file if it already exists?\n            Defaults to False.\n        num_example_rows (int, optional): Number of example rows per comparison\n            vector. Defaults to 2.\n        return_html_as_string: If True, return the html as a string\n\n    Examples:\n        ```py\n        df_predictions = linker.predict()\n        linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n        ```\n\n        Optionally, in Jupyter, you can display the results inline\n        Otherwise you can just load the html file in your browser\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n        ```\n\n    \"\"\"\n    self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n    sql = comparison_vector_distribution_sql(self)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vector_distribution\")\n\n    sqls = comparison_viewer_table_sqls(self, num_example_rows)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    df = self._execute_sql_pipeline([df_predict])\n\n    rendered = render_splink_comparison_viewer_html(\n        df.as_record_dict(),\n        self._settings_obj._as_completed_dict(),\n        out_path,\n        overwrite,\n    )\n    if return_html_as_string:\n        return rendered\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.count_num_comparisons_from_blocking_rule","title":"<code>count_num_comparisons_from_blocking_rule(blocking_rule)</code>","text":"<p>Compute the number of pairwise record comparisons that would be generated by a blocking rule</p> <p>Parameters:</p> Name Type Description Default <code>blocking_rule</code> <code>str</code> <p>The blocking rule to analyse</p> required <code>link_type</code> <code>str</code> <p>The link type.  This is needed only if the linker has not yet been provided with a settings dictionary.  Defaults to None.</p> required <code>unique_id_column_name</code> <code>str</code> <p>This is needed only if the linker has not yet been provided with a settings dictionary.  Defaults to None.</p> required <p>Examples:</p> <pre><code>br = \"l.first_name = r.first_name\"\nlinker.count_num_comparisons_from_blocking_rule(br)\n</code></pre> <p>19387 <pre><code>br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\"\nlinker.count_num_comparisons_from_blocking_rule(br)\n</code></pre> 394</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of comparisons generated by the blocking rule</p> Source code in <code>splink/linker.py</code> <pre><code>def count_num_comparisons_from_blocking_rule(\n    self,\n    blocking_rule: str,\n) -&gt; int:\n\"\"\"Compute the number of pairwise record comparisons that would be generated by\n    a blocking rule\n\n    Args:\n        blocking_rule (str): The blocking rule to analyse\n        link_type (str, optional): The link type.  This is needed only if the\n            linker has not yet been provided with a settings dictionary.  Defaults\n            to None.\n        unique_id_column_name (str, optional):  This is needed only if the\n            linker has not yet been provided with a settings dictionary.  Defaults\n            to None.\n\n    Examples:\n        ```py\n        br = \"l.first_name = r.first_name\"\n        linker.count_num_comparisons_from_blocking_rule(br)\n        ```\n        &gt; 19387\n        ```py\n        br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n        linker.count_num_comparisons_from_blocking_rule(br)\n        ```\n        &gt; 394\n\n    Returns:\n        int: The number of comparisons generated by the blocking rule\n    \"\"\"\n\n    sql = vertically_concatenate_sql(self)\n    self._enqueue_sql(sql, \"__splink__df_concat\")\n\n    sql = number_of_comparisons_generated_by_blocking_rule_sql(self, blocking_rule)\n    self._enqueue_sql(sql, \"__splink__analyse_blocking_rule\")\n    res = self._execute_sql_pipeline().as_record_dict()[0]\n    return res[\"count_of_pairwise_comparisons_generated\"]\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.count_num_comparisons_from_blocking_rules_for_prediction","title":"<code>count_num_comparisons_from_blocking_rules_for_prediction(df_predict)</code>","text":"<p>Counts the maginal number of edges created from each of the blocking rules in <code>blocking_rules_to_generate_predictions</code></p> <p>This is different to <code>count_num_comparisons_from_blocking_rule</code> because it (a) analyses multiple blocking rules rather than a single rule, and (b) deduplicates any comparisons that are generated, to tell you the marginal effect of each entry in <code>blocking_rules_to_generate_predictions</code></p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>SplinkDataFrame with match weights</p> required <p>Examples:</p> <pre><code>linker = DuckDBLinker(df, connection=\":memory:\")\nlinker.load_settings(\"saved_settings.json\")\ndf_predict = linker.predict(threshold_match_probability=0.95)\ncount_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict)\ncount_pairwise.as_pandas_dataframe(limit=5)\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>A SplinkDataFrame of the pairwise comparisons and estimated pairwise comparisons generated by the blocking rules.</p> Source code in <code>splink/linker.py</code> <pre><code>def count_num_comparisons_from_blocking_rules_for_prediction(self, df_predict):\n\"\"\"Counts the maginal number of edges created from each of the blocking rules\n    in `blocking_rules_to_generate_predictions`\n\n    This is different to `count_num_comparisons_from_blocking_rule`\n    because it (a) analyses multiple blocking rules rather than a single rule, and\n    (b) deduplicates any comparisons that are generated, to tell you the\n    marginal effect of each entry in `blocking_rules_to_generate_predictions`\n\n    Args:\n        df_predict (SplinkDataFrame): SplinkDataFrame with match weights\n        and probabilities of rows matching\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df, connection=\":memory:\")\n        linker.load_settings(\"saved_settings.json\")\n        df_predict = linker.predict(threshold_match_probability=0.95)\n        count_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict)\n        count_pairwise.as_pandas_dataframe(limit=5)\n        ```\n\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons and\n            estimated pairwise comparisons generated by the blocking rules.\n    \"\"\"  # noqa: E501\n    sql = count_num_comparisons_from_blocking_rules_for_prediction_sql(\n        self, df_predict\n    )\n    match_key_analysis = self._sql_to_splink_dataframe_checking_cache(\n        sql, \"__splink__match_key_analysis\"\n    )\n    return match_key_analysis\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.compute_tf_table","title":"<code>compute_tf_table(column_name)</code>","text":"<p>Compute a term frequency table for a given column and persist to the database</p> <p>This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time</p> <p>Examples:</p> DuckDBSpark <p>Real time linkage <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.compute_tf_table(\"surname\")\nlinker.compare_two_records(record_left, record_right)\n</code></pre> Pre-computed term frequency tables <pre><code>linker = DuckDBLinker(df)\ndf_first_name_tf = linker.compute_tf_table(\"first_name\")\ndf_first_name_tf.write.parquet(\"folder/first_name_tf\")\n&gt;&gt;&gt;\n# On subsequent data linking job, read this table rather than recompute\ndf_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\ndf_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n</code></pre></p> <p>Real time linkage <pre><code>linker = SparkLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.compute_tf_table(\"surname\")\nlinker.compare_two_records(record_left, record_right)\n</code></pre> Pre-computed term frequency tables <pre><code>linker = SparkLinker(df)\ndf_first_name_tf = linker.compute_tf_table(\"first_name\")\ndf_first_name_tf.write.parquet(\"folder/first_name_tf\")\n&gt;&gt;&gt;\n# On subsequent data linking job, read this table rather than recompute\ndf_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\ndf_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <code>str</code> <p>The column name in the input table</p> required <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>The resultant table as a splink data frame</p> Source code in <code>splink/linker.py</code> <pre><code>def compute_tf_table(self, column_name: str) -&gt; SplinkDataFrame:\n\"\"\"Compute a term frequency table for a given column and persist to the database\n\n    This method is useful if you want to pre-compute term frequency tables e.g.\n    so that real time linkage executes faster, or so that you can estimate\n    various models without having to recompute term frequency tables each time\n\n    Examples:\n        === \"DuckDB\"\n            Real time linkage\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compute_tf_table(\"surname\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n            Pre-computed term frequency tables\n            ```py\n            linker = DuckDBLinker(df)\n            df_first_name_tf = linker.compute_tf_table(\"first_name\")\n            df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n            &gt;&gt;&gt;\n            # On subsequent data linking job, read this table rather than recompute\n            df_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\n            df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n            ```\n        === \"Spark\"\n            Real time linkage\n            ```py\n            linker = SparkLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compute_tf_table(\"surname\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n            Pre-computed term frequency tables\n            ```py\n            linker = SparkLinker(df)\n            df_first_name_tf = linker.compute_tf_table(\"first_name\")\n            df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n            &gt;&gt;&gt;\n            # On subsequent data linking job, read this table rather than recompute\n            df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\n            df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n            ```\n\n    Args:\n        column_name (str): The column name in the input table\n\n    Returns:\n        SplinkDataFrame: The resultant table as a splink data frame\n    \"\"\"\n\n    input_col = InputColumn(column_name, settings_obj=self._settings_obj)\n    tf_tablename = colname_to_tf_tablename(input_col)\n    cache = self._intermediate_table_cache\n    concat_tf_tables = [\n        remove_quotes_from_identifiers(tf_col.input_name_as_tree).sql()\n        for tf_col in self._settings_obj._term_frequency_columns\n    ]\n\n    if tf_tablename in cache:\n        tf_df = cache[tf_tablename]\n    elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables:\n        self._pipeline.reset()\n        # If our df_concat_with_tf table already exists, use backwards inference to\n        # find a given tf table\n        colname = InputColumn(column_name)\n        sql = term_frequencies_from_concat_with_tf(colname)\n        self._enqueue_sql(sql, colname_to_tf_tablename(colname))\n        tf_df = self._execute_sql_pipeline(\n            [cache[\"__splink__df_concat_with_tf\"]], materialise_as_hash=True\n        )\n        self._intermediate_table_cache[tf_tablename] = tf_df\n    else:\n        # Clear the pipeline if we are materialising\n        self._pipeline.reset()\n        df_concat = self._initialise_df_concat()\n        input_dfs = []\n        if df_concat:\n            input_dfs.append(df_concat)\n        sql = term_frequencies_for_single_column_sql(input_col)\n        self._enqueue_sql(sql, tf_tablename)\n        tf_df = self._execute_sql_pipeline(input_dfs, materialise_as_hash=True)\n        self._intermediate_table_cache[tf_tablename] = tf_df\n\n    return tf_df\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.cumulative_comparisons_from_blocking_rules_records","title":"<code>cumulative_comparisons_from_blocking_rules_records(blocking_rules=None)</code>","text":"<p>Output the number of comparisons generated by each successive blocking rule.</p> <p>This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total.</p> <p>Parameters:</p> Name Type Description Default <code>blocking_rules</code> <code>str or list</code> <p>The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used.</p> <code>None</code> <p>Examples:</p> <pre><code>linker_settings = DuckDBLinker(df, settings)\n# Compute the cumulative number of comparisons generated by the rules\n# in your settings object.\nlinker_settings.cumulative_comparisons_from_blocking_rules_records()\n&gt;&gt;&gt;\n# Generate total comparisons with custom blocking rules.\nblocking_rules = [\n   \"l.surname = r.surname\",\n   \"l.first_name = r.first_name\n    and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n]\n&gt;&gt;&gt;\nlinker_settings.cumulative_comparisons_from_blocking_rules_records(\n    blocking_rules\n )\n</code></pre> <p>Returns:</p> Name Type Description <code>List</code> <p>A list of blocking rules and the corresponding number of comparisons it is forecast to generate.</p> Source code in <code>splink/linker.py</code> <pre><code>def cumulative_comparisons_from_blocking_rules_records(\n    self,\n    blocking_rules: str or list = None,\n):\n\"\"\"Output the number of comparisons generated by each successive blocking rule.\n\n    This is equivalent to the output size of df_predict and details how many\n    comparisons each of your individual blocking rules will contribute to the\n    total.\n\n    Args:\n        blocking_rules (str or list): The blocking rule(s) to compute comparisons\n            for. If null, the rules set out in your settings object will be used.\n\n    Examples:\n        ```py\n        linker_settings = DuckDBLinker(df, settings)\n        # Compute the cumulative number of comparisons generated by the rules\n        # in your settings object.\n        linker_settings.cumulative_comparisons_from_blocking_rules_records()\n        &gt;&gt;&gt;\n        # Generate total comparisons with custom blocking rules.\n        blocking_rules = [\n           \"l.surname = r.surname\",\n           \"l.first_name = r.first_name\n            and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n        ]\n        &gt;&gt;&gt;\n        linker_settings.cumulative_comparisons_from_blocking_rules_records(\n            blocking_rules\n         )\n        ```\n\n    Returns:\n        List: A list of blocking rules and the corresponding number of\n            comparisons it is forecast to generate.\n    \"\"\"\n    if blocking_rules:\n        blocking_rules = ensure_is_list(blocking_rules)\n\n    records = cumulative_comparisons_generated_by_blocking_rules(\n        self, blocking_rules, output_chart=False\n    )\n\n    return records\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.cumulative_num_comparisons_from_blocking_rules_chart","title":"<code>cumulative_num_comparisons_from_blocking_rules_chart(blocking_rules=None)</code>","text":"<p>Display a chart with the cumulative number of comparisons generated by a selection of blocking rules.</p> <p>This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total.</p> <p>Parameters:</p> Name Type Description Default <code>blocking_rules</code> <code>str or list</code> <p>The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used.</p> <code>None</code> <p>Examples:</p> <pre><code>linker_settings = DuckDBLinker(df, settings)\n# Compute the cumulative number of comparisons generated by the rules\n# in your settings object.\nlinker_settings.cumulative_num_comparisons_from_blocking_rules_chart()\n&gt;&gt;&gt;\n# Generate total comparisons with custom blocking rules.\nblocking_rules = [\n   \"l.surname = r.surname\",\n   \"l.first_name = r.first_name\n    and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n]\n&gt;&gt;&gt;\nlinker_settings.cumulative_num_comparisons_from_blocking_rules_chart(\n    blocking_rules\n )\n</code></pre> <p>Returns:</p> Name Type Description <code>VegaLite</code> <p>A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the <code>spec</code> attribute.</p> Source code in <code>splink/linker.py</code> <pre><code>def cumulative_num_comparisons_from_blocking_rules_chart(\n    self,\n    blocking_rules: str or list = None,\n):\n\"\"\"Display a chart with the cumulative number of comparisons generated by a\n    selection of blocking rules.\n\n    This is equivalent to the output size of df_predict and details how many\n    comparisons each of your individual blocking rules will contribute to the\n    total.\n\n    Args:\n        blocking_rules (str or list): The blocking rule(s) to compute comparisons\n            for. If null, the rules set out in your settings object will be used.\n\n    Examples:\n        ```py\n        linker_settings = DuckDBLinker(df, settings)\n        # Compute the cumulative number of comparisons generated by the rules\n        # in your settings object.\n        linker_settings.cumulative_num_comparisons_from_blocking_rules_chart()\n        &gt;&gt;&gt;\n        # Generate total comparisons with custom blocking rules.\n        blocking_rules = [\n           \"l.surname = r.surname\",\n           \"l.first_name = r.first_name\n            and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n        ]\n        &gt;&gt;&gt;\n        linker_settings.cumulative_num_comparisons_from_blocking_rules_chart(\n            blocking_rules\n         )\n        ```\n\n    Returns:\n        VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n            The vegalite spec is available as a dictionary using the `spec`\n            attribute.\n    \"\"\"\n\n    if blocking_rules:\n        blocking_rules = ensure_is_list(blocking_rules)\n\n    records = cumulative_comparisons_generated_by_blocking_rules(\n        self, blocking_rules, output_chart=True\n    )\n\n    return cumulative_blocking_rule_comparisons_generated(records)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.deterministic_link","title":"<code>deterministic_link()</code>","text":"<p>Uses the blocking rules specified by <code>blocking_rules_to_generate_predictions</code> in the settings dictionary to generate pairwise record comparisons.</p> <p>For deterministic linkage, this should be a list of blocking rules which are strict enough to generate only true links.</p> <p>Deterministic linkage, however, is likely to result in missed links (false negatives).</p> <p>Examples:</p> DuckDB <pre><code>from splink.duckdb.duckdb_linker import DuckDBLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = DuckDBLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> Spark <pre><code>from splink.spark.spark_linker import SparkLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = SparkLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> Athena <pre><code>from splink.athena.athena_linker import AthenaLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = AthenaLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> SQLite <pre><code>from splink.sqlite.sqlite_linker import SQLiteLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = SQLiteLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>A SplinkDataFrame of the pairwise comparisons.  This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data.</p> Source code in <code>splink/linker.py</code> <pre><code>def deterministic_link(self) -&gt; SplinkDataFrame:\n\"\"\"Uses the blocking rules specified by\n    `blocking_rules_to_generate_predictions` in the settings dictionary to\n    generate pairwise record comparisons.\n\n    For deterministic linkage, this should be a list of blocking rules which\n    are strict enough to generate only true links.\n\n    Deterministic linkage, however, is likely to result in missed links\n    (false negatives).\n\n    Examples:\n        === \"DuckDB\"\n        ```py\n        from splink.duckdb.duckdb_linker import DuckDBLinker\n\n        settings = {\n            \"link_type\": \"dedupe_only\",\n            \"blocking_rules_to_generate_predictions\": [\n                \"l.first_name = r.first_name\",\n                \"l.surname = r.surname\",\n            ],\n            \"comparisons\": []\n        }\n        &gt;&gt;&gt;\n        linker = DuckDBLinker(df, settings)\n        df = linker.deterministic_link()\n        ```\n        === \"Spark\"\n        ```py\n        from splink.spark.spark_linker import SparkLinker\n\n        settings = {\n            \"link_type\": \"dedupe_only\",\n            \"blocking_rules_to_generate_predictions\": [\n                \"l.first_name = r.first_name\",\n                \"l.surname = r.surname\",\n            ],\n            \"comparisons\": []\n        }\n        &gt;&gt;&gt;\n        linker = SparkLinker(df, settings)\n        df = linker.deterministic_link()\n        ```\n        === \"Athena\"\n        ```py\n        from splink.athena.athena_linker import AthenaLinker\n\n        settings = {\n            \"link_type\": \"dedupe_only\",\n            \"blocking_rules_to_generate_predictions\": [\n                \"l.first_name = r.first_name\",\n                \"l.surname = r.surname\",\n            ],\n            \"comparisons\": []\n        }\n        &gt;&gt;&gt;\n        linker = AthenaLinker(df, settings)\n        df = linker.deterministic_link()\n        ```\n        === \"SQLite\"\n        ```py\n        from splink.sqlite.sqlite_linker import SQLiteLinker\n\n        settings = {\n            \"link_type\": \"dedupe_only\",\n            \"blocking_rules_to_generate_predictions\": [\n                \"l.first_name = r.first_name\",\n                \"l.surname = r.surname\",\n            ],\n            \"comparisons\": []\n        }\n        &gt;&gt;&gt;\n        linker = SQLiteLinker(df, settings)\n        df = linker.deterministic_link()\n        ```\n\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n            represents a table materialised in the database. Methods on the\n            SplinkDataFrame allow you to access the underlying data.\n    \"\"\"\n\n    # Allows clustering during a deterministic linkage.\n    # This is used in `cluster_pairwise_predictions_at_threshold`\n    # to set the cluster threshold to 1\n    self._deterministic_link_mode = True\n\n    concat_with_tf = self._initialise_df_concat_with_tf()\n    sql = block_using_rules_sql(self)\n    self._enqueue_sql(sql, \"__splink__df_blocked\")\n    return self._execute_sql_pipeline([concat_with_tf])\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.estimate_m_from_label_column","title":"<code>estimate_m_from_label_column(label_colname)</code>","text":"<p>Estimate the m parameters of the linkage model from a label (ground truth) column in the input dataframe(s).</p> <p>The m parameters represent the proportion of record comparisons that fall into each comparison level amongst truly matching records.</p> <p>The ground truth column is used to generate pairwise record comparisons which are then assumed to be matches.</p> <p>For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model.</p> <p>Note that this column does not need to be fully populated.  A common case is where a unique identifier such as social security number is only partially populated.</p> <p>Parameters:</p> Name Type Description Default <code>label_colname</code> <code>str</code> <p>The name of the column containing the ground truth label in the input data.</p> required <p>Examples:</p> <pre><code>linker.estimate_m_from_label_column(\"social_security_number\")\n</code></pre> <p>Returns:</p> Type Description <p>Updates the estimated m parameters within the linker object</p> <p>and returns nothing.</p> Source code in <code>splink/linker.py</code> <pre><code>def estimate_m_from_label_column(self, label_colname: str):\n\"\"\"Estimate the m parameters of the linkage model from a label (ground truth)\n    column in the input dataframe(s).\n\n    The m parameters represent the proportion of record comparisons that fall\n    into each comparison level amongst truly matching records.\n\n    The ground truth column is used to generate pairwise record comparisons\n    which are then assumed to be matches.\n\n    For example, if the entity being matched is persons, and your input dataset(s)\n    contain social security number, this could be used to estimate the m values\n    for the model.\n\n    Note that this column does not need to be fully populated.  A common case is\n    where a unique identifier such as social security number is only partially\n    populated.\n\n    Args:\n        label_colname (str): The name of the column containing the ground truth\n            label in the input data.\n\n    Examples:\n        ```py\n        linker.estimate_m_from_label_column(\"social_security_number\")\n        ```\n\n    Returns:\n        Updates the estimated m parameters within the linker object\n        and returns nothing.\n    \"\"\"\n\n    # Ensure this has been run on the main linker so that it can be used by\n    # training linked when it checks the cache\n    self._initialise_df_concat_with_tf()\n    estimate_m_values_from_label_column(\n        self,\n        self._input_tables_dict,\n        label_colname,\n    )\n    self._populate_m_u_from_trained_values()\n\n    self._settings_obj._columns_without_estimated_parameters_message()\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.estimate_parameters_using_expectation_maximisation","title":"<code>estimate_parameters_using_expectation_maximisation(blocking_rule, comparisons_to_deactivate=None, comparison_levels_to_reverse_blocking_rule=None, fix_probability_two_random_records_match=False, fix_m_probabilities=False, fix_u_probabilities=True, populate_probability_two_random_records_match_from_trained_values=False)</code>","text":"<p>Estimate the parameters of the linkage model using expectation maximisation.</p> <p>By default, the m probabilities are estimated, but not the u probabilities, because good estimates for the u probabilities can be obtained from <code>linker.estimate_u_using_random_sampling()</code>.  You can change this by setting <code>fix_u_probabilities</code> to False.</p> <p>The blocking rule provided is used to generate pairwise record comparisons. Usually, this should be a blocking rule that results in a dataframe where matches are between about 1% and 99% of the comparisons.</p> <p>By default, m parameters are estimated for all comparisons except those which are included in the blocking rule.</p> <p>For example, if the blocking rule is <code>l.first_name = r.first_name</code>, then parameter esimates will be made for all comparison except those which use <code>first_name</code> in their sql_condition</p> <p>By default, the probability two random records match is estimated for the blocked data, and then the m and u parameters for the columns specified in the blocking rules are used to estiamte the global probability two random records match.</p> <p>To control which comparisons should have their parameter estimated, and the process of 'reversing out' the global probability two random records match, the user may specify <code>comparisons_to_deactivate</code> and <code>comparison_levels_to_reverse_blocking_rule</code>.   This is useful, for example if you block on the dmetaphone of a column but match on the original column.</p> <p>Examples:</p> <p>Default behaviour <pre><code>br_training = \"l.first_name = r.first_name and l.dob = r.dob\"\nlinker.estimate_parameters_using_expectation_maximisation(br_training)\n</code></pre> Specify which comparisons to deactivate <pre><code>br_training = \"l.dmeta_first_name = r.dmeta_first_name\"\nsettings_obj = linker._settings_obj\ncomp = settings_obj._get_comparison_by_output_column_name(\"first_name\")\ndmeta_level = comp._get_comparison_level_by_comparison_vector_value(1)\nlinker.estimate_parameters_using_expectation_maximisation(\n    br_training,\n    comparisons_to_deactivate=[\"first_name\"],\n    comparison_levels_to_reverse_blocking_rule=[dmeta_level],\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>blocking_rule</code> <code>str</code> <p>The blocking rule used to generate pairwise record comparisons.</p> required <code>comparisons_to_deactivate</code> <code>list</code> <p>By default, splink will analyse the blocking rule provided and estimate the m parameters for all comaprisons except those included in the blocking rule.  If comparisons_to_deactivate are provided, spink will instead estimate m parameters for all comparison except those specified in the comparisons_to_deactivate list.  This list can either contain the output_column_name of the Comparison as a string, or Comparison objects.  Defaults to None.</p> <code>None</code> <code>comparison_levels_to_reverse_blocking_rule</code> <code>list</code> <p>By default, splink will analyse the blocking rule provided and adjust the global probability two random records match to account for the matches specified in the blocking rule. If provided, this argument will overrule this default behaviour. The user must provide a list of ComparisonLevel objects.  Defaults to None.</p> <code>None</code> <code>fix_probability_two_random_records_match</code> <code>bool</code> <p>If True, do not update the probability two random records match after each iteration. Defaults to False.</p> <code>False</code> <code>fix_m_probabilities</code> <code>bool</code> <p>If True, do not update the m probabilities after each iteration. Defaults to False.</p> <code>False</code> <code>fix_u_probabilities</code> <code>bool</code> <p>If True, do not update the u probabilities after each iteration. Defaults to True.</p> <code>True</code> <p>Examples:</p> <pre><code>blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\"\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n</code></pre> <p>Returns:</p> Name Type Description <code>EMTrainingSession</code> <code>EMTrainingSession</code> <p>An object containing information about the training session such as how parameters changed during the iteration history</p> Source code in <code>splink/linker.py</code> <pre><code>def estimate_parameters_using_expectation_maximisation(\n    self,\n    blocking_rule: str,\n    comparisons_to_deactivate: list[str | Comparison] = None,\n    comparison_levels_to_reverse_blocking_rule: list[ComparisonLevel] = None,\n    fix_probability_two_random_records_match: bool = False,\n    fix_m_probabilities=False,\n    fix_u_probabilities=True,\n    populate_probability_two_random_records_match_from_trained_values=False,\n) -&gt; EMTrainingSession:\n\"\"\"Estimate the parameters of the linkage model using expectation maximisation.\n\n    By default, the m probabilities are estimated, but not the u probabilities,\n    because good estimates for the u probabilities can be obtained from\n    `linker.estimate_u_using_random_sampling()`.  You can change this by setting\n    `fix_u_probabilities` to False.\n\n    The blocking rule provided is used to generate pairwise record comparisons.\n    Usually, this should be a blocking rule that results in a dataframe where\n    matches are between about 1% and 99% of the comparisons.\n\n    By default, m parameters are estimated for all comparisons except those which\n    are included in the blocking rule.\n\n    For example, if the blocking rule is `l.first_name = r.first_name`, then\n    parameter esimates will be made for all comparison except those which use\n    `first_name` in their sql_condition\n\n    By default, the probability two random records match is estimated for the\n    blocked data, and then the m and u parameters for the columns specified in the\n    blocking rules are used to estiamte the global probability two random records\n    match.\n\n    To control which comparisons should have their parameter estimated, and the\n    process of 'reversing out' the global probability two random records match, the\n    user may specify `comparisons_to_deactivate` and\n    `comparison_levels_to_reverse_blocking_rule`.   This is useful, for example\n    if you block on the dmetaphone of a column but match on the original column.\n\n    Examples:\n        Default behaviour\n        ```py\n        br_training = \"l.first_name = r.first_name and l.dob = r.dob\"\n        linker.estimate_parameters_using_expectation_maximisation(br_training)\n        ```\n        Specify which comparisons to deactivate\n        ```py\n        br_training = \"l.dmeta_first_name = r.dmeta_first_name\"\n        settings_obj = linker._settings_obj\n        comp = settings_obj._get_comparison_by_output_column_name(\"first_name\")\n        dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1)\n        linker.estimate_parameters_using_expectation_maximisation(\n            br_training,\n            comparisons_to_deactivate=[\"first_name\"],\n            comparison_levels_to_reverse_blocking_rule=[dmeta_level],\n        )\n        ```\n\n    Args:\n        blocking_rule (str): The blocking rule used to generate pairwise record\n            comparisons.\n        comparisons_to_deactivate (list, optional): By default, splink will\n            analyse the blocking rule provided and estimate the m parameters for\n            all comaprisons except those included in the blocking rule.  If\n            comparisons_to_deactivate are provided, spink will instead\n            estimate m parameters for all comparison except those specified\n            in the comparisons_to_deactivate list.  This list can either contain\n            the output_column_name of the Comparison as a string, or Comparison\n            objects.  Defaults to None.\n        comparison_levels_to_reverse_blocking_rule (list, optional): By default,\n            splink will analyse the blocking rule provided and adjust the\n            global probability two random records match to account for the matches\n            specified in the blocking rule. If provided, this argument will overrule\n            this default behaviour. The user must provide a list of ComparisonLevel\n            objects.  Defaults to None.\n        fix_probability_two_random_records_match (bool, optional): If True, do not\n            update the probability two random records match after each iteration.\n            Defaults to False.\n        fix_m_probabilities (bool, optional): If True, do not update the m\n            probabilities after each iteration. Defaults to False.\n        fix_u_probabilities (bool, optional): If True, do not update the u\n            probabilities after each iteration. Defaults to True.\n        populate_probability_two_random_records_match_from_trained_values\n            (bool, optional): If True, derive this parameter from\n            the blocked value. Defaults to False.\n\n    Examples:\n        ```py\n        blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\"\n        linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n        ```\n\n    Returns:\n        EMTrainingSession:  An object containing information about the training\n            session such as how parameters changed during the iteration history\n\n    \"\"\"\n    # Ensure this has been run on the main linker so that it's in the cache\n    # to be used by the training linkers\n    self._initialise_df_concat_with_tf()\n\n    if comparisons_to_deactivate:\n        # If user provided a string, convert to Comparison object\n        comparisons_to_deactivate = [\n            self._settings_obj._get_comparison_by_output_column_name(n)\n            if isinstance(n, str)\n            else n\n            for n in comparisons_to_deactivate\n        ]\n        if comparison_levels_to_reverse_blocking_rule is None:\n            logger.warning(\n                \"\\nWARNING: \\n\"\n                \"You have provided comparisons_to_deactivate but not \"\n                \"comparison_levels_to_reverse_blocking_rule.\\n\"\n                \"If comparisons_to_deactivate is provided, then \"\n                \"you usually need to provide corresponding \"\n                \"comparison_levels_to_reverse_blocking_rule \"\n                \"because each comparison to deactivate is effectively treated \"\n                \"as an exact match.\"\n            )\n\n    em_training_session = EMTrainingSession(\n        self,\n        blocking_rule,\n        fix_u_probabilities=fix_u_probabilities,\n        fix_m_probabilities=fix_m_probabilities,\n        fix_probability_two_random_records_match=fix_probability_two_random_records_match,  # noqa 501\n        comparisons_to_deactivate=comparisons_to_deactivate,\n        comparison_levels_to_reverse_blocking_rule=comparison_levels_to_reverse_blocking_rule,  # noqa 501\n    )\n\n    em_training_session._train()\n\n    self._populate_m_u_from_trained_values()\n\n    if populate_probability_two_random_records_match_from_trained_values:\n        self._populate_probability_two_random_records_match_from_trained_values()\n\n    self._settings_obj._columns_without_estimated_parameters_message()\n\n    return em_training_session\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.estimate_probability_two_random_records_match","title":"<code>estimate_probability_two_random_records_match(deterministic_matching_rules, recall)</code>","text":"<p>Estimate the model parameter <code>probability_two_random_records_match</code> using a direct estimation approach.</p> <p>See here for discussion of methodology</p> <p>Parameters:</p> Name Type Description Default <code>deterministic_matching_rules</code> <code>list</code> <p>A list of deterministic matching rules that should be designed to admit very few (none if possible) false positives</p> required <code>recall</code> <code>float</code> <p>A guess at the recall the deterministic matching rules will attain.  i.e. what proportion of true matches will be recovered by these deterministic rules</p> required Source code in <code>splink/linker.py</code> <pre><code>def estimate_probability_two_random_records_match(\n    self, deterministic_matching_rules, recall\n):\n\"\"\"Estimate the model parameter `probability_two_random_records_match` using\n    a direct estimation approach.\n\n    See [here](https://github.com/moj-analytical-services/splink/issues/462)\n    for discussion of methodology\n\n    Args:\n        deterministic_matching_rules (list): A list of deterministic matching\n            rules that should be designed to admit very few (none if possible)\n            false positives\n        recall (float): A guess at the recall the deterministic matching rules\n            will attain.  i.e. what proportion of true matches will be recovered\n            by these deterministic rules\n    \"\"\"\n\n    if (recall &gt; 1) or (recall &lt;= 0):\n        raise ValueError(\n            f\"Estimated recall must be greater than 0 \"\n            f\"and no more than 1. Supplied value {recall}.\"\n        )\n\n    # If user, by error, provides a single rule as a string\n    if isinstance(deterministic_matching_rules, str):\n        deterministic_matching_rules = [deterministic_matching_rules]\n\n    records = cumulative_comparisons_generated_by_blocking_rules(\n        self,\n        deterministic_matching_rules,\n    )\n\n    summary_record = records[-1]\n    num_observed_matches = summary_record[\"cumulative_rows\"]\n    num_total_comparisons = summary_record[\"cartesian\"]\n\n    if num_observed_matches &gt; num_total_comparisons * recall:\n        raise ValueError(\n            f\"Deterministic matching rules led to more \"\n            f\"observed matches than is consistent with supplied recall. \"\n            f\"With these rules, recall must be at least \"\n            f\"{num_observed_matches/num_total_comparisons:,.2f}.\"\n        )\n\n    num_expected_matches = num_observed_matches / recall\n    prob = num_expected_matches / num_total_comparisons\n\n    # warn about boundary values, as these will usually be in error\n    if num_observed_matches == 0:\n        logger.warning(\n            f\"WARNING: Deterministic matching rules led to no observed matches! \"\n            f\"This means that no possible record pairs are matches, \"\n            f\"and no records are linked to one another.\\n\"\n            f\"If this is truly the case then you do not need \"\n            f\"to run the linkage model.\\n\"\n            f\"However this is usually in error; \"\n            f\"expected rules to have recall of {100*recall:,.0f}%. \"\n            f\"Consider revising rules as they may have an error.\"\n        )\n    if prob == 1:\n        logger.warning(\n            \"WARNING: Probability two random records match is estimated to be 1.\\n\"\n            \"This means that all possible record pairs are matches, \"\n            \"and all records are linked to one another.\\n\"\n            \"If this is truly the case then you do not need \"\n            \"to run the linkage model.\\n\"\n            \"However, it is more likely that this estimate is faulty. \"\n            \"Perhaps your deterministic matching rules include \"\n            \"too many false positives?\"\n        )\n\n    self._settings_obj._probability_two_random_records_match = prob\n\n    reciprocal_prob = \"Infinity\" if prob == 0 else f\"{1/prob:,.2f}\"\n    logger.info(\n        f\"Probability two random records match is estimated to be  {prob:.3g}.\\n\"\n        f\"This means that amongst all possible pairwise record comparisons, one in \"\n        f\"{reciprocal_prob} are expected to match.  \"\n        f\"With {num_total_comparisons:,.0f} total\"\n        \" possible comparisons, we expect a total of around \"\n        f\"{num_expected_matches:,.2f} matching pairs\"\n    )\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.estimate_u_using_random_sampling","title":"<code>estimate_u_using_random_sampling(max_pairs=None, seed=None, *, target_rows=None)</code>","text":"<p>Estimate the u parameters of the linkage model using random sampling.</p> <p>The u parameters represent the proportion of record comparisons that fall into each comparison level amongst truly non-matching records.</p> <p>This procedure takes a sample of the data and generates the cartesian product of pairwise record comparisons amongst the sampled records. The validity of the u values rests on the assumption that the resultant pairwise comparisons are non-matches (or at least, they are very unlikely to be matches). For large datasets, this is typically true.</p> <p>The results of estimate_u_using_random_sampling, and therefore an entire splink model, can be made reproducible by setting the seed parameter. Setting the seed will have performance implications as additional processing is required.</p> <p>Parameters:</p> Name Type Description Default <code>max_pairs</code> <code>int</code> <p>The maximum number of pairwise record comparisons to</p> <code>None</code> <code>seed</code> <code>int</code> <p>Seed for random sampling. Assign to get reproducible u</p> <code>None</code> <p>Examples:</p> <pre><code>linker.estimate_u_using_random_sampling(1e8)\n</code></pre> <p>Returns:</p> Name Type Description <code>None</code> <p>Updates the estimated u parameters within the linker object</p> <p>and returns nothing.</p> Source code in <code>splink/linker.py</code> <pre><code>def estimate_u_using_random_sampling(\n    self, max_pairs: int = None, seed: int = None, *, target_rows=None\n):\n\"\"\"Estimate the u parameters of the linkage model using random sampling.\n\n    The u parameters represent the proportion of record comparisons that fall\n    into each comparison level amongst truly non-matching records.\n\n    This procedure takes a sample of the data and generates the cartesian\n    product of pairwise record comparisons amongst the sampled records.\n    The validity of the u values rests on the assumption that the resultant\n    pairwise comparisons are non-matches (or at least, they are very unlikely to be\n    matches). For large datasets, this is typically true.\n\n    The results of estimate_u_using_random_sampling, and therefore an entire splink\n    model, can be made reproducible by setting the seed parameter. Setting the seed\n    will have performance implications as additional processing is required.\n\n    Args:\n        max_pairs (int): The maximum number of pairwise record comparisons to\n        sample. Larger will give more accurate estimates\n        but lead to longer runtimes.  In our experience at least 1e9 (one billion)\n        gives best results but can take a long time to compute. 1e7 (ten million)\n        is often adequate whilst testing different model specifications, before\n        the final model is estimated.\n        seed (int): Seed for random sampling. Assign to get reproducible u\n        probabilities. Note, seed for random sampling is only supported for\n        DuckDB and Spark, for Athena and SQLite set to None.\n\n    Examples:\n        ```py\n        linker.estimate_u_using_random_sampling(1e8)\n        ```\n\n    Returns:\n        None: Updates the estimated u parameters within the linker object\n        and returns nothing.\n    \"\"\"\n    # TODO: Remove this compatibility code in a future release once we drop\n    # support for \"target_rows\". Deprecation warning added in 3.7.0\n    if max_pairs is not None and target_rows is not None:\n        # user supplied both\n        raise TypeError(\"Just use max_pairs\")\n    elif max_pairs is not None:\n        # user is doing it correctly\n        pass\n    elif target_rows is not None:\n        # user is using deprecated argument\n        warnings.warn(\n            \"target_rows is deprecated; use max_pairs\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        max_pairs = target_rows\n    else:\n        raise TypeError(\"Missing argument max_pairs\")\n\n    estimate_u_values(self, max_pairs, seed)\n    self._populate_m_u_from_trained_values()\n\n    self._settings_obj._columns_without_estimated_parameters_message()\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.find_matches_to_new_records","title":"<code>find_matches_to_new_records(records_or_tablename, blocking_rules=[], match_weight_threshold=-4)</code>","text":"<p>Given one or more records, find records in the input dataset(s) which match and return in order of the splink prediction score.</p> <p>This effectively provides a way of searching the input datasets for given record(s)</p> <p>Parameters:</p> Name Type Description Default <code>records_or_tablename</code> <code>List[dict]</code> <p>Input search record(s) as list of dict, or a table registered to the database.</p> required <code>blocking_rules</code> <code>list</code> <p>Blocking rules to select which records to find and score. If [], do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to [].</p> <code>[]</code> <code>match_weight_threshold</code> <code>int</code> <p>Return matches with a match weight above this threshold. Defaults to -4.</p> <code>-4</code> <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\n# Pre-compute tf tables for any tables with\n# term frequency adjustments\nlinker.compute_tf_table(\"first_name\")\nrecord = {'unique_id': 1,\n    'first_name': \"John\",\n    'surname': \"Smith\",\n    'dob': \"1971-05-24\",\n    'city': \"London\",\n    'email': \"john@smith.net\"\n    }\ndf = linker.find_matches_to_new_records([record], blocking_rules=[])\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>The pairwise comparisons.</p> Source code in <code>splink/linker.py</code> <pre><code>def find_matches_to_new_records(\n    self,\n    records_or_tablename,\n    blocking_rules=[],\n    match_weight_threshold=-4,\n) -&gt; SplinkDataFrame:\n\"\"\"Given one or more records, find records in the input dataset(s) which match\n    and return in order of the splink prediction score.\n\n    This effectively provides a way of searching the input datasets\n    for given record(s)\n\n    Args:\n        records_or_tablename (List[dict]): Input search record(s) as list of dict,\n            or a table registered to the database.\n        blocking_rules (list, optional): Blocking rules to select\n            which records to find and score. If [], do not use a blocking\n            rule - meaning the input records will be compared to all records\n            provided to the linker when it was instantiated. Defaults to [].\n        match_weight_threshold (int, optional): Return matches with a match weight\n            above this threshold. Defaults to -4.\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        # Pre-compute tf tables for any tables with\n        # term frequency adjustments\n        linker.compute_tf_table(\"first_name\")\n        record = {'unique_id': 1,\n            'first_name': \"John\",\n            'surname': \"Smith\",\n            'dob': \"1971-05-24\",\n            'city': \"London\",\n            'email': \"john@smith.net\"\n            }\n        df = linker.find_matches_to_new_records([record], blocking_rules=[])\n        ```\n\n    Returns:\n        SplinkDataFrame: The pairwise comparisons.\n    \"\"\"\n\n    original_blocking_rules = (\n        self._settings_obj._blocking_rules_to_generate_predictions\n    )\n    original_link_type = self._settings_obj._link_type\n\n    if not isinstance(records_or_tablename, str):\n        uid = ascii_uid(8)\n        self.register_table(\n            records_or_tablename, f\"__splink__df_new_records_{uid}\", overwrite=True\n        )\n        new_records_tablename = f\"__splink__df_new_records_{uid}\"\n    else:\n        new_records_tablename = records_or_tablename\n\n    cache = self._intermediate_table_cache\n    input_dfs = []\n    # If our df_concat_with_tf table already exists, use backwards inference to\n    # find all underlying term frequency tables.\n    if \"__splink__df_concat_with_tf\" in cache:\n        concat_with_tf = cache[\"__splink__df_concat_with_tf\"]\n        tf_tables = compute_term_frequencies_from_concat_with_tf(self)\n        # This queues up our tf tables, rather materialising them\n        for tf in tf_tables:\n            # if tf is a SplinkDataFrame, then the table already exists\n            if isinstance(tf, SplinkDataFrame):\n                input_dfs.append(tf)\n            else:\n                self._enqueue_sql(tf[\"sql\"], tf[\"output_table_name\"])\n    else:\n        # This queues up our cols_with_tf and df_concat_with_tf tables.\n        concat_with_tf = self._initialise_df_concat_with_tf(materialise=False)\n\n    if concat_with_tf:\n        input_dfs.append(concat_with_tf)\n\n    rules = []\n    for r in blocking_rules:\n        br_as_obj = BlockingRule(r) if not isinstance(r, BlockingRule) else r\n        br_as_obj.preceding_rules = rules.copy()\n        rules.append(br_as_obj)\n    blocking_rules = rules\n\n    self._settings_obj._blocking_rules_to_generate_predictions = blocking_rules\n\n    self._settings_obj._link_type = \"link_only_find_matches_to_new_records\"\n    self._find_new_matches_mode = True\n\n    sql = _join_tf_to_input_df_sql(self)\n    sql = sql.replace(\"__splink__df_concat\", new_records_tablename)\n    self._enqueue_sql(sql, \"__splink__df_new_records_with_tf\")\n\n    sql = block_using_rules_sql(self)\n    self._enqueue_sql(sql, \"__splink__df_blocked\")\n\n    sql = compute_comparison_vector_values_sql(self._settings_obj)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n    sqls = predict_from_comparison_vectors_sqls(\n        self._settings_obj,\n        sql_infinity_expression=self._infinity_expression,\n    )\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    sql = f\"\"\"\n    select * from __splink__df_predict\n    where match_weight &gt; {match_weight_threshold}\n    \"\"\"\n\n    self._enqueue_sql(sql, \"__splink__find_matches_predictions\")\n\n    predictions = self._execute_sql_pipeline(\n        input_dataframes=input_dfs, use_cache=False\n    )\n\n    self._settings_obj._blocking_rules_to_generate_predictions = (\n        original_blocking_rules\n    )\n    self._settings_obj._link_type = original_link_type\n    self._find_new_matches_mode = False\n\n    return predictions\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.load_settings","title":"<code>load_settings(settings_dict)</code>","text":"<p>Initialise settings for the linker.  To be used if settings were not passed to the linker on creation. This can either be in the form of a settings dictionary or a filepath to a json file containing a valid settings dictionary.</p> <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.load_settings(settings_dict)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>settings_dict</code> <code>dict | str | Path</code> <p>A Splink settings dictionary or the path to your settings json file.</p> required Source code in <code>splink/linker.py</code> <pre><code>def load_settings(self, settings_dict: dict | str | Path):\n\"\"\"Initialise settings for the linker.  To be used if settings were\n    not passed to the linker on creation. This can either be in the form\n    of a settings dictionary or a filepath to a json file containing a\n    valid settings dictionary.\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.profile_columns([\"first_name\", \"surname\"])\n        linker.load_settings(settings_dict)\n        ```\n\n    Args:\n        settings_dict (dict | str | Path): A Splink settings dictionary or\n            the path to your settings json file.\n    \"\"\"\n\n    if not isinstance(settings_dict, dict):\n        p = Path(settings_dict)\n        if not p.is_file():  # check if it's a valid file/filepath\n            raise FileNotFoundError(\n                \"The filepath you have provided is either not a valid file \"\n                \"or doesn't exist along the path provided.\"\n            )\n        settings_dict = json.loads(p.read_text())\n\n    # Store the cache ID so it can be reloaded after cache invalidation\n    cache_id = self._cache_uid\n    # So we don't run into any issues with generated tables having\n    # invalid columns as settings have been tweaked, invalidate\n    # the cache and allow these tables to be recomputed.\n\n    # This is less efficient, but triggers infrequently and ensures we don't\n    # run into issues where the defaults used conflict with the actual values\n    # supplied in settings.\n\n    # This is particularly relevant with `source_dataset`, which appears within\n    # concat_with_tf.\n    self.invalidate_cache()\n\n    # If a uid already exists in your settings object, prioritise this\n    settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", cache_id)\n    settings_dict[\"sql_dialect\"] = settings_dict.get(\n        \"sql_dialect\", self._sql_dialect\n    )\n    self._settings_dict = settings_dict\n    self._settings_obj_ = Settings(settings_dict)\n    self._validate_input_dfs()\n    self._validate_dialect()\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.load_model","title":"<code>load_model(model_path)</code>","text":"<p>Load a pre-defined model from a json file into the linker. This is intended to be used with the output of <code>save_model_to_json()</code>.</p> <p>Examples:</p> <pre><code>linker.load_model(\"my_settings.json\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>Path</code> <p>A path to your model settings json file.</p> required Source code in <code>splink/linker.py</code> <pre><code>def load_model(self, model_path: Path):\n\"\"\"\n    Load a pre-defined model from a json file into the linker.\n    This is intended to be used with the output of\n    `save_model_to_json()`.\n\n    Examples:\n        ```py\n        linker.load_model(\"my_settings.json\")\n        ```\n\n    Args:\n        model_path (Path): A path to your model settings json file.\n    \"\"\"\n\n    return self.load_settings(model_path)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.initialise_settings","title":"<code>initialise_settings(settings_dict)</code>","text":"<p>This method is now deprecated. Please use <code>load_settings</code> when loading existing settings or <code>load_model</code> when loading  a pre-trained model.</p> <p>Initialise settings for the linker.  To be used if settings were not passed to the linker on creation.</p> <p>Examples:</p> DuckDBSparkAthenaSQLite <pre><code>linker = DuckDBLinker(df\")\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre> <pre><code>linker = SparkLinker(df\")\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre> <pre><code>linker = AthenaLinker(df\")\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre> <pre><code>linker = SQLiteLinker(df\")\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.initialise_settings(settings_dict)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>settings_dict</code> <code>dict</code> <p>A Splink settings dictionary</p> required Source code in <code>splink/linker.py</code> <pre><code>def initialise_settings(self, settings_dict: dict):\n\"\"\"*This method is now deprecated. Please use `load_settings`\n    when loading existing settings or `load_model` when loading\n     a pre-trained model.*\n\n    Initialise settings for the linker.  To be used if settings were\n    not passed to the linker on creation.\n    Examples:\n        === \"DuckDB\"\n            ```py\n            linker = DuckDBLinker(df\")\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n        === \"Spark\"\n            ```py\n            linker = SparkLinker(df\")\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n        === \"Athena\"\n            ```py\n            linker = AthenaLinker(df\")\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n        === \"SQLite\"\n            ```py\n            linker = SQLiteLinker(df\")\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.initialise_settings(settings_dict)\n            ```\n    Args:\n        settings_dict (dict): A Splink settings dictionary\n    \"\"\"\n    # If a uid already exists in your settings object, prioritise this\n    settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", self._cache_uid)\n    settings_dict[\"sql_dialect\"] = settings_dict.get(\n        \"sql_dialect\", self._sql_dialect\n    )\n    self._settings_dict = settings_dict\n    self._settings_obj_ = Settings(settings_dict)\n    self._validate_input_dfs()\n    self._validate_dialect()\n\n    warnings.warn(\n        \"`initialise_settings` is deprecated. We advise you use \"\n        \"`linker.load_settings()` when loading in your settings or a previously \"\n        \"trained model.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.load_settings_from_json","title":"<code>load_settings_from_json(in_path)</code>","text":"<p>This method is now deprecated. Please use <code>load_settings</code> when loading existing settings or <code>load_model</code> when loading  a pre-trained model.</p> <p>Load settings from a <code>.json</code> file. This <code>.json</code> file would usually be the output of <code>linker.save_model_to_json()</code></p> <p>Examples:</p> <pre><code>linker.load_settings_from_json(\"my_settings.json\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>in_path</code> <code>str</code> <p>Path to settings json file</p> required Source code in <code>splink/linker.py</code> <pre><code>def load_settings_from_json(self, in_path: str | Path):\n\"\"\"*This method is now deprecated. Please use `load_settings`\n    when loading existing settings or `load_model` when loading\n     a pre-trained model.*\n\n    Load settings from a `.json` file.\n    This `.json` file would usually be the output of\n    `linker.save_model_to_json()`\n    Examples:\n        ```py\n        linker.load_settings_from_json(\"my_settings.json\")\n        ```\n    Args:\n        in_path (str): Path to settings json file\n    \"\"\"\n    self.load_settings(in_path)\n\n    warnings.warn(\n        \"`load_settings_from_json` is deprecated. We advise you use \"\n        \"`linker.load_settings()` when loading in your settings or a previously \"\n        \"trained model.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.m_u_parameters_chart","title":"<code>m_u_parameters_chart()</code>","text":"<p>Display a chart of the m and u parameters of the linkage model</p> <p>Examples:</p> <p><pre><code>linker.m_u_parameters_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.match_weights_chart()\nsave_offline_chart(c.spec, \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500)\n</code></pre></p> <p>Returns:</p> Name Type Description <code>VegaLite</code> <p>A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the <code>spec</code> attribute.</p> Source code in <code>splink/linker.py</code> <pre><code>def m_u_parameters_chart(self):\n\"\"\"Display a chart of the m and u parameters of the linkage model\n\n    Examples:\n        ```py\n        linker.m_u_parameters_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.match_weights_chart()\n        save_offline_chart(c.spec, \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500)\n        ```\n\n    Returns:\n        VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n            The vegalite spec is available as a dictionary using the `spec`\n            attribute.\n    \"\"\"\n\n    return self._settings_obj.m_u_parameters_chart()\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.match_weights_chart","title":"<code>match_weights_chart()</code>","text":"<p>Display a chart of the (partial) match weights of the linkage model</p> <p>Examples:</p> <p><pre><code>linker.match_weights_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.match_weights_chart()\nsave_offline_chart(c.spec, \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500)\n</code></pre></p> <p>Returns:</p> Name Type Description <code>VegaLite</code> <p>A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the <code>spec</code> attribute.</p> Source code in <code>splink/linker.py</code> <pre><code>def match_weights_chart(self):\n\"\"\"Display a chart of the (partial) match weights of the linkage model\n\n    Examples:\n        ```py\n        linker.match_weights_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.match_weights_chart()\n        save_offline_chart(c.spec, \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500)\n        ```\n\n    Returns:\n        VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n            The vegalite spec is available as a dictionary using the `spec`\n            attribute.\n    \"\"\"\n    return self._settings_obj.match_weights_chart()\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.missingness_chart","title":"<code>missingness_chart(input_dataset=None)</code>","text":"<p>Generate a summary chart of the missingness (prevalence of nulls) of columns in the input datasets.  By default, missingness is assessed across all input datasets</p> <p>Parameters:</p> Name Type Description Default <code>input_dataset</code> <code>str</code> <p>Name of one of the input tables in the</p> <code>None</code> <p>Examples:</p> <p><pre><code>linker.missingness_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.missingness_chart()\nsave_offline_chart(c.spec, \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500\n</code></pre></p> Source code in <code>splink/linker.py</code> <pre><code>def missingness_chart(self, input_dataset: str = None):\n\"\"\"Generate a summary chart of the missingness (prevalence of nulls) of\n    columns in the input datasets.  By default, missingness is assessed across\n    all input datasets\n\n    Args:\n        input_dataset (str, optional): Name of one of the input tables in the\n        database.  If provided, missingness will be computed for this table alone.\n        Defaults to None.\n\n    Examples:\n        ```py\n        linker.missingness_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.missingness_chart()\n        save_offline_chart(c.spec, \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500\n        ```\n    \"\"\"\n    records = missingness_data(self, input_dataset)\n    return missingness_chart(records)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.parameter_estimate_comparisons_chart","title":"<code>parameter_estimate_comparisons_chart(include_m=True, include_u=True)</code>","text":"<p>Show a chart that shows how parameter estimates have differed across the different estimation methods you have used.</p> <p>For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates</p> <p>Parameters:</p> Name Type Description Default <code>include_m</code> <code>bool</code> <p>Show different estimates of m values. Defaults to True.</p> <code>True</code> <code>include_u</code> <code>bool</code> <p>Show different estimates of u values. Defaults to True.</p> <code>True</code> Source code in <code>splink/linker.py</code> <pre><code>def parameter_estimate_comparisons_chart(self, include_m=True, include_u=True):\n\"\"\"Show a chart that shows how parameter estimates have differed across\n    the different estimation methods you have used.\n\n    For example, if you have run two EM estimation sessions, blocking on\n    different variables, and both result in parameter estimates for\n    first_name, this chart will enable easy comparison of the different\n    estimates\n\n    Args:\n        include_m (bool, optional): Show different estimates of m values. Defaults\n            to True.\n        include_u (bool, optional): Show different estimates of u values. Defaults\n            to True.\n\n    \"\"\"\n    records = self._settings_obj._parameter_estimates_as_records\n\n    to_retain = []\n    if include_m:\n        to_retain.append(\"m\")\n    if include_u:\n        to_retain.append(\"u\")\n\n    records = [r for r in records if r[\"m_or_u\"] in to_retain]\n\n    return parameter_estimate_comparisons(records)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.precision_recall_chart_from_labels_column","title":"<code>precision_recall_chart_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a precision-recall chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>linker.precision_recall_chart_from_labels_column(\"ground_truth\")\n</code></pre> <p>Returns:</p> Name Type Description <code>VegaLite</code> <p>A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the <code>spec</code> attribute.</p> Source code in <code>splink/linker.py</code> <pre><code>def precision_recall_chart_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n\"\"\"Generate a precision-recall chart from ground truth data, whereby the ground\n    truth is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n    Examples:\n        ```py\n        linker.precision_recall_chart_from_labels_column(\"ground_truth\")\n        ```\n\n    Returns:\n        VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n            The vegalite spec is available as a dictionary using the `spec`\n            attribute.\n    \"\"\"\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return precision_recall_chart(recs)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.precision_recall_chart_from_labels_table","title":"<code>precision_recall_chart_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a precision-recall chart from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered as a table with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSpark <pre><code>labels = pd.read_csv(\"my_labels.csv\")\nlinker.register_table(labels, \"labels\")\nlinker.precision_recall_chart_from_labels_table(\"labels\")\n</code></pre> <pre><code>labels = spark.read.csv(\"my_labels.csv\", header=True)\nlabels.createDataFrame(\"labels\")\nlinker.precision_recall_chart_from_labels_table(\"labels\")\n</code></pre> <p>Returns:</p> Name Type Description <code>VegaLite</code> <p>A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the <code>spec</code> attribute.</p> Source code in <code>splink/linker.py</code> <pre><code>def precision_recall_chart_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n\"\"\"Generate a precision-recall chart from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    as a table with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n    Examples:\n        === \"DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.precision_recall_chart_from_labels_table(\"labels\")\n            ```\n        === \"Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.precision_recall_chart_from_labels_table(\"labels\")\n            ```\n\n    Returns:\n        VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n            The vegalite spec is available as a dictionary using the `spec`\n            attribute.\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return precision_recall_chart(recs)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.predict","title":"<code>predict(threshold_match_probability=None, threshold_match_weight=None, materialise_after_computing_term_frequencies=True)</code>","text":"<p>Create a dataframe of scored pairwise comparisons using the parameters of the linkage model.</p> <p>Uses the blocking rules specified in the <code>blocking_rules_to_generate_predictions</code> of the settings dictionary to generate the pairwise comparisons.</p> <p>Parameters:</p> Name Type Description Default <code>threshold_match_probability</code> <code>float</code> <p>If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None.</p> <code>None</code> <code>threshold_match_weight</code> <code>float</code> <p>If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None.</p> <code>None</code> <code>materialise_after_computing_term_frequencies</code> <code>bool</code> <p>If true, Splink will materialise the table containing the input nodes (rows) joined to any term frequencies which have been asked for in the settings object.  If False, this will be computed as part of one possibly gigantic CTE pipeline.   Defaults to True</p> <code>True</code> <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\ndf = linker.predict(threshold_match_probability=0.95)\ndf.as_pandas_dataframe(limit=5)\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>A SplinkDataFrame of the pairwise comparisons.  This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data.</p> Source code in <code>splink/linker.py</code> <pre><code>def predict(\n    self,\n    threshold_match_probability: float = None,\n    threshold_match_weight: float = None,\n    materialise_after_computing_term_frequencies=True,\n) -&gt; SplinkDataFrame:\n\"\"\"Create a dataframe of scored pairwise comparisons using the parameters\n    of the linkage model.\n\n    Uses the blocking rules specified in the\n    `blocking_rules_to_generate_predictions` of the settings dictionary to\n    generate the pairwise comparisons.\n\n    Args:\n        threshold_match_probability (float, optional): If specified,\n            filter the results to include only pairwise comparisons with a\n            match_probability above this threshold. Defaults to None.\n        threshold_match_weight (float, optional): If specified,\n            filter the results to include only pairwise comparisons with a\n            match_weight above this threshold. Defaults to None.\n        materialise_after_computing_term_frequencies (bool): If true, Splink\n            will materialise the table containing the input nodes (rows)\n            joined to any term frequencies which have been asked\n            for in the settings object.  If False, this will be\n            computed as part of one possibly gigantic CTE\n            pipeline.   Defaults to True\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        df = linker.predict(threshold_match_probability=0.95)\n        df.as_pandas_dataframe(limit=5)\n        ```\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n            represents a table materialised in the database. Methods on the\n            SplinkDataFrame allow you to access the underlying data.\n\n    \"\"\"\n\n    # If materialise_after_computing_term_frequencies=False and the user only\n    # calls predict, it runs as a single pipeline with no materialisation\n    # of anything.\n\n    # _initialise_df_concat_with_tf returns None if the table doesn't exist\n    # and only SQL is queued in this step.\n    nodes_with_tf = self._initialise_df_concat_with_tf(\n        materialise=materialise_after_computing_term_frequencies\n    )\n\n    input_dataframes = []\n    if nodes_with_tf:\n        input_dataframes.append(nodes_with_tf)\n\n    sql = block_using_rules_sql(self)\n    self._enqueue_sql(sql, \"__splink__df_blocked\")\n\n    repartition_after_blocking = getattr(self, \"repartition_after_blocking\", False)\n\n    # repartition after blocking only exists on the SparkLinker\n    if repartition_after_blocking:\n        df_blocked = self._execute_sql_pipeline(input_dataframes)\n        input_dataframes.append(df_blocked)\n\n    sql = compute_comparison_vector_values_sql(self._settings_obj)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n    sqls = predict_from_comparison_vectors_sqls(\n        self._settings_obj,\n        threshold_match_probability,\n        threshold_match_weight,\n        sql_infinity_expression=self._infinity_expression,\n    )\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    predictions = self._execute_sql_pipeline(input_dataframes)\n    self._predict_warning()\n    return predictions\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.prediction_errors_from_labels_table","title":"<code>prediction_errors_from_labels_table(labels_splinkdataframe_or_table_name, include_false_positives=True, include_false_negatives=True, threshold=0.5)</code>","text":"<p>Generate a dataframe containing false positives and false negatives based on the comparison between the clerical_match_score in the labels table compared with the splink predicted match probability</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>include_false_positives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>include_false_negatives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>threshold</code> <code>float</code> <p>Threshold above which a score is considered to be a match. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Table containing false positives and negatives</p> Source code in <code>splink/linker.py</code> <pre><code>def prediction_errors_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    include_false_positives=True,\n    include_false_negatives=True,\n    threshold=0.5,\n):\n\"\"\"Generate a dataframe containing false positives and false negatives\n    based on the comparison between the clerical_match_score in the labels\n    table compared with the splink predicted match probability\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        include_false_positives (bool, optional): Defaults to True.\n        include_false_negatives (bool, optional): Defaults to True.\n        threshold (float, optional): Threshold above which a score is considered\n            to be a match. Defaults to 0.5.\n\n    Returns:\n        SplinkDataFrame:  Table containing false positives and negatives\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    return prediction_errors_from_labels_table(\n        self,\n        labels_tablename,\n        include_false_positives,\n        include_false_negatives,\n        threshold,\n    )\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.profile_columns","title":"<code>profile_columns(column_expressions, top_n=10, bottom_n=10)</code>","text":"Source code in <code>splink/linker.py</code> <pre><code>def profile_columns(\n    self, column_expressions: str | list[str], top_n=10, bottom_n=10\n):\n    return profile_columns(self, column_expressions, top_n=top_n, bottom_n=bottom_n)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.query_sql","title":"<code>query_sql(sql, output_type='pandas')</code>","text":"<p>Run a SQL query against your backend database and return the resulting output.</p> <p>Examples:</p> DuckDBSparkAthenaSQLite <pre><code>linker = DuckDBLinker(df, settings)\ndf_predict = linker.predict()\nlinker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n</code></pre> <pre><code>linker = SparkLinker(df, settings)\ndf_predict = linker.predict()\nlinker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n</code></pre> <pre><code>linker = AthenaLinker(df, settings)\ndf_predict = linker.predict()\nlinker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n</code></pre> <p>```py linker = SQLiteLinker(df, settings) df_predict = linker.predict() linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")</p> <p>```</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>The SQL to be queried.</p> required <code>output_type</code> <code>str</code> <p>One of splink_df/splinkdf or pandas. This determines the type of table that your results are output in.</p> <code>'pandas'</code> Source code in <code>splink/linker.py</code> <pre><code>def query_sql(self, sql, output_type=\"pandas\"):\n\"\"\"\n    Run a SQL query against your backend database and return\n    the resulting output.\n\n    Examples:\n        === \"DuckDB\"\n            ```py\n            linker = DuckDBLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n        === \"Spark\"\n            ```py\n            linker = SparkLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n        === \"Athena\"\n            ```py\n            linker = AthenaLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n        === \"SQLite\"\n            ```py\n            linker = SQLiteLinker(df, settings)\n            df_predict = linker.predict()\n            linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n        ```\n\n    Args:\n        sql (str): The SQL to be queried.\n        output_type (str): One of splink_df/splinkdf or pandas.\n            This determines the type of table that your results are output in.\n    \"\"\"\n\n    output_tablename_templated = \"__splink__df_sql_query\"\n\n    splink_dataframe = self._sql_to_splink_dataframe_checking_cache(\n        sql,\n        output_tablename_templated,\n        materialise_as_hash=False,\n        use_cache=False,\n    )\n\n    if output_type in (\"splink_df\", \"splinkdf\"):\n        return splink_dataframe\n    elif output_type == \"pandas\":\n        out = splink_dataframe.as_pandas_dataframe()\n        # If pandas, drop the table to cleanup the db\n        splink_dataframe.drop_table_from_database()\n        return out\n    else:\n        raise ValueError(\n            f\"output_type '{output_type}' is not supported.\",\n            \"Must be one of 'splink_df'/'splinkdf' or 'pandas'\",\n        )\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.register_table","title":"<code>register_table(input, table_name, overwrite=False)</code>","text":"<p>Register a table to your backend database, to be used in one of the splink methods, or simply to allow querying.</p> <p>Tables can be of type: dictionary, record level dictionary, pandas dataframe, pyarrow table and in the spark case, a spark df.</p> <p>Examples:</p> <pre><code>test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]}\nlinker.register_table(test_dict, \"test_dict\")\nlinker.query_sql(\"select * from test_dict\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input</code> <p>The data you wish to register. This can be either a dictionary, pandas dataframe, pyarrow table or a spark dataframe.</p> required <code>table_name</code> <code>str</code> <p>The name you wish to assign to the table.</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite the table in the underlying database if it exists</p> <code>False</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>An abstraction representing the table created by the sql pipeline</p> Source code in <code>splink/linker.py</code> <pre><code>def register_table(self, input, table_name, overwrite=False):\n\"\"\"\n    Register a table to your backend database, to be used in one of the\n    splink methods, or simply to allow querying.\n\n    Tables can be of type: dictionary, record level dictionary,\n    pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n    Examples:\n        ```py\n        test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]}\n        linker.register_table(test_dict, \"test_dict\")\n        linker.query_sql(\"select * from test_dict\")\n        ```\n\n    Args:\n        input: The data you wish to register. This can be either a dictionary,\n            pandas dataframe, pyarrow table or a spark dataframe.\n        table_name (str): The name you wish to assign to the table.\n        overwrite (bool): Overwrite the table in the underlying database if it\n            exists\n\n    Returns:\n        SplinkDataFrame: An abstraction representing the table created by the sql\n            pipeline\n    \"\"\"\n\n    raise NotImplementedError(f\"register_table not implemented for {type(self)}\")\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.roc_chart_from_labels_column","title":"<code>roc_chart_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a ROC chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>linker.roc_chart_from_labels_column(\"labels\")\n</code></pre> <p>Returns:</p> Name Type Description <code>VegaLite</code> <p>A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the <code>spec</code> attribute.</p> Source code in <code>splink/linker.py</code> <pre><code>def roc_chart_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n\"\"\"Generate a ROC chart from ground truth data, whereby the ground truth\n    is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        ```py\n        linker.roc_chart_from_labels_column(\"labels\")\n        ```\n\n    Returns:\n        VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n            The vegalite spec is available as a dictionary using the `spec`\n            attribute.\n    \"\"\"\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return roc_chart(recs)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.roc_chart_from_labels_table","title":"<code>roc_chart_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a ROC chart from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSpark <pre><code>labels = pd.read_csv(\"my_labels.csv\")\nlinker.register_table(labels, \"labels\")\nlinker.roc_chart_from_labels_table(\"labels\")\n</code></pre> <pre><code>labels = spark.read.csv(\"my_labels.csv\", header=True)\nlabels.createDataFrame(\"labels\")\nlinker.roc_chart_from_labels_table(\"labels\")\n</code></pre> <p>Returns:</p> Name Type Description <code>VegaLite</code> <p>A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the <code>spec</code> attribute.</p> Source code in <code>splink/linker.py</code> <pre><code>def roc_chart_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name: str | SplinkDataFrame,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n\"\"\"Generate a ROC chart from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        === \"DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.roc_chart_from_labels_table(\"labels\")\n            ```\n        === \"Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.roc_chart_from_labels_table(\"labels\")\n            ```\n\n    Returns:\n        VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n            The vegalite spec is available as a dictionary using the `spec`\n            attribute.\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return roc_chart(recs)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.save_model_to_json","title":"<code>save_model_to_json(out_path=None, overwrite=False)</code>","text":"<p>Save the configuration and parameters of the linkage model to a <code>.json</code> file.</p> <p>The model can later be loaded back in using <code>linker.load_model()</code>. The settings dict is also returned in case you want to save it a different way.</p> <p>Examples:</p> <pre><code>linker.save_model_to_json(\"my_settings.json\", overwrite=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>out_path</code> <code>str</code> <p>File path for json file. If None, don't save to file. Defaults to None.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Overwrite if already exists? Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The settings as a dictionary.</p> Source code in <code>splink/linker.py</code> <pre><code>def save_model_to_json(\n    self, out_path: str | None = None, overwrite: bool = False\n) -&gt; dict:\n\"\"\"Save the configuration and parameters of the linkage model to a `.json` file.\n\n    The model can later be loaded back in using `linker.load_model()`.\n    The settings dict is also returned in case you want to save it a different way.\n\n    Examples:\n        ```py\n        linker.save_model_to_json(\"my_settings.json\", overwrite=True)\n        ```\n    Args:\n        out_path (str, optional): File path for json file. If None, don't save to\n            file. Defaults to None.\n        overwrite (bool, optional): Overwrite if already exists? Defaults to False.\n\n    Returns:\n        dict: The settings as a dictionary.\n    \"\"\"\n    model_dict = self._settings_obj.as_dict()\n    if out_path:\n        if os.path.isfile(out_path) and not overwrite:\n            raise ValueError(\n                f\"The path {out_path} already exists. Please provide a different \"\n                \"path or set overwrite=True\"\n            )\n        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(model_dict, f, indent=4)\n    return model_dict\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.save_settings_to_json","title":"<code>save_settings_to_json(out_path=None, overwrite=False)</code>","text":"<p>This function is deprecated. Use save_model_to_json() instead.</p> Source code in <code>splink/linker.py</code> <pre><code>def save_settings_to_json(\n    self, out_path: str | None = None, overwrite: bool = False\n) -&gt; dict:\n\"\"\"\n    This function is deprecated. Use save_model_to_json() instead.\n    \"\"\"\n    warnings.warn(\n        \"This function is deprecated. Use save_model_to_json() instead.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    return self.save_model_to_json(out_path, overwrite)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.truth_space_table_from_labels_column","title":"<code>truth_space_table_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart.</p> <p>Your labels_column_name should include the ground truth cluster (unique identifier) that groups entities which are the same</p> <p>Parameters:</p> Name Type Description Default <code>labels_tablename</code> <code>str</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>linker.truth_space_table_from_labels_column(\"cluster\")\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Table of truth statistics</p> Source code in <code>splink/linker.py</code> <pre><code>def truth_space_table_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n\"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n    match_probability, suitable for plotting a ROC chart.\n\n    Your labels_column_name should include the ground truth cluster (unique\n    identifier) that groups entities which are the same\n\n    Args:\n        labels_tablename (str): Name of table containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        ```py\n        linker.truth_space_table_from_labels_column(\"cluster\")\n        ```\n\n    Returns:\n        SplinkDataFrame:  Table of truth statistics\n    \"\"\"\n\n    return truth_space_table_from_labels_column(\n        self, labels_column_name, threshold_actual, match_weight_round_to_nearest\n    )\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.truth_space_table_from_labels_table","title":"<code>truth_space_table_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart.</p> <p>The table of labels should be in the following format, and should be registered with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSpark <pre><code>labels = pd.read_csv(\"my_labels.csv\")\nlinker.register_table(labels, \"labels\")\nlinker.truth_space_table_from_labels_table(\"labels\")\n</code></pre> <pre><code>labels = spark.read.csv(\"my_labels.csv\", header=True)\nlabels.createDataFrame(\"labels\")\nlinker.truth_space_table_from_labels_table(\"labels\")\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>Table of truth statistics</p> Source code in <code>splink/linker.py</code> <pre><code>def truth_space_table_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n) -&gt; SplinkDataFrame:\n\"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n    match_probability, suitable for plotting a ROC chart.\n\n    The table of labels should be in the following format, and should be registered\n    with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        === \"DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.truth_space_table_from_labels_table(\"labels\")\n            ```\n        === \"Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.truth_space_table_from_labels_table(\"labels\")\n            ```\n    Returns:\n        SplinkDataFrame:  Table of truth statistics\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    return truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.unlinkables_chart","title":"<code>unlinkables_chart(x_col='match_weight', source_dataset=None, as_dict=False)</code>","text":"<p>Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters.</p> <p>Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match.</p> <p>Parameters:</p> Name Type Description Default <code>x_col</code> <code>str</code> <p>Column to use for the x-axis. Defaults to \"match_weight\".</p> <code>'match_weight'</code> <code>source_dataset</code> <code>str</code> <p>Name of the source dataset to use for the title of the output chart.</p> <code>None</code> <code>as_dict</code> <code>bool</code> <p>If True, return a dict version of the chart.</p> <code>False</code> <p>Examples:</p> <p>For the simplest code pipeline, load a pre-trained model and run this against the test data. <pre><code>df = pd.read_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\")\nlinker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.unlinkables_chart()\n</code></pre> For more complex code pipelines, you can run an entire pipeline that estimates your m and u values, before `unlinkables_chart().</p> <p>Returns:</p> Name Type Description <code>VegaLite</code> <p>A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the <code>spec</code> attribute.</p> Source code in <code>splink/linker.py</code> <pre><code>def unlinkables_chart(\n    self,\n    x_col=\"match_weight\",\n    source_dataset=None,\n    as_dict=False,\n):\n\"\"\"Generate an interactive chart displaying the proportion of records that\n    are \"unlinkable\" for a given splink score threshold and model parameters.\n\n    Unlinkable records are those that, even when compared with themselves, do not\n    contain enough information to confirm a match.\n\n    Args:\n        x_col (str, optional): Column to use for the x-axis.\n            Defaults to \"match_weight\".\n        source_dataset (str, optional): Name of the source dataset to use for\n            the title of the output chart.\n        as_dict (bool, optional): If True, return a dict version of the chart.\n\n    Examples:\n        For the simplest code pipeline, load a pre-trained model\n        and run this against the test data.\n        ```py\n        df = pd.read_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\")\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        linker.unlinkables_chart()\n        ```\n        For more complex code pipelines, you can run an entire pipeline\n        that estimates your m and u values, before `unlinkables_chart().\n\n    Returns:\n        VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n            The vegalite spec is available as a dictionary using the `spec`\n            attribute.\n    \"\"\"\n\n    # Link our initial df on itself and calculate the % of unlinkable entries\n    records = unlinkables_data(self)\n    return unlinkables_chart(records, x_col, source_dataset, as_dict)\n</code></pre>","tags":["API"]},{"location":"linker.html#splink.linker.Linker.waterfall_chart","title":"<code>waterfall_chart(records, filter_nulls=True)</code>","text":"<p>Visualise how the final match weight is computed for the provided pairwise record comparisons.</p> <p>Records must be provided as a list of dictionaries. This would usually be obtained from <code>df.as_record_dict(limit=n)</code> where <code>df</code> is a SplinkDataFrame.</p> <p>Examples:</p> <pre><code>df = linker.predict(threshold_match_weight=2)\nrecords = df.as_record_dict(limit=10)\nlinker.waterfall_chart(records)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>records</code> <code>List[dict]</code> <p>Usually be obtained from <code>df.as_record_dict(limit=n)</code> where <code>df</code> is a SplinkDataFrame.</p> required <code>filter_nulls</code> <code>bool</code> <p>Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>VegaLite</code> <p>A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the <code>spec</code> attribute.</p> Source code in <code>splink/linker.py</code> <pre><code>def waterfall_chart(self, records: list[dict], filter_nulls=True):\n\"\"\"Visualise how the final match weight is computed for the provided pairwise\n    record comparisons.\n\n    Records must be provided as a list of dictionaries. This would usually be\n    obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame.\n\n    Examples:\n        ```py\n        df = linker.predict(threshold_match_weight=2)\n        records = df.as_record_dict(limit=10)\n        linker.waterfall_chart(records)\n        ```\n\n    Args:\n        records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)`\n            where `df` is a SplinkDataFrame.\n        filter_nulls (bool, optional): Whether the visualiation shows null\n            comparisons, which have no effect on final match weight. Defaults to\n            True.\n\n\n    Returns:\n        VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n            The vegalite spec is available as a dictionary using the `spec`\n            attribute.\n\n    \"\"\"\n    self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n    return waterfall_chart(records, self._settings_obj, filter_nulls)\n</code></pre>","tags":["API"]},{"location":"linkerest.html","title":"Documentation for <code>Linker</code> object methods related to parameter estimation","text":"<p>The Linker object manages the data linkage process and holds the data linkage model.</p> <p>Most of Splink's functionality can  be accessed by calling methods (functions) on the linker, such as <code>linker.predict()</code>, <code>linker.profile_columns()</code> etc.</p> <p>The Linker class is intended for subclassing for specific backends, e.g. a <code>DuckDBLinker</code>.</p>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.estimate_m_from_label_column","title":"<code>estimate_m_from_label_column(label_colname)</code>","text":"<p>Estimate the m parameters of the linkage model from a label (ground truth) column in the input dataframe(s).</p> <p>The m parameters represent the proportion of record comparisons that fall into each comparison level amongst truly matching records.</p> <p>The ground truth column is used to generate pairwise record comparisons which are then assumed to be matches.</p> <p>For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model.</p> <p>Note that this column does not need to be fully populated.  A common case is where a unique identifier such as social security number is only partially populated.</p> <p>Parameters:</p> Name Type Description Default <code>label_colname</code> <code>str</code> <p>The name of the column containing the ground truth label in the input data.</p> required <p>Examples:</p> <pre><code>linker.estimate_m_from_label_column(\"social_security_number\")\n</code></pre> <p>Returns:</p> Type Description <p>Updates the estimated m parameters within the linker object</p> <p>and returns nothing.</p>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.estimate_parameters_using_expectation_maximisation","title":"<code>estimate_parameters_using_expectation_maximisation(blocking_rule, comparisons_to_deactivate=None, comparison_levels_to_reverse_blocking_rule=None, fix_probability_two_random_records_match=False, fix_m_probabilities=False, fix_u_probabilities=True, populate_probability_two_random_records_match_from_trained_values=False)</code>","text":"<p>Estimate the parameters of the linkage model using expectation maximisation.</p> <p>By default, the m probabilities are estimated, but not the u probabilities, because good estimates for the u probabilities can be obtained from <code>linker.estimate_u_using_random_sampling()</code>.  You can change this by setting <code>fix_u_probabilities</code> to False.</p> <p>The blocking rule provided is used to generate pairwise record comparisons. Usually, this should be a blocking rule that results in a dataframe where matches are between about 1% and 99% of the comparisons.</p> <p>By default, m parameters are estimated for all comparisons except those which are included in the blocking rule.</p> <p>For example, if the blocking rule is <code>l.first_name = r.first_name</code>, then parameter esimates will be made for all comparison except those which use <code>first_name</code> in their sql_condition</p> <p>By default, the probability two random records match is estimated for the blocked data, and then the m and u parameters for the columns specified in the blocking rules are used to estiamte the global probability two random records match.</p> <p>To control which comparisons should have their parameter estimated, and the process of 'reversing out' the global probability two random records match, the user may specify <code>comparisons_to_deactivate</code> and <code>comparison_levels_to_reverse_blocking_rule</code>.   This is useful, for example if you block on the dmetaphone of a column but match on the original column.</p> <p>Examples:</p> <p>Default behaviour <pre><code>br_training = \"l.first_name = r.first_name and l.dob = r.dob\"\nlinker.estimate_parameters_using_expectation_maximisation(br_training)\n</code></pre> Specify which comparisons to deactivate <pre><code>br_training = \"l.dmeta_first_name = r.dmeta_first_name\"\nsettings_obj = linker._settings_obj\ncomp = settings_obj._get_comparison_by_output_column_name(\"first_name\")\ndmeta_level = comp._get_comparison_level_by_comparison_vector_value(1)\nlinker.estimate_parameters_using_expectation_maximisation(\n    br_training,\n    comparisons_to_deactivate=[\"first_name\"],\n    comparison_levels_to_reverse_blocking_rule=[dmeta_level],\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>blocking_rule</code> <code>str</code> <p>The blocking rule used to generate pairwise record comparisons.</p> required <code>comparisons_to_deactivate</code> <code>list</code> <p>By default, splink will analyse the blocking rule provided and estimate the m parameters for all comaprisons except those included in the blocking rule.  If comparisons_to_deactivate are provided, spink will instead estimate m parameters for all comparison except those specified in the comparisons_to_deactivate list.  This list can either contain the output_column_name of the Comparison as a string, or Comparison objects.  Defaults to None.</p> <code>None</code> <code>comparison_levels_to_reverse_blocking_rule</code> <code>list</code> <p>By default, splink will analyse the blocking rule provided and adjust the global probability two random records match to account for the matches specified in the blocking rule. If provided, this argument will overrule this default behaviour. The user must provide a list of ComparisonLevel objects.  Defaults to None.</p> <code>None</code> <code>fix_probability_two_random_records_match</code> <code>bool</code> <p>If True, do not update the probability two random records match after each iteration. Defaults to False.</p> <code>False</code> <code>fix_m_probabilities</code> <code>bool</code> <p>If True, do not update the m probabilities after each iteration. Defaults to False.</p> <code>False</code> <code>fix_u_probabilities</code> <code>bool</code> <p>If True, do not update the u probabilities after each iteration. Defaults to True.</p> <code>True</code> <p>Examples:</p> <pre><code>blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\"\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n</code></pre> <p>Returns:</p> Name Type Description <code>EMTrainingSession</code> <code>EMTrainingSession</code> <p>An object containing information about the training session such as how parameters changed during the iteration history</p>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.estimate_u_using_random_sampling","title":"<code>estimate_u_using_random_sampling(max_pairs=None, seed=None, *, target_rows=None)</code>","text":"<p>Estimate the u parameters of the linkage model using random sampling.</p> <p>The u parameters represent the proportion of record comparisons that fall into each comparison level amongst truly non-matching records.</p> <p>This procedure takes a sample of the data and generates the cartesian product of pairwise record comparisons amongst the sampled records. The validity of the u values rests on the assumption that the resultant pairwise comparisons are non-matches (or at least, they are very unlikely to be matches). For large datasets, this is typically true.</p> <p>The results of estimate_u_using_random_sampling, and therefore an entire splink model, can be made reproducible by setting the seed parameter. Setting the seed will have performance implications as additional processing is required.</p> <p>Parameters:</p> Name Type Description Default <code>max_pairs</code> <code>int</code> <p>The maximum number of pairwise record comparisons to</p> <code>None</code> <code>seed</code> <code>int</code> <p>Seed for random sampling. Assign to get reproducible u</p> <code>None</code> <p>Examples:</p> <pre><code>linker.estimate_u_using_random_sampling(1e8)\n</code></pre> <p>Returns:</p> Name Type Description <code>None</code> <p>Updates the estimated u parameters within the linker object</p> <p>and returns nothing.</p>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.save_model_to_json","title":"<code>save_model_to_json(out_path=None, overwrite=False)</code>","text":"<p>Save the configuration and parameters of the linkage model to a <code>.json</code> file.</p> <p>The model can later be loaded back in using <code>linker.load_model()</code>. The settings dict is also returned in case you want to save it a different way.</p> <p>Examples:</p> <pre><code>linker.save_model_to_json(\"my_settings.json\", overwrite=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>out_path</code> <code>str</code> <p>File path for json file. If None, don't save to file. Defaults to None.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Overwrite if already exists? Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The settings as a dictionary.</p>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.estimate_m_from_pairwise_labels","title":"<code>estimate_m_from_pairwise_labels(labels_splinkdataframe_or_table_name)</code>","text":"<p>Estimate the m parameters of the linkage model from a dataframe of pairwise labels.</p> <p>The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r| |----------------|-----------|----------------|-----------| |df_1            |1          |df_2            |2          | |df_1            |1          |df_2            |3          |</p> <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object. Note that at the moment, this method does not respect values in a <code>clerical_match_score</code> column.  If provided, these are ignored and it is assumed that every row in the table of labels is a score of 1, i.e. a perfect match.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str</code> <p>Name of table containing labels in the database or SplinkDataframe</p> required <p>Examples:</p> <pre><code>pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\")\nlinker.register_table(pairwise_labels, \"labels\", overwrite=True)\nlinker.estimate_m_from_pairwise_labels(\"labels\")\n</code></pre>","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerest.html#splink.linker.Linker.estimate_probability_two_random_records_match","title":"<code>estimate_probability_two_random_records_match(deterministic_matching_rules, recall)</code>","text":"<p>Estimate the model parameter <code>probability_two_random_records_match</code> using a direct estimation approach.</p> <p>See here for discussion of methodology</p> <p>Parameters:</p> Name Type Description Default <code>deterministic_matching_rules</code> <code>list</code> <p>A list of deterministic matching rules that should be designed to admit very few (none if possible) false positives</p> required <code>recall</code> <code>float</code> <p>A guess at the recall the deterministic matching rules will attain.  i.e. what proportion of true matches will be recovered by these deterministic rules</p> required","tags":["API","Model Training","M Probability","U Probability","Prior (Lambda)"]},{"location":"linkerexp.html","title":"Documentation for <code>Linker</code> object methods related to exploratory analysis","text":"<p>The Linker object manages the data linkage process and holds the data linkage model.</p> <p>Most of Splink's functionality can  be accessed by calling methods (functions) on the linker, such as <code>linker.predict()</code>, <code>linker.profile_columns()</code> etc.</p> <p>The Linker class is intended for subclassing for specific backends, e.g. a <code>DuckDBLinker</code>.</p>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.count_num_comparisons_from_blocking_rule","title":"<code>count_num_comparisons_from_blocking_rule(blocking_rule)</code>","text":"<p>Compute the number of pairwise record comparisons that would be generated by a blocking rule</p> <p>Parameters:</p> Name Type Description Default <code>blocking_rule</code> <code>str</code> <p>The blocking rule to analyse</p> required <code>link_type</code> <code>str</code> <p>The link type.  This is needed only if the linker has not yet been provided with a settings dictionary.  Defaults to None.</p> required <code>unique_id_column_name</code> <code>str</code> <p>This is needed only if the linker has not yet been provided with a settings dictionary.  Defaults to None.</p> required <p>Examples:</p> <pre><code>br = \"l.first_name = r.first_name\"\nlinker.count_num_comparisons_from_blocking_rule(br)\n</code></pre> <p>19387 <pre><code>br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\"\nlinker.count_num_comparisons_from_blocking_rule(br)\n</code></pre> 394</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of comparisons generated by the blocking rule</p>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.cumulative_comparisons_from_blocking_rules_records","title":"<code>cumulative_comparisons_from_blocking_rules_records(blocking_rules=None)</code>","text":"<p>Output the number of comparisons generated by each successive blocking rule.</p> <p>This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total.</p> <p>Parameters:</p> Name Type Description Default <code>blocking_rules</code> <code>str or list</code> <p>The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used.</p> <code>None</code> <p>Examples:</p> <pre><code>linker_settings = DuckDBLinker(df, settings)\n# Compute the cumulative number of comparisons generated by the rules\n# in your settings object.\nlinker_settings.cumulative_comparisons_from_blocking_rules_records()\n&gt;&gt;&gt;\n# Generate total comparisons with custom blocking rules.\nblocking_rules = [\n   \"l.surname = r.surname\",\n   \"l.first_name = r.first_name\n    and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n]\n&gt;&gt;&gt;\nlinker_settings.cumulative_comparisons_from_blocking_rules_records(\n    blocking_rules\n )\n</code></pre> <p>Returns:</p> Name Type Description <code>List</code> <p>A list of blocking rules and the corresponding number of comparisons it is forecast to generate.</p>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.cumulative_num_comparisons_from_blocking_rules_chart","title":"<code>cumulative_num_comparisons_from_blocking_rules_chart(blocking_rules=None)</code>","text":"<p>Display a chart with the cumulative number of comparisons generated by a selection of blocking rules.</p> <p>This is equivalent to the output size of df_predict and details how many comparisons each of your individual blocking rules will contribute to the total.</p> <p>Parameters:</p> Name Type Description Default <code>blocking_rules</code> <code>str or list</code> <p>The blocking rule(s) to compute comparisons for. If null, the rules set out in your settings object will be used.</p> <code>None</code> <p>Examples:</p> <pre><code>linker_settings = DuckDBLinker(df, settings)\n# Compute the cumulative number of comparisons generated by the rules\n# in your settings object.\nlinker_settings.cumulative_num_comparisons_from_blocking_rules_chart()\n&gt;&gt;&gt;\n# Generate total comparisons with custom blocking rules.\nblocking_rules = [\n   \"l.surname = r.surname\",\n   \"l.first_name = r.first_name\n    and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n]\n&gt;&gt;&gt;\nlinker_settings.cumulative_num_comparisons_from_blocking_rules_chart(\n    blocking_rules\n )\n</code></pre> <p>Returns:</p> Name Type Description <code>VegaLite</code> <p>A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the <code>spec</code> attribute.</p>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.missingness_chart","title":"<code>missingness_chart(input_dataset=None)</code>","text":"<p>Generate a summary chart of the missingness (prevalence of nulls) of columns in the input datasets.  By default, missingness is assessed across all input datasets</p> <p>Parameters:</p> Name Type Description Default <code>input_dataset</code> <code>str</code> <p>Name of one of the input tables in the</p> <code>None</code> <p>Examples:</p> <p><pre><code>linker.missingness_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.missingness_chart()\nsave_offline_chart(c.spec, \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500\n</code></pre></p>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.profile_columns","title":"<code>profile_columns(column_expressions, top_n=10, bottom_n=10)</code>","text":"","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerexp.html#splink.linker.Linker.unlinkables_chart","title":"<code>unlinkables_chart(x_col='match_weight', source_dataset=None, as_dict=False)</code>","text":"<p>Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters.</p> <p>Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match.</p> <p>Parameters:</p> Name Type Description Default <code>x_col</code> <code>str</code> <p>Column to use for the x-axis. Defaults to \"match_weight\".</p> <code>'match_weight'</code> <code>source_dataset</code> <code>str</code> <p>Name of the source dataset to use for the title of the output chart.</p> <code>None</code> <code>as_dict</code> <code>bool</code> <p>If True, return a dict version of the chart.</p> <code>False</code> <p>Examples:</p> <p>For the simplest code pipeline, load a pre-trained model and run this against the test data. <pre><code>df = pd.read_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\")\nlinker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.unlinkables_chart()\n</code></pre> For more complex code pipelines, you can run an entire pipeline that estimates your m and u values, before `unlinkables_chart().</p> <p>Returns:</p> Name Type Description <code>VegaLite</code> <p>A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the <code>spec</code> attribute.</p>","tags":["API","Exploratory Data Analysis","Profiling","Blocking Rules","Missingness"]},{"location":"linkerpred.html","title":"Documentation for <code>Linker</code> object methods related to link prediction","text":"<p>The Linker object manages the data linkage process and holds the data linkage model.</p> <p>Most of Splink's functionality can  be accessed by calling methods (functions) on the linker, such as <code>linker.predict()</code>, <code>linker.profile_columns()</code> etc.</p> <p>The Linker class is intended for subclassing for specific backends, e.g. a <code>DuckDBLinker</code>.</p> Source code in <code>splink/linker.py</code> <pre><code>class Linker:\n\"\"\"The Linker object manages the data linkage process and holds the data linkage\n    model.\n\n    Most of Splink's functionality can  be accessed by calling methods (functions)\n    on the linker, such as `linker.predict()`, `linker.profile_columns()` etc.\n\n    The Linker class is intended for subclassing for specific backends, e.g.\n    a `DuckDBLinker`.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_table_or_tables: str | list,\n        settings_dict: dict | Path,\n        accepted_df_dtypes,\n        set_up_basic_logging: bool = True,\n        input_table_aliases: str | list = None,\n    ):\n\"\"\"Initialise the linker object, which manages the data linkage process and\n        holds the data linkage model.\n\n        Examples:\n            === \"DuckDB\"\n                Dedupe\n                ```py\n                df = pd.read_csv(\"data_to_dedupe.csv\")\n                linker = DuckDBLinker(df, settings_dict)\n                ```\n                Link\n                ```py\n                df_1 = pd.read_parquet(\"table_1/\")\n                df_2 = pd.read_parquet(\"table_2/\")\n                linker = DuckDBLinker(\n                    [df_1, df_2],\n                    settings_dict,\n                    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                    )\n                ```\n                Dedupe with a pre-trained model read from a json file\n                ```py\n                df = pd.read_csv(\"data_to_dedupe.csv\")\n                linker = DuckDBLinker(df, \"model.json\")\n                ```\n            === \"Spark\"\n                Dedupe\n                ```py\n                df = spark.read.csv(\"data_to_dedupe.csv\")\n                linker = SparkLinker(df, settings_dict)\n                ```\n                Link\n                ```py\n                df_1 = spark.read.parquet(\"table_1/\")\n                df_2 = spark.read.parquet(\"table_2/\")\n                linker = SparkLinker(\n                    [df_1, df_2],\n                    settings_dict,\n                    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                    )\n                ```\n                Dedupe with a pre-trained model read from a json file\n                ```py\n                df = spark.read.csv(\"data_to_dedupe.csv\")\n                linker = SparkLinker(df, \"model.json\")\n                ```\n\n        Args:\n            input_table_or_tables (Union[str, list]): Input data into the linkage model.\n                Either a single string (the name of a table in a database) for\n                deduplication jobs, or a list of strings  (the name of tables in a\n                database) for link_only or link_and_dedupe.  For some linkers, such as\n                the DuckDBLinker and the SparkLinker, it's also possible to pass in\n                dataframes (Pandas and Spark respectively) rather than strings.\n            settings_dict (dict | Path, optional): A Splink settings dictionary, or a\n                path to a json defining a settingss dictionary or pre-trained model.\n                If not provided when the object is created, can later be added using\n                `linker.load_settings()` or `linker.load_model()` Defaults to None.\n            set_up_basic_logging (bool, optional): If true, sets ups up basic logging\n                so that Splink sends messages at INFO level to stdout. Defaults to True.\n            input_table_aliases (Union[str, list], optional): Labels assigned to\n                input tables in Splink outputs.  If the names of the tables in the\n                input database are long or unspecific, this argument can be used\n                to attach more easily readable/interpretable names. Defaults to None.\n        \"\"\"\n\n        if set_up_basic_logging:\n            logging.basicConfig(\n                format=\"%(message)s\",\n            )\n            splink_logger = logging.getLogger(\"splink\")\n            splink_logger.setLevel(logging.INFO)\n\n        self._pipeline = SQLPipeline()\n\n        self._names_of_tables_created_by_splink: set = set()\n        self._intermediate_table_cache: dict = CacheDictWithLogging()\n\n        if not isinstance(settings_dict, (dict, type(None))):\n            # Run if you've entered a filepath\n            # feed it a blank settings dictionary\n            self._setup_settings_objs(None)\n            self.load_settings(settings_dict)\n        else:\n            settings_dict = deepcopy(settings_dict)\n            self._setup_settings_objs(settings_dict)\n\n        homogenised_tables, homogenised_aliases = self._register_input_tables(\n            input_table_or_tables,\n            input_table_aliases,\n            accepted_df_dtypes,\n        )\n\n        self._input_tables_dict = self._get_input_tables_dict(\n            homogenised_tables, homogenised_aliases\n        )\n\n        self._validate_input_dfs()\n        self._em_training_sessions = []\n\n        self._find_new_matches_mode = False\n        self._train_u_using_random_sample_mode = False\n        self._compare_two_records_mode = False\n        self._self_link_mode = False\n        self._analyse_blocking_mode = False\n        self._deterministic_link_mode = False\n\n        self.debug_mode = False\n\n    @property\n    def _cache_uid(self):\n        if self._settings_dict:\n            return self._settings_obj._cache_uid\n        else:\n            return self._cache_uid_no_settings\n\n    @_cache_uid.setter\n    def _cache_uid(self, value):\n        if self._settings_dict:\n            self._settings_obj._cache_uid = value\n        else:\n            self._cache_uid_no_settings = value\n\n    @property\n    def _settings_obj(self) -&gt; Settings:\n        if self._settings_obj_ is None:\n            raise ValueError(\n                \"You did not provide a settings dictionary when you \"\n                \"created the linker.  To continue, you need to provide a settings \"\n                \"dictionary using the `load_settings()` method on your linker \"\n                \"object. i.e. linker.load_settings(settings_dict)\"\n            )\n        return self._settings_obj_\n\n    @property\n    def _input_tablename_l(self):\n        if self._find_new_matches_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._self_link_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._compare_two_records_mode:\n            return \"__splink__compare_two_records_left_with_tf\"\n\n        if self._train_u_using_random_sample_mode:\n            return \"__splink__df_concat_with_tf_sample\"\n\n        if self._analyse_blocking_mode:\n            return \"__splink__df_concat\"\n\n        if self._two_dataset_link_only:\n            return \"__splink__df_concat_with_tf_left\"\n\n        return \"__splink__df_concat_with_tf\"\n\n    @property\n    def _input_tablename_r(self):\n        if self._find_new_matches_mode:\n            return \"__splink__df_new_records_with_tf\"\n\n        if self._self_link_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._compare_two_records_mode:\n            return \"__splink__compare_two_records_right_with_tf\"\n\n        if self._train_u_using_random_sample_mode:\n            return \"__splink__df_concat_with_tf_sample\"\n\n        if self._analyse_blocking_mode:\n            return \"__splink__df_concat\"\n\n        if self._two_dataset_link_only:\n            return \"__splink_df_concat_with_tf_right\"\n        return \"__splink__df_concat_with_tf\"\n\n    @property\n    def _source_dataset_column_name(self):\n        if self._settings_obj_ is None:\n            return None\n\n        # Used throughout the scripts to feed our SQL\n        if self._settings_obj._source_dataset_column_name_is_required:\n            df_obj = next(iter(self._input_tables_dict.values()))\n            columns = df_obj.columns_escaped\n\n            input_column, src_ds_col = self._settings_obj_._source_dataset_col\n            return \"__splink_source_dataset\" if src_ds_col in columns else input_column\n        else:\n            return None\n\n    @property\n    def _two_dataset_link_only(self):\n        # Two dataset link only join is a special case where an inner join of the\n        # two datasets is much more efficient than self-joining the vertically\n        # concatenation of all input datasets\n        if self._find_new_matches_mode:\n            return True\n\n        if self._compare_two_records_mode:\n            return True\n\n        # in u-train sample mode we are joining the concatenated table mixing\n        # both data sets - hence if we inner join on True we will end up with\n        # samples which both originate from the same dataset\n        if self._train_u_using_random_sample_mode:\n            return False\n\n        if self._analyse_blocking_mode:\n            return False\n\n        if (\n            len(self._input_tables_dict) == 2\n            and self._settings_obj._link_type == \"link_only\"\n        ):\n            return True\n        else:\n            return False\n\n    @property\n    def _sql_dialect(self):\n        if self._sql_dialect_ is None:\n            raise NotImplementedError(\n                f\"No SQL dialect set on object of type {type(self)}. \"\n                \"Did you make sure to create a dialect-specific Linker?\"\n            )\n        return self._sql_dialect_\n\n    @property\n    def _infinity_expression(self):\n        raise NotImplementedError(\n            f\"infinity sql expression not available for {type(self)}\"\n        )\n\n    @property\n    def _verify_link_only_job(self):\n        cache = self._intermediate_table_cache\n        if \"__splink__df_concat_with_tf\" not in cache:\n            return\n\n        if self._settings_obj._link_type == \"link_only\":\n            # if input datasets &gt; 1 then skip\n            if len(self._input_tables_dict) &gt; 1:\n                return\n\n            # else, check if source dataset column is populated...\n            src_ds = self._source_dataset_column_name\n            if src_ds == \"__splink_source_dataset\":\n                _, src_ds = self._settings_obj_._source_dataset_col\n\n            sql = find_unique_source_dataset(src_ds)\n            self._enqueue_sql(sql, \"source_ds_distinct\")\n            src_ds_distinct = self._execute_sql_pipeline(\n                [cache[\"__splink__df_concat_with_tf\"]]\n            )\n            if len(src_ds_distinct.as_record_dict()) == 1:\n                raise SplinkException(\n                    \"if `link_type` is `link_only`, it should have at least two \"\n                    \"input dataframes, or one dataframe with a `source_dataset` \"\n                    \"column outlining which dataset each record belongs to.\"\n                )\n\n    def _register_input_tables(self, input_tables, input_aliases, accepted_df_dtypes):\n        # 'homogenised' means all entries are strings representing tables\n        homogenised_tables = []\n        homogenised_aliases = []\n        accepted_df_dtypes = ensure_is_tuple(accepted_df_dtypes)\n\n        existing_tables = []\n        for alias in input_aliases:\n            # Check if alias is a string (indicating a table name) and that it is not\n            # a file path.\n            if not isinstance(alias, str) or re.match(pattern=r\".*\", string=alias):\n                continue\n            exists = self._table_exists_in_database(alias)\n            if exists:\n                existing_tables.append(f\"'{alias}'\")\n        if existing_tables:\n            input_tables = \", \".join(existing_tables)\n            raise ValueError(\n                f\"Table(s): {input_tables} already exists in database. \"\n                \"Please remove or rename it/them before retrying\"\n            )\n\n        for i, (table, alias) in enumerate(zip(input_tables, input_aliases)):\n            if isinstance(alias, accepted_df_dtypes):\n                alias = f\"__splink__input_table_{i}\"\n\n            if isinstance(table, accepted_df_dtypes):\n                self._table_registration(table, alias)\n                table = alias\n\n            homogenised_tables.append(table)\n            homogenised_aliases.append(alias)\n\n        return homogenised_tables, homogenised_aliases\n\n    def _setup_settings_objs(self, settings_dict):\n        # Setup the linker class's required settings\n        self._settings_dict = settings_dict\n\n        # if settings_dict is passed, set sql_dialect on it if missing, and make sure\n        # incompatible dialect not passed\n        if settings_dict is not None and settings_dict.get(\"sql_dialect\", None) is None:\n            settings_dict[\"sql_dialect\"] = self._sql_dialect\n\n        if settings_dict is None:\n            self._cache_uid_no_settings = ascii_uid(8)\n        else:\n            uid = settings_dict.get(\"linker_uid\", ascii_uid(8))\n            settings_dict[\"linker_uid\"] = uid\n\n        if settings_dict is None:\n            self._settings_obj_ = None\n        else:\n            self._settings_obj_ = Settings(settings_dict)\n\n            self._validate_dialect()\n\n    def _initialise_df_concat(self, materialise=False):\n        cache = self._intermediate_table_cache\n        concat_df = None\n        if \"__splink__df_concat\" in cache:\n            concat_df = cache[\"__splink__df_concat\"]\n        elif \"__splink__df_concat_with_tf\" in cache:\n            concat_df = cache[\"__splink__df_concat_with_tf\"]\n            concat_df.templated_name = \"__splink__df_concat\"\n        else:\n            if materialise:\n                # Clear the pipeline if we are materialising\n                # There's no reason not to do this, since when\n                # we execute the pipeline, it'll get cleared anyway\n                self._pipeline.reset()\n            sql = vertically_concatenate_sql(self)\n            self._enqueue_sql(sql, \"__splink__df_concat\")\n            if materialise:\n                concat_df = self._execute_sql_pipeline()\n                cache[\"__splink__df_concat\"] = concat_df\n\n        return concat_df\n\n    def _initialise_df_concat_with_tf(self, materialise=True):\n        cache = self._intermediate_table_cache\n        nodes_with_tf = None\n        if \"__splink__df_concat_with_tf\" in cache:\n            nodes_with_tf = cache[\"__splink__df_concat_with_tf\"]\n\n        else:\n            if materialise:\n                # Clear the pipeline if we are materialising\n                # There's no reason not to do this, since when\n                # we execute the pipeline, it'll get cleared anyway\n                self._pipeline.reset()\n\n            sql = vertically_concatenate_sql(self)\n            self._enqueue_sql(sql, \"__splink__df_concat\")\n\n            sqls = compute_all_term_frequencies_sqls(self)\n            for sql in sqls:\n                self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n            if materialise:\n                nodes_with_tf = self._execute_sql_pipeline()\n                cache[\"__splink__df_concat_with_tf\"] = nodes_with_tf\n\n        # verify the link job\n        if self._settings_obj_ is not None:\n            self._verify_link_only_job\n\n        return nodes_with_tf\n\n    def _table_to_splink_dataframe(\n        self, templated_name, physical_name\n    ) -&gt; SplinkDataFrame:\n\"\"\"Create a SplinkDataframe from a table in the underlying database called\n        `physical_name`.\n\n        Associate a `templated_name` with this table, which signifies the purpose\n        or 'meaning' of this table to splink. (e.g. `__splink__df_blocked`)\n\n        Args:\n            templated_name (str): The purpose of the table to Splink\n            physical_name (str): The name of the table in the underlying databse\n        \"\"\"\n        raise NotImplementedError(\n            \"_table_to_splink_dataframe not implemented on this linker\"\n        )\n\n    def _enqueue_sql(self, sql, output_table_name):\n\"\"\"Add sql to the current pipeline, but do not execute the pipeline.\"\"\"\n        self._pipeline.enqueue_sql(sql, output_table_name)\n\n    def _execute_sql_pipeline(\n        self,\n        input_dataframes: list[SplinkDataFrame] = [],\n        materialise_as_hash=True,\n        use_cache=True,\n    ) -&gt; SplinkDataFrame:\n\"\"\"Execute the SQL queued in the current pipeline as a single statement\n        e.g. `with a as (), b as , c as (), select ... from c`, then execute the\n        pipeline, returning the resultant table as a SplinkDataFrame\n\n        Args:\n            input_dataframes (List[SplinkDataFrame], optional): A 'starting point' of\n                SplinkDataFrames if needed. Defaults to [].\n            materialise_as_hash (bool, optional): If true, the output tablename will end\n                in a unique identifer. Defaults to True.\n            use_cache (bool, optional): If true, look at whether the SQL pipeline has\n                been executed before, and if so, use the existing result. Defaults to\n                True.\n\n        Returns:\n            SplinkDataFrame: An abstraction representing the table created by the sql\n                pipeline\n        \"\"\"\n\n        if not self.debug_mode:\n            sql_gen = self._pipeline._generate_pipeline(input_dataframes)\n\n            output_tablename_templated = self._pipeline.queue[-1].output_table_name\n\n            try:\n                dataframe = self._sql_to_splink_dataframe_checking_cache(\n                    sql_gen,\n                    output_tablename_templated,\n                    materialise_as_hash,\n                    use_cache,\n                )\n            except Exception as e:\n                raise e\n            finally:\n                self._pipeline.reset()\n\n            return dataframe\n        else:\n            # In debug mode, we do not pipeline the sql and print the\n            # results of each part of the pipeline\n            for task in self._pipeline._generate_pipeline_parts(input_dataframes):\n                output_tablename = task.output_table_name\n                sql = task.sql\n                print(\"------\")\n                print(f\"--------Creating table: {output_tablename}--------\")\n\n                dataframe = self._sql_to_splink_dataframe_checking_cache(\n                    sql,\n                    output_tablename,\n                    materialise_as_hash=False,\n                    use_cache=False,\n                )\n            self._pipeline.reset()\n            return dataframe\n\n    def _execute_sql_against_backend(\n        self, sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n\"\"\"Execute a single sql SELECT statement, returning a SplinkDataFrame.\n\n        Subclasses should implement this, using _log_and_run_sql_execution() within\n        their implementation, maybe doing some SQL translation or other prep/cleanup\n        work before/after.\n        \"\"\"\n        raise NotImplementedError(\n            f\"_execute_sql_against_backend not implemented for {type(self)}\"\n        )\n\n    def _run_sql_execution(\n        self, final_sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n\"\"\"**Actually** execute the sql against the backend database.\n\n        This is intended to be implemented by a subclass, but not actually called\n        directly. Instead, call _log_and_run_sql_execution, and that will call\n        this method.\n\n        This could return something, or not. It's up to the Linker subclass to decide.\n        \"\"\"\n        raise NotImplementedError(\n            f\"_run_sql_execution not implemented for {type(self)}\"\n        )\n\n    def _log_and_run_sql_execution(\n        self, final_sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n\"\"\"Log the sql, then call _run_sql_execution(), wrapping any errors\"\"\"\n        logger.debug(execute_sql_logging_message_info(templated_name, physical_name))\n        logger.log(5, log_sql(final_sql))\n        try:\n            return self._run_sql_execution(final_sql, templated_name, physical_name)\n        except Exception as e:\n            # Parse our SQL through sqlglot to pretty print\n            try:\n                final_sql = sqlglot.parse_one(\n                    final_sql,\n                    read=self._sql_dialect,\n                ).sql(pretty=True)\n                # if sqlglot produces any errors, just report the raw SQL\n            except Exception:\n                pass\n\n            raise SplinkException(\n                f\"Error executing the following sql for table \"\n                f\"`{templated_name}` ({physical_name}):\\n{final_sql}\"\n            ) from e\n\n    def register_table(self, input, table_name, overwrite=False):\n\"\"\"\n        Register a table to your backend database, to be used in one of the\n        splink methods, or simply to allow querying.\n\n        Tables can be of type: dictionary, record level dictionary,\n        pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n        Examples:\n            ```py\n            test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]}\n            linker.register_table(test_dict, \"test_dict\")\n            linker.query_sql(\"select * from test_dict\")\n            ```\n\n        Args:\n            input: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            table_name (str): The name you wish to assign to the table.\n            overwrite (bool): Overwrite the table in the underlying database if it\n                exists\n\n        Returns:\n            SplinkDataFrame: An abstraction representing the table created by the sql\n                pipeline\n        \"\"\"\n\n        raise NotImplementedError(f\"register_table not implemented for {type(self)}\")\n\n    def _table_registration(self, input, table_name):\n\"\"\"\n        Register a table to your backend database, to be used in one of the\n        splink methods, or simply to allow querying.\n\n        Tables can be of type: dictionary, record level dictionary,\n        pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n        This function is contains no overwrite functionality, so it can be used\n        where we don't want to allow for overwriting.\n\n        Args:\n            input: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            table_name (str): The name you wish to assign to the table.\n\n        Returns:\n            None\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"_table_registration not implemented for {type(self)}\"\n        )\n\n    def query_sql(self, sql, output_type=\"pandas\"):\n\"\"\"\n        Run a SQL query against your backend database and return\n        the resulting output.\n\n        Examples:\n            === \"DuckDB\"\n                ```py\n                linker = DuckDBLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \"Spark\"\n                ```py\n                linker = SparkLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \"Athena\"\n                ```py\n                linker = AthenaLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \"SQLite\"\n                ```py\n                linker = SQLiteLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n\n        Args:\n            sql (str): The SQL to be queried.\n            output_type (str): One of splink_df/splinkdf or pandas.\n                This determines the type of table that your results are output in.\n        \"\"\"\n\n        output_tablename_templated = \"__splink__df_sql_query\"\n\n        splink_dataframe = self._sql_to_splink_dataframe_checking_cache(\n            sql,\n            output_tablename_templated,\n            materialise_as_hash=False,\n            use_cache=False,\n        )\n\n        if output_type in (\"splink_df\", \"splinkdf\"):\n            return splink_dataframe\n        elif output_type == \"pandas\":\n            out = splink_dataframe.as_pandas_dataframe()\n            # If pandas, drop the table to cleanup the db\n            splink_dataframe.drop_table_from_database()\n            return out\n        else:\n            raise ValueError(\n                f\"output_type '{output_type}' is not supported.\",\n                \"Must be one of 'splink_df'/'splinkdf' or 'pandas'\",\n            )\n\n    def _sql_to_splink_dataframe_checking_cache(\n        self,\n        sql,\n        output_tablename_templated,\n        materialise_as_hash=True,\n        use_cache=True,\n    ) -&gt; SplinkDataFrame:\n\"\"\"Execute sql, or if identical sql has been run before, return cached results.\n\n        This function\n            - is used by _execute_sql_pipeline to to execute SQL\n            - or can be used directly if you have a single SQL statement that's\n              not in a pipeline\n\n        Return a SplinkDataFrame representing the results of the SQL\n        \"\"\"\n\n        to_hash = (sql + self._cache_uid).encode(\"utf-8\")\n        hash = hashlib.sha256(to_hash).hexdigest()[:9]\n        # Ensure hash is valid sql table name\n        table_name_hash = f\"{output_tablename_templated}_{hash}\"\n\n        if use_cache:\n            if self._table_exists_in_database(output_tablename_templated):\n                logger.debug(f\"Using existing table {output_tablename_templated}\")\n                return self._table_to_splink_dataframe(\n                    output_tablename_templated, output_tablename_templated\n                )\n\n            if self._table_exists_in_database(table_name_hash):\n                logger.debug(\n                    f\"Using cache for {output_tablename_templated}\"\n                    f\" with physical name {table_name_hash}\"\n                )\n                return self._table_to_splink_dataframe(\n                    output_tablename_templated, table_name_hash\n                )\n\n        if self.debug_mode:\n            print(sql)\n\n        if materialise_as_hash:\n            splink_dataframe = self._execute_sql_against_backend(\n                sql, output_tablename_templated, table_name_hash\n            )\n        else:\n            splink_dataframe = self._execute_sql_against_backend(\n                sql,\n                output_tablename_templated,\n                output_tablename_templated,\n            )\n\n        self._names_of_tables_created_by_splink.add(splink_dataframe.physical_name)\n\n        if self.debug_mode:\n            df_pd = splink_dataframe.as_pandas_dataframe()\n            try:\n                from IPython.display import display\n\n                display(df_pd)\n            except ModuleNotFoundError:\n                print(df_pd)\n\n        return splink_dataframe\n\n    def __deepcopy__(self, memo):\n\"\"\"When we do EM training, we need a copy of the linker which is independent\n        of the main linker e.g. setting parameters on the copy will not affect the\n        main linker.  This method implements ensures linker can be deepcopied.\n        \"\"\"\n        new_linker = copy(self)\n        new_linker._em_training_sessions = []\n        new_settings = deepcopy(self._settings_obj_)\n        new_linker._settings_obj_ = new_settings\n        return new_linker\n\n    def _ensure_aliases_populated_and_is_list(\n        self, input_table_or_tables, input_table_aliases\n    ):\n        if input_table_aliases is None:\n            input_table_aliases = input_table_or_tables\n\n        input_table_aliases = ensure_is_list(input_table_aliases)\n\n        return input_table_aliases\n\n    def _get_input_tables_dict(self, input_table_or_tables, input_table_aliases):\n        input_table_or_tables = ensure_is_list(input_table_or_tables)\n\n        input_table_aliases = self._ensure_aliases_populated_and_is_list(\n            input_table_or_tables, input_table_aliases\n        )\n\n        d = {}\n        for table_name, table_alias in zip(input_table_or_tables, input_table_aliases):\n            d[table_alias] = self._table_to_splink_dataframe(table_alias, table_name)\n        return d\n\n    def _get_input_tf_dict(self, df_dict):\n        d = {}\n        for df_name, df_value in df_dict.items():\n            renamed = colname_to_tf_tablename(df_name)\n            d[renamed] = self._table_to_splink_dataframe(renamed, df_value)\n        return d\n\n    def _predict_warning(self):\n        if not self._settings_obj._is_fully_trained:\n            msg = (\n                \"\\n -- WARNING --\\n\"\n                \"You have called predict(), but there are some parameter \"\n                \"estimates which have neither been estimated or specified in your \"\n                \"settings dictionary.  To produce predictions the following\"\n                \" untrained trained parameters will use default values.\"\n            )\n            messages = self._settings_obj._not_trained_messages()\n\n            warn_message = \"\\n\".join([msg] + messages)\n\n            logger.warning(warn_message)\n\n    def _table_exists_in_database(self, table_name):\n        raise NotImplementedError(\n            f\"table_exists_in_database not implemented for {type(self)}\"\n        )\n\n    def _validate_input_dfs(self):\n        if not hasattr(self, \"_input_tables_dict\"):\n            # This is only triggered where a user loads a settings dict from a\n            # given file path.\n            return\n\n        for df in self._input_tables_dict.values():\n            df.validate()\n\n        if self._settings_obj_ is not None:\n            if self._settings_obj._link_type == \"dedupe_only\":\n                if len(self._input_tables_dict) &gt; 1:\n                    raise ValueError(\n                        'If link_type = \"dedupe only\" then input tables must contain '\n                        \"only a single input table\",\n                    )\n\n    def _validate_dialect(self):\n        settings_dialect = self._settings_obj._sql_dialect\n        if settings_dialect != self._sql_dialect:\n            raise ValueError(\n                f\"Incompatible SQL dialect! `settings` dictionary uses \"\n                f\"dialect {settings_dialect}, but expecting \"\n                f\"'{self._sql_dialect}' for Linker of type {type(self)}\"\n            )\n\n    def _populate_probability_two_random_records_match_from_trained_values(self):\n        recip_prop_matches_estimates = []\n\n        logger.log(\n            15,\n            (\n                \"---- Using training sessions to compute \"\n                \"probability two random records match ----\"\n            ),\n        )\n        for em_training_session in self._em_training_sessions:\n            training_lambda = (\n                em_training_session._settings_obj._probability_two_random_records_match\n            )\n            training_lambda_bf = prob_to_bayes_factor(training_lambda)\n            reverse_levels = (\n                em_training_session._comparison_levels_to_reverse_blocking_rule\n            )\n\n            logger.log(\n                15,\n                \"\\n\"\n                f\"Probability two random records match from trained model blocking on \"\n                f\"{em_training_session._blocking_rule_for_training.blocking_rule}: \"\n                f\"{training_lambda:,.3f}\",\n            )\n\n            for reverse_level in reverse_levels:\n                # Get comparison level on current settings obj\n                cc = self._settings_obj._get_comparison_by_output_column_name(\n                    reverse_level.comparison._output_column_name\n                )\n\n                cl = cc._get_comparison_level_by_comparison_vector_value(\n                    reverse_level._comparison_vector_value\n                )\n\n                if cl._has_estimated_values:\n                    bf = cl._trained_m_median / cl._trained_u_median\n                else:\n                    bf = cl._bayes_factor\n\n                logger.log(\n                    15,\n                    f\"Reversing comparison level {cc._output_column_name}\"\n                    f\" using bayes factor {bf:,.3f}\",\n                )\n\n                training_lambda_bf = training_lambda_bf / bf\n\n                as_prob = bayes_factor_to_prob(training_lambda_bf)\n\n                logger.log(\n                    15,\n                    (\n                        \"This estimate of probability two random records match now: \"\n                        f\" {as_prob:,.3f} \"\n                        f\"with reciprocal {(1/as_prob):,.3f}\"\n                    ),\n                )\n            logger.log(15, \"\\n---------\")\n            p = bayes_factor_to_prob(training_lambda_bf)\n            recip_prop_matches_estimates.append(1 / p)\n\n        prop_matches_estimate = 1 / median(recip_prop_matches_estimates)\n\n        self._settings_obj._probability_two_random_records_match = prop_matches_estimate\n        logger.log(\n            15,\n            \"\\nMedian of prop of matches estimates: \"\n            f\"{self._settings_obj._probability_two_random_records_match:,.3f} \"\n            \"reciprocal \"\n            f\"{1/self._settings_obj._probability_two_random_records_match:,.3f}\",\n        )\n\n    def _populate_m_u_from_trained_values(self):\n        ccs = self._settings_obj.comparisons\n\n        for cc in ccs:\n            for cl in cc._comparison_levels_excluding_null:\n                if cl._has_estimated_u_values:\n                    cl.u_probability = cl._trained_u_median\n                if cl._has_estimated_m_values:\n                    cl.m_probability = cl._trained_m_median\n\n    def _delete_tables_created_by_splink_from_db(\n        self, retain_term_frequency=True, retain_df_concat_with_tf=True\n    ):\n        to_remove = set()\n        for name in self._names_of_tables_created_by_splink:\n            # Only delete tables explicitly marked as having been created by splink\n            if \"__splink__\" not in name:\n                continue\n            if name == \"__splink__df_concat_with_tf\":\n                if not retain_df_concat_with_tf:\n                    self._delete_table_from_database(name)\n                    to_remove.add(name)\n            elif name.startswith(\"__splink__df_tf_\"):\n                if not retain_term_frequency:\n                    self._delete_table_from_database(name)\n                    to_remove.add(name)\n            else:\n                self._delete_table_from_database(name)\n                to_remove.add(name)\n\n        self._names_of_tables_created_by_splink = (\n            self._names_of_tables_created_by_splink - to_remove\n        )\n\n    def _raise_error_if_necessary_waterfall_columns_not_computed(self):\n        ricc = self._settings_obj._retain_intermediate_calculation_columns\n        rmc = self._settings_obj._retain_matching_columns\n        if not (ricc and rmc):\n            raise ValueError(\n                \"retain_intermediate_calculation_columns and \"\n                \"retain_matching_columns must both be set to True in your settings\"\n                \" dictionary to use this function, because otherwise the necessary \"\n                \"columns will not be available in the input records.\"\n                f\" Their current values are {ricc} and {rmc}, respectively. \"\n                \"Please re-run your linkage with them both set to True.\"\n            )\n\n    def _raise_error_if_necessary_accuracy_columns_not_computed(self):\n        rmc = self._settings_obj._retain_matching_columns\n        if not (rmc):\n            raise ValueError(\n                \"retain_matching_columns must be set to True in your settings\"\n                \" dictionary to use this function, because otherwise the necessary \"\n                \"columns will not be available in the input records.\"\n                f\" Its current value is {rmc}. \"\n                \"Please re-run your linkage with it set to True.\"\n            )\n\n    def load_settings(self, settings_dict: dict | str | Path):\n\"\"\"Initialise settings for the linker.  To be used if settings were\n        not passed to the linker on creation. This can either be in the form\n        of a settings dictionary or a filepath to a json file containing a\n        valid settings dictionary.\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.load_settings(settings_dict)\n            ```\n\n        Args:\n            settings_dict (dict | str | Path): A Splink settings dictionary or\n                the path to your settings json file.\n        \"\"\"\n\n        if not isinstance(settings_dict, dict):\n            p = Path(settings_dict)\n            if not p.is_file():  # check if it's a valid file/filepath\n                raise FileNotFoundError(\n                    \"The filepath you have provided is either not a valid file \"\n                    \"or doesn't exist along the path provided.\"\n                )\n            settings_dict = json.loads(p.read_text())\n\n        # Store the cache ID so it can be reloaded after cache invalidation\n        cache_id = self._cache_uid\n        # So we don't run into any issues with generated tables having\n        # invalid columns as settings have been tweaked, invalidate\n        # the cache and allow these tables to be recomputed.\n\n        # This is less efficient, but triggers infrequently and ensures we don't\n        # run into issues where the defaults used conflict with the actual values\n        # supplied in settings.\n\n        # This is particularly relevant with `source_dataset`, which appears within\n        # concat_with_tf.\n        self.invalidate_cache()\n\n        # If a uid already exists in your settings object, prioritise this\n        settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", cache_id)\n        settings_dict[\"sql_dialect\"] = settings_dict.get(\n            \"sql_dialect\", self._sql_dialect\n        )\n        self._settings_dict = settings_dict\n        self._settings_obj_ = Settings(settings_dict)\n        self._validate_input_dfs()\n        self._validate_dialect()\n\n    def load_model(self, model_path: Path):\n\"\"\"\n        Load a pre-defined model from a json file into the linker.\n        This is intended to be used with the output of\n        `save_model_to_json()`.\n\n        Examples:\n            ```py\n            linker.load_model(\"my_settings.json\")\n            ```\n\n        Args:\n            model_path (Path): A path to your model settings json file.\n        \"\"\"\n\n        return self.load_settings(model_path)\n\n    def initialise_settings(self, settings_dict: dict):\n\"\"\"*This method is now deprecated. Please use `load_settings`\n        when loading existing settings or `load_model` when loading\n         a pre-trained model.*\n\n        Initialise settings for the linker.  To be used if settings were\n        not passed to the linker on creation.\n        Examples:\n            === \"DuckDB\"\n                ```py\n                linker = DuckDBLinker(df\")\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \"Spark\"\n                ```py\n                linker = SparkLinker(df\")\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \"Athena\"\n                ```py\n                linker = AthenaLinker(df\")\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \"SQLite\"\n                ```py\n                linker = SQLiteLinker(df\")\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n        Args:\n            settings_dict (dict): A Splink settings dictionary\n        \"\"\"\n        # If a uid already exists in your settings object, prioritise this\n        settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", self._cache_uid)\n        settings_dict[\"sql_dialect\"] = settings_dict.get(\n            \"sql_dialect\", self._sql_dialect\n        )\n        self._settings_dict = settings_dict\n        self._settings_obj_ = Settings(settings_dict)\n        self._validate_input_dfs()\n        self._validate_dialect()\n\n        warnings.warn(\n            \"`initialise_settings` is deprecated. We advise you use \"\n            \"`linker.load_settings()` when loading in your settings or a previously \"\n            \"trained model.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    def load_settings_from_json(self, in_path: str | Path):\n\"\"\"*This method is now deprecated. Please use `load_settings`\n        when loading existing settings or `load_model` when loading\n         a pre-trained model.*\n\n        Load settings from a `.json` file.\n        This `.json` file would usually be the output of\n        `linker.save_model_to_json()`\n        Examples:\n            ```py\n            linker.load_settings_from_json(\"my_settings.json\")\n            ```\n        Args:\n            in_path (str): Path to settings json file\n        \"\"\"\n        self.load_settings(in_path)\n\n        warnings.warn(\n            \"`load_settings_from_json` is deprecated. We advise you use \"\n            \"`linker.load_settings()` when loading in your settings or a previously \"\n            \"trained model.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    def compute_tf_table(self, column_name: str) -&gt; SplinkDataFrame:\n\"\"\"Compute a term frequency table for a given column and persist to the database\n\n        This method is useful if you want to pre-compute term frequency tables e.g.\n        so that real time linkage executes faster, or so that you can estimate\n        various models without having to recompute term frequency tables each time\n\n        Examples:\n            === \"DuckDB\"\n                Real time linkage\n                ```py\n                linker = DuckDBLinker(df)\n                linker.load_settings(\"saved_settings.json\")\n                linker.compute_tf_table(\"surname\")\n                linker.compare_two_records(record_left, record_right)\n                ```\n                Pre-computed term frequency tables\n                ```py\n                linker = DuckDBLinker(df)\n                df_first_name_tf = linker.compute_tf_table(\"first_name\")\n                df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n                &gt;&gt;&gt;\n                # On subsequent data linking job, read this table rather than recompute\n                df_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\n                df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n                ```\n            === \"Spark\"\n                Real time linkage\n                ```py\n                linker = SparkLinker(df)\n                linker.load_settings(\"saved_settings.json\")\n                linker.compute_tf_table(\"surname\")\n                linker.compare_two_records(record_left, record_right)\n                ```\n                Pre-computed term frequency tables\n                ```py\n                linker = SparkLinker(df)\n                df_first_name_tf = linker.compute_tf_table(\"first_name\")\n                df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n                &gt;&gt;&gt;\n                # On subsequent data linking job, read this table rather than recompute\n                df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\n                df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n                ```\n\n        Args:\n            column_name (str): The column name in the input table\n\n        Returns:\n            SplinkDataFrame: The resultant table as a splink data frame\n        \"\"\"\n\n        input_col = InputColumn(column_name, settings_obj=self._settings_obj)\n        tf_tablename = colname_to_tf_tablename(input_col)\n        cache = self._intermediate_table_cache\n        concat_tf_tables = [\n            remove_quotes_from_identifiers(tf_col.input_name_as_tree).sql()\n            for tf_col in self._settings_obj._term_frequency_columns\n        ]\n\n        if tf_tablename in cache:\n            tf_df = cache[tf_tablename]\n        elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables:\n            self._pipeline.reset()\n            # If our df_concat_with_tf table already exists, use backwards inference to\n            # find a given tf table\n            colname = InputColumn(column_name)\n            sql = term_frequencies_from_concat_with_tf(colname)\n            self._enqueue_sql(sql, colname_to_tf_tablename(colname))\n            tf_df = self._execute_sql_pipeline(\n                [cache[\"__splink__df_concat_with_tf\"]], materialise_as_hash=True\n            )\n            self._intermediate_table_cache[tf_tablename] = tf_df\n        else:\n            # Clear the pipeline if we are materialising\n            self._pipeline.reset()\n            df_concat = self._initialise_df_concat()\n            input_dfs = []\n            if df_concat:\n                input_dfs.append(df_concat)\n            sql = term_frequencies_for_single_column_sql(input_col)\n            self._enqueue_sql(sql, tf_tablename)\n            tf_df = self._execute_sql_pipeline(input_dfs, materialise_as_hash=True)\n            self._intermediate_table_cache[tf_tablename] = tf_df\n\n        return tf_df\n\n    def deterministic_link(self) -&gt; SplinkDataFrame:\n\"\"\"Uses the blocking rules specified by\n        `blocking_rules_to_generate_predictions` in the settings dictionary to\n        generate pairwise record comparisons.\n\n        For deterministic linkage, this should be a list of blocking rules which\n        are strict enough to generate only true links.\n\n        Deterministic linkage, however, is likely to result in missed links\n        (false negatives).\n\n        Examples:\n            === \"DuckDB\"\n            ```py\n            from splink.duckdb.duckdb_linker import DuckDBLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = DuckDBLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n            === \"Spark\"\n            ```py\n            from splink.spark.spark_linker import SparkLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = SparkLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n            === \"Athena\"\n            ```py\n            from splink.athena.athena_linker import AthenaLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = AthenaLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n            === \"SQLite\"\n            ```py\n            from splink.sqlite.sqlite_linker import SQLiteLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = SQLiteLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n                represents a table materialised in the database. Methods on the\n                SplinkDataFrame allow you to access the underlying data.\n        \"\"\"\n\n        # Allows clustering during a deterministic linkage.\n        # This is used in `cluster_pairwise_predictions_at_threshold`\n        # to set the cluster threshold to 1\n        self._deterministic_link_mode = True\n\n        concat_with_tf = self._initialise_df_concat_with_tf()\n        sql = block_using_rules_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_blocked\")\n        return self._execute_sql_pipeline([concat_with_tf])\n\n    def estimate_u_using_random_sampling(\n        self, max_pairs: int = None, seed: int = None, *, target_rows=None\n    ):\n\"\"\"Estimate the u parameters of the linkage model using random sampling.\n\n        The u parameters represent the proportion of record comparisons that fall\n        into each comparison level amongst truly non-matching records.\n\n        This procedure takes a sample of the data and generates the cartesian\n        product of pairwise record comparisons amongst the sampled records.\n        The validity of the u values rests on the assumption that the resultant\n        pairwise comparisons are non-matches (or at least, they are very unlikely to be\n        matches). For large datasets, this is typically true.\n\n        The results of estimate_u_using_random_sampling, and therefore an entire splink\n        model, can be made reproducible by setting the seed parameter. Setting the seed\n        will have performance implications as additional processing is required.\n\n        Args:\n            max_pairs (int): The maximum number of pairwise record comparisons to\n            sample. Larger will give more accurate estimates\n            but lead to longer runtimes.  In our experience at least 1e9 (one billion)\n            gives best results but can take a long time to compute. 1e7 (ten million)\n            is often adequate whilst testing different model specifications, before\n            the final model is estimated.\n            seed (int): Seed for random sampling. Assign to get reproducible u\n            probabilities. Note, seed for random sampling is only supported for\n            DuckDB and Spark, for Athena and SQLite set to None.\n\n        Examples:\n            ```py\n            linker.estimate_u_using_random_sampling(1e8)\n            ```\n\n        Returns:\n            None: Updates the estimated u parameters within the linker object\n            and returns nothing.\n        \"\"\"\n        # TODO: Remove this compatibility code in a future release once we drop\n        # support for \"target_rows\". Deprecation warning added in 3.7.0\n        if max_pairs is not None and target_rows is not None:\n            # user supplied both\n            raise TypeError(\"Just use max_pairs\")\n        elif max_pairs is not None:\n            # user is doing it correctly\n            pass\n        elif target_rows is not None:\n            # user is using deprecated argument\n            warnings.warn(\n                \"target_rows is deprecated; use max_pairs\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            max_pairs = target_rows\n        else:\n            raise TypeError(\"Missing argument max_pairs\")\n\n        estimate_u_values(self, max_pairs, seed)\n        self._populate_m_u_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n    def estimate_m_from_label_column(self, label_colname: str):\n\"\"\"Estimate the m parameters of the linkage model from a label (ground truth)\n        column in the input dataframe(s).\n\n        The m parameters represent the proportion of record comparisons that fall\n        into each comparison level amongst truly matching records.\n\n        The ground truth column is used to generate pairwise record comparisons\n        which are then assumed to be matches.\n\n        For example, if the entity being matched is persons, and your input dataset(s)\n        contain social security number, this could be used to estimate the m values\n        for the model.\n\n        Note that this column does not need to be fully populated.  A common case is\n        where a unique identifier such as social security number is only partially\n        populated.\n\n        Args:\n            label_colname (str): The name of the column containing the ground truth\n                label in the input data.\n\n        Examples:\n            ```py\n            linker.estimate_m_from_label_column(\"social_security_number\")\n            ```\n\n        Returns:\n            Updates the estimated m parameters within the linker object\n            and returns nothing.\n        \"\"\"\n\n        # Ensure this has been run on the main linker so that it can be used by\n        # training linked when it checks the cache\n        self._initialise_df_concat_with_tf()\n        estimate_m_values_from_label_column(\n            self,\n            self._input_tables_dict,\n            label_colname,\n        )\n        self._populate_m_u_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n    def estimate_parameters_using_expectation_maximisation(\n        self,\n        blocking_rule: str,\n        comparisons_to_deactivate: list[str | Comparison] = None,\n        comparison_levels_to_reverse_blocking_rule: list[ComparisonLevel] = None,\n        fix_probability_two_random_records_match: bool = False,\n        fix_m_probabilities=False,\n        fix_u_probabilities=True,\n        populate_probability_two_random_records_match_from_trained_values=False,\n    ) -&gt; EMTrainingSession:\n\"\"\"Estimate the parameters of the linkage model using expectation maximisation.\n\n        By default, the m probabilities are estimated, but not the u probabilities,\n        because good estimates for the u probabilities can be obtained from\n        `linker.estimate_u_using_random_sampling()`.  You can change this by setting\n        `fix_u_probabilities` to False.\n\n        The blocking rule provided is used to generate pairwise record comparisons.\n        Usually, this should be a blocking rule that results in a dataframe where\n        matches are between about 1% and 99% of the comparisons.\n\n        By default, m parameters are estimated for all comparisons except those which\n        are included in the blocking rule.\n\n        For example, if the blocking rule is `l.first_name = r.first_name`, then\n        parameter esimates will be made for all comparison except those which use\n        `first_name` in their sql_condition\n\n        By default, the probability two random records match is estimated for the\n        blocked data, and then the m and u parameters for the columns specified in the\n        blocking rules are used to estiamte the global probability two random records\n        match.\n\n        To control which comparisons should have their parameter estimated, and the\n        process of 'reversing out' the global probability two random records match, the\n        user may specify `comparisons_to_deactivate` and\n        `comparison_levels_to_reverse_blocking_rule`.   This is useful, for example\n        if you block on the dmetaphone of a column but match on the original column.\n\n        Examples:\n            Default behaviour\n            ```py\n            br_training = \"l.first_name = r.first_name and l.dob = r.dob\"\n            linker.estimate_parameters_using_expectation_maximisation(br_training)\n            ```\n            Specify which comparisons to deactivate\n            ```py\n            br_training = \"l.dmeta_first_name = r.dmeta_first_name\"\n            settings_obj = linker._settings_obj\n            comp = settings_obj._get_comparison_by_output_column_name(\"first_name\")\n            dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1)\n            linker.estimate_parameters_using_expectation_maximisation(\n                br_training,\n                comparisons_to_deactivate=[\"first_name\"],\n                comparison_levels_to_reverse_blocking_rule=[dmeta_level],\n            )\n            ```\n\n        Args:\n            blocking_rule (str): The blocking rule used to generate pairwise record\n                comparisons.\n            comparisons_to_deactivate (list, optional): By default, splink will\n                analyse the blocking rule provided and estimate the m parameters for\n                all comaprisons except those included in the blocking rule.  If\n                comparisons_to_deactivate are provided, spink will instead\n                estimate m parameters for all comparison except those specified\n                in the comparisons_to_deactivate list.  This list can either contain\n                the output_column_name of the Comparison as a string, or Comparison\n                objects.  Defaults to None.\n            comparison_levels_to_reverse_blocking_rule (list, optional): By default,\n                splink will analyse the blocking rule provided and adjust the\n                global probability two random records match to account for the matches\n                specified in the blocking rule. If provided, this argument will overrule\n                this default behaviour. The user must provide a list of ComparisonLevel\n                objects.  Defaults to None.\n            fix_probability_two_random_records_match (bool, optional): If True, do not\n                update the probability two random records match after each iteration.\n                Defaults to False.\n            fix_m_probabilities (bool, optional): If True, do not update the m\n                probabilities after each iteration. Defaults to False.\n            fix_u_probabilities (bool, optional): If True, do not update the u\n                probabilities after each iteration. Defaults to True.\n            populate_probability_two_random_records_match_from_trained_values\n                (bool, optional): If True, derive this parameter from\n                the blocked value. Defaults to False.\n\n        Examples:\n            ```py\n            blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\"\n            linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n            ```\n\n        Returns:\n            EMTrainingSession:  An object containing information about the training\n                session such as how parameters changed during the iteration history\n\n        \"\"\"\n        # Ensure this has been run on the main linker so that it's in the cache\n        # to be used by the training linkers\n        self._initialise_df_concat_with_tf()\n\n        if comparisons_to_deactivate:\n            # If user provided a string, convert to Comparison object\n            comparisons_to_deactivate = [\n                self._settings_obj._get_comparison_by_output_column_name(n)\n                if isinstance(n, str)\n                else n\n                for n in comparisons_to_deactivate\n            ]\n            if comparison_levels_to_reverse_blocking_rule is None:\n                logger.warning(\n                    \"\\nWARNING: \\n\"\n                    \"You have provided comparisons_to_deactivate but not \"\n                    \"comparison_levels_to_reverse_blocking_rule.\\n\"\n                    \"If comparisons_to_deactivate is provided, then \"\n                    \"you usually need to provide corresponding \"\n                    \"comparison_levels_to_reverse_blocking_rule \"\n                    \"because each comparison to deactivate is effectively treated \"\n                    \"as an exact match.\"\n                )\n\n        em_training_session = EMTrainingSession(\n            self,\n            blocking_rule,\n            fix_u_probabilities=fix_u_probabilities,\n            fix_m_probabilities=fix_m_probabilities,\n            fix_probability_two_random_records_match=fix_probability_two_random_records_match,  # noqa 501\n            comparisons_to_deactivate=comparisons_to_deactivate,\n            comparison_levels_to_reverse_blocking_rule=comparison_levels_to_reverse_blocking_rule,  # noqa 501\n        )\n\n        em_training_session._train()\n\n        self._populate_m_u_from_trained_values()\n\n        if populate_probability_two_random_records_match_from_trained_values:\n            self._populate_probability_two_random_records_match_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n        return em_training_session\n\n    def predict(\n        self,\n        threshold_match_probability: float = None,\n        threshold_match_weight: float = None,\n        materialise_after_computing_term_frequencies=True,\n    ) -&gt; SplinkDataFrame:\n\"\"\"Create a dataframe of scored pairwise comparisons using the parameters\n        of the linkage model.\n\n        Uses the blocking rules specified in the\n        `blocking_rules_to_generate_predictions` of the settings dictionary to\n        generate the pairwise comparisons.\n\n        Args:\n            threshold_match_probability (float, optional): If specified,\n                filter the results to include only pairwise comparisons with a\n                match_probability above this threshold. Defaults to None.\n            threshold_match_weight (float, optional): If specified,\n                filter the results to include only pairwise comparisons with a\n                match_weight above this threshold. Defaults to None.\n            materialise_after_computing_term_frequencies (bool): If true, Splink\n                will materialise the table containing the input nodes (rows)\n                joined to any term frequencies which have been asked\n                for in the settings object.  If False, this will be\n                computed as part of one possibly gigantic CTE\n                pipeline.   Defaults to True\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            df = linker.predict(threshold_match_probability=0.95)\n            df.as_pandas_dataframe(limit=5)\n            ```\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n                represents a table materialised in the database. Methods on the\n                SplinkDataFrame allow you to access the underlying data.\n\n        \"\"\"\n\n        # If materialise_after_computing_term_frequencies=False and the user only\n        # calls predict, it runs as a single pipeline with no materialisation\n        # of anything.\n\n        # _initialise_df_concat_with_tf returns None if the table doesn't exist\n        # and only SQL is queued in this step.\n        nodes_with_tf = self._initialise_df_concat_with_tf(\n            materialise=materialise_after_computing_term_frequencies\n        )\n\n        input_dataframes = []\n        if nodes_with_tf:\n            input_dataframes.append(nodes_with_tf)\n\n        sql = block_using_rules_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_blocked\")\n\n        repartition_after_blocking = getattr(self, \"repartition_after_blocking\", False)\n\n        # repartition after blocking only exists on the SparkLinker\n        if repartition_after_blocking:\n            df_blocked = self._execute_sql_pipeline(input_dataframes)\n            input_dataframes.append(df_blocked)\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            threshold_match_probability,\n            threshold_match_weight,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        predictions = self._execute_sql_pipeline(input_dataframes)\n        self._predict_warning()\n        return predictions\n\n    def find_matches_to_new_records(\n        self,\n        records_or_tablename,\n        blocking_rules=[],\n        match_weight_threshold=-4,\n    ) -&gt; SplinkDataFrame:\n\"\"\"Given one or more records, find records in the input dataset(s) which match\n        and return in order of the splink prediction score.\n\n        This effectively provides a way of searching the input datasets\n        for given record(s)\n\n        Args:\n            records_or_tablename (List[dict]): Input search record(s) as list of dict,\n                or a table registered to the database.\n            blocking_rules (list, optional): Blocking rules to select\n                which records to find and score. If [], do not use a blocking\n                rule - meaning the input records will be compared to all records\n                provided to the linker when it was instantiated. Defaults to [].\n            match_weight_threshold (int, optional): Return matches with a match weight\n                above this threshold. Defaults to -4.\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            # Pre-compute tf tables for any tables with\n            # term frequency adjustments\n            linker.compute_tf_table(\"first_name\")\n            record = {'unique_id': 1,\n                'first_name': \"John\",\n                'surname': \"Smith\",\n                'dob': \"1971-05-24\",\n                'city': \"London\",\n                'email': \"john@smith.net\"\n                }\n            df = linker.find_matches_to_new_records([record], blocking_rules=[])\n            ```\n\n        Returns:\n            SplinkDataFrame: The pairwise comparisons.\n        \"\"\"\n\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        if not isinstance(records_or_tablename, str):\n            uid = ascii_uid(8)\n            self.register_table(\n                records_or_tablename, f\"__splink__df_new_records_{uid}\", overwrite=True\n            )\n            new_records_tablename = f\"__splink__df_new_records_{uid}\"\n        else:\n            new_records_tablename = records_or_tablename\n\n        cache = self._intermediate_table_cache\n        input_dfs = []\n        # If our df_concat_with_tf table already exists, use backwards inference to\n        # find all underlying term frequency tables.\n        if \"__splink__df_concat_with_tf\" in cache:\n            concat_with_tf = cache[\"__splink__df_concat_with_tf\"]\n            tf_tables = compute_term_frequencies_from_concat_with_tf(self)\n            # This queues up our tf tables, rather materialising them\n            for tf in tf_tables:\n                # if tf is a SplinkDataFrame, then the table already exists\n                if isinstance(tf, SplinkDataFrame):\n                    input_dfs.append(tf)\n                else:\n                    self._enqueue_sql(tf[\"sql\"], tf[\"output_table_name\"])\n        else:\n            # This queues up our cols_with_tf and df_concat_with_tf tables.\n            concat_with_tf = self._initialise_df_concat_with_tf(materialise=False)\n\n        if concat_with_tf:\n            input_dfs.append(concat_with_tf)\n\n        rules = []\n        for r in blocking_rules:\n            br_as_obj = BlockingRule(r) if not isinstance(r, BlockingRule) else r\n            br_as_obj.preceding_rules = rules.copy()\n            rules.append(br_as_obj)\n        blocking_rules = rules\n\n        self._settings_obj._blocking_rules_to_generate_predictions = blocking_rules\n\n        self._settings_obj._link_type = \"link_only_find_matches_to_new_records\"\n        self._find_new_matches_mode = True\n\n        sql = _join_tf_to_input_df_sql(self)\n        sql = sql.replace(\"__splink__df_concat\", new_records_tablename)\n        self._enqueue_sql(sql, \"__splink__df_new_records_with_tf\")\n\n        sql = block_using_rules_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_blocked\")\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = f\"\"\"\n        select * from __splink__df_predict\n        where match_weight &gt; {match_weight_threshold}\n        \"\"\"\n\n        self._enqueue_sql(sql, \"__splink__find_matches_predictions\")\n\n        predictions = self._execute_sql_pipeline(\n            input_dataframes=input_dfs, use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._find_new_matches_mode = False\n\n        return predictions\n\n    def compare_two_records(self, record_1: dict, record_2: dict):\n\"\"\"Use the linkage model to compare and score a pairwise record comparison\n        based on the two input records provided\n\n        Args:\n            record_1 (dict): dictionary representing the first record.  Columns names\n                and data types must be the same as the columns in the settings object\n            record_2 (dict): dictionary representing the second record.  Columns names\n                and data types must be the same as the columns in the settings object\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n\n        Returns:\n            SplinkDataFrame: Pairwise comparison with scored prediction\n        \"\"\"\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        self._compare_two_records_mode = True\n        self._settings_obj._blocking_rules_to_generate_predictions = []\n\n        uid = ascii_uid(8)\n        df_records_left = self.register_table(\n            [record_1], f\"__splink__compare_two_records_left_{uid}\", overwrite=True\n        )\n        df_records_left.templated_name = \"__splink__compare_two_records_left\"\n\n        df_records_right = self.register_table(\n            [record_2], f\"__splink__compare_two_records_right_{uid}\", overwrite=True\n        )\n        df_records_right.templated_name = \"__splink__compare_two_records_right\"\n\n        sql_join_tf = _join_tf_to_input_df_sql(self)\n\n        sql_join_tf = sql_join_tf.replace(\n            \"__splink__df_concat\", \"__splink__compare_two_records_left\"\n        )\n        self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_left_with_tf\")\n\n        sql_join_tf = sql_join_tf.replace(\n            \"__splink__compare_two_records_left\", \"__splink__compare_two_records_right\"\n        )\n\n        self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_right_with_tf\")\n\n        sql = block_using_rules_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_blocked\")\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        predictions = self._execute_sql_pipeline(\n            [df_records_left, df_records_right], use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._compare_two_records_mode = False\n\n        return predictions\n\n    def _self_link(self) -&gt; SplinkDataFrame:\n\"\"\"Use the linkage model to compare and score all records in our input df with\n            themselves.\n\n        Returns:\n            SplinkDataFrame: Scored pairwise comparisons of the input records to\n                themselves.\n        \"\"\"\n\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        # Changes our sql to allow for a self link.\n        # This is used in `_sql_gen_where_condition` in blocking.py\n        # to remove any 'where' clauses when blocking (normally when blocking\n        # we want to *remove* self links!)\n        self._self_link_mode = True\n\n        # Block on uid i.e. create pairwise record comparisons where the uid matches\n        uid_cols = self._settings_obj._unique_id_input_columns\n        uid_l = _composite_unique_id_from_edges_sql(uid_cols, None, \"l\")\n        uid_r = _composite_unique_id_from_edges_sql(uid_cols, None, \"r\")\n\n        self._settings_obj._blocking_rules_to_generate_predictions = [\n            BlockingRule(f\"{uid_l} = {uid_r}\")\n        ]\n\n        nodes_with_tf = self._initialise_df_concat_with_tf()\n\n        sql = block_using_rules_sql(self)\n\n        self._enqueue_sql(sql, \"__splink__df_blocked\")\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            output_table_name = sql[\"output_table_name\"]\n            output_table_name = output_table_name.replace(\"predict\", \"self_link\")\n            self._enqueue_sql(sql[\"sql\"], output_table_name)\n\n        predictions = self._execute_sql_pipeline(\n            input_dataframes=[nodes_with_tf], use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._self_link_mode = False\n\n        return predictions\n\n    def cluster_pairwise_predictions_at_threshold(\n        self,\n        df_predict: SplinkDataFrame,\n        threshold_match_probability: float = None,\n        pairwise_formatting: bool = False,\n        filter_pairwise_format_for_clusters: bool = True,\n    ) -&gt; SplinkDataFrame:\n\"\"\"Clusters the pairwise match predictions that result from `linker.predict()`\n        into groups of connected record using the connected components graph clustering\n        algorithm\n\n        Records with an estimated `match_probability` above\n        `threshold_match_probability` are considered to be a match (i.e. they represent\n        the same entity).\n\n        Args:\n            df_predict (SplinkDataFrame): The results of `linker.predict()`\n            threshold_match_probability (float): Filter the pairwise match predictions\n                to include only pairwise comparisons with a match_probability above this\n                threshold. This dataframe is then fed into the clustering\n                algorithm.\n            pairwise_formatting (bool): Whether to output the pairwise match predictions\n                from linker.predict() with cluster IDs.\n                If this is set to false, the output will be a list of all IDs, clustered\n                into groups based on the desired match threshold.\n            filter_pairwise_format_for_clusters (bool): If pairwise formatting has been\n                selected, whether to output all columns found within linker.predict(),\n                or just return clusters.\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered\n                into groups based on the desired match threshold.\n\n        \"\"\"\n\n        # Feeding in df_predict forces materiailisation, if it exists in your database\n        concat_with_tf = self._initialise_df_concat_with_tf(df_predict)\n\n        edges_table = _cc_create_unique_id_cols(\n            self,\n            concat_with_tf.physical_name,\n            df_predict.physical_name,\n            threshold_match_probability,\n        )\n\n        cc = solve_connected_components(\n            self,\n            edges_table,\n            df_predict,\n            concat_with_tf,\n            pairwise_formatting,\n            filter_pairwise_format_for_clusters,\n        )\n\n        return cc\n\n    def profile_columns(\n        self, column_expressions: str | list[str], top_n=10, bottom_n=10\n    ):\n        return profile_columns(self, column_expressions, top_n=top_n, bottom_n=bottom_n)\n\n    def _get_labels_tablename_from_input(\n        self, labels_splinkdataframe_or_table_name: str | SplinkDataFrame\n    ):\n        if isinstance(labels_splinkdataframe_or_table_name, SplinkDataFrame):\n            labels_tablename = labels_splinkdataframe_or_table_name.physical_name\n        elif isinstance(labels_splinkdataframe_or_table_name, str):\n            labels_tablename = labels_splinkdataframe_or_table_name\n        else:\n            raise ValueError(\n                \"The 'labels_splinkdataframe_or_table_name' argument\"\n                \" must be of type SplinkDataframe or a string representing a tablename\"\n                \" in the input database\"\n            )\n        return labels_tablename\n\n    def estimate_m_from_pairwise_labels(self, labels_splinkdataframe_or_table_name):\n\"\"\"Estimate the m parameters of the linkage model from a dataframe of pairwise\n        labels.\n\n        The table of labels should be in the following format, and should\n        be registered with your database:\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|\n        |----------------|-----------|----------------|-----------|\n        |df_1            |1          |df_2            |2          |\n        |df_1            |1          |df_2            |3          |\n\n        Note that `source_dataset` and `unique_id` should correspond to the\n        values specified in the settings dict, and the `input_table_aliases`\n        passed to the `linker` object. Note that at the moment, this method does\n        not respect values in a `clerical_match_score` column.  If provided, these\n        are ignored and it is assumed that every row in the table of labels is a score\n        of 1, i.e. a perfect match.\n\n        Args:\n          labels_splinkdataframe_or_table_name (str): Name of table containing labels\n            in the database or SplinkDataframe\n\n        Examples:\n            ```py\n            pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\")\n            linker.register_table(pairwise_labels, \"labels\", overwrite=True)\n            linker.estimate_m_from_pairwise_labels(\"labels\")\n            ```\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        estimate_m_from_pairwise_labels(self, labels_tablename)\n\n    def truth_space_table_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ) -&gt; SplinkDataFrame:\n\"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n        match_probability, suitable for plotting a ROC chart.\n\n        The table of labels should be in the following format, and should be registered\n        with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            === \"DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.truth_space_table_from_labels_table(\"labels\")\n                ```\n            === \"Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.truth_space_table_from_labels_table(\"labels\")\n                ```\n        Returns:\n            SplinkDataFrame:  Table of truth statistics\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        return truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n\n    def roc_chart_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name: str | SplinkDataFrame,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n\"\"\"Generate a ROC chart from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            === \"DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.roc_chart_from_labels_table(\"labels\")\n                ```\n            === \"Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.roc_chart_from_labels_table(\"labels\")\n                ```\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return roc_chart(recs)\n\n    def precision_recall_chart_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n\"\"\"Generate a precision-recall chart from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        as a table with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n        Examples:\n            === \"DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.precision_recall_chart_from_labels_table(\"labels\")\n                ```\n            === \"Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.precision_recall_chart_from_labels_table(\"labels\")\n                ```\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return precision_recall_chart(recs)\n\n    def prediction_errors_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        include_false_positives=True,\n        include_false_negatives=True,\n        threshold=0.5,\n    ):\n\"\"\"Generate a dataframe containing false positives and false negatives\n        based on the comparison between the clerical_match_score in the labels\n        table compared with the splink predicted match probability\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            include_false_positives (bool, optional): Defaults to True.\n            include_false_negatives (bool, optional): Defaults to True.\n            threshold (float, optional): Threshold above which a score is considered\n                to be a match. Defaults to 0.5.\n\n        Returns:\n            SplinkDataFrame:  Table containing false positives and negatives\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        return prediction_errors_from_labels_table(\n            self,\n            labels_tablename,\n            include_false_positives,\n            include_false_negatives,\n            threshold,\n        )\n\n    def truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n\"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n        match_probability, suitable for plotting a ROC chart.\n\n        Your labels_column_name should include the ground truth cluster (unique\n        identifier) that groups entities which are the same\n\n        Args:\n            labels_tablename (str): Name of table containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            ```py\n            linker.truth_space_table_from_labels_column(\"cluster\")\n            ```\n\n        Returns:\n            SplinkDataFrame:  Table of truth statistics\n        \"\"\"\n\n        return truth_space_table_from_labels_column(\n            self, labels_column_name, threshold_actual, match_weight_round_to_nearest\n        )\n\n    def roc_chart_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n\"\"\"Generate a ROC chart from ground truth data, whereby the ground truth\n        is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            ```py\n            linker.roc_chart_from_labels_column(\"labels\")\n            ```\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n        \"\"\"\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return roc_chart(recs)\n\n    def precision_recall_chart_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n\"\"\"Generate a precision-recall chart from ground truth data, whereby the ground\n        truth is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n        Examples:\n            ```py\n            linker.precision_recall_chart_from_labels_column(\"ground_truth\")\n            ```\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n        \"\"\"\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return precision_recall_chart(recs)\n\n    def prediction_errors_from_labels_column(\n        self,\n        label_colname,\n        include_false_positives=True,\n        include_false_negatives=True,\n        threshold=0.5,\n    ):\n\"\"\"Generate a dataframe containing false positives and false negatives\n        based on the comparison between the splink match probability and the\n        labels column.  A label column is a column in the input dataset that contains\n        the 'ground truth' cluster to which the record belongs\n\n        Args:\n            label_colname (str): Name of labels column in input data\n            include_false_positives (bool, optional): Defaults to True.\n            include_false_negatives (bool, optional): Defaults to True.\n            threshold (float, optional): Threshold above which a score is considered\n                to be a match. Defaults to 0.5.\n\n        Returns:\n            SplinkDataFrame:  Table containing false positives and negatives\n        \"\"\"\n        return prediction_errors_from_label_column(\n            self,\n            label_colname,\n            include_false_positives,\n            include_false_negatives,\n            threshold,\n        )\n\n    def match_weights_histogram(\n        self, df_predict: SplinkDataFrame, target_bins: int = 30, width=600, height=250\n    ):\n\"\"\"Generate a histogram that shows the distribution of match weights in\n        `df_predict`\n\n        Args:\n            df_predict (SplinkDataFrame): Output of `linker.predict()`\n            target_bins (int, optional): Target number of bins in histogram. Defaults to\n                30.\n            width (int, optional): Width of output. Defaults to 600.\n            height (int, optional): Height of output chart. Defaults to 250.\n\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n\n        \"\"\"\n        df = histogram_data(self, df_predict, target_bins)\n        recs = df.as_record_dict()\n        return match_weights_histogram(recs, width=width, height=height)\n\n    def waterfall_chart(self, records: list[dict], filter_nulls=True):\n\"\"\"Visualise how the final match weight is computed for the provided pairwise\n        record comparisons.\n\n        Records must be provided as a list of dictionaries. This would usually be\n        obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame.\n\n        Examples:\n            ```py\n            df = linker.predict(threshold_match_weight=2)\n            records = df.as_record_dict(limit=10)\n            linker.waterfall_chart(records)\n            ```\n\n        Args:\n            records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)`\n                where `df` is a SplinkDataFrame.\n            filter_nulls (bool, optional): Whether the visualiation shows null\n                comparisons, which have no effect on final match weight. Defaults to\n                True.\n\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        return waterfall_chart(records, self._settings_obj, filter_nulls)\n\n    def unlinkables_chart(\n        self,\n        x_col=\"match_weight\",\n        source_dataset=None,\n        as_dict=False,\n    ):\n\"\"\"Generate an interactive chart displaying the proportion of records that\n        are \"unlinkable\" for a given splink score threshold and model parameters.\n\n        Unlinkable records are those that, even when compared with themselves, do not\n        contain enough information to confirm a match.\n\n        Args:\n            x_col (str, optional): Column to use for the x-axis.\n                Defaults to \"match_weight\".\n            source_dataset (str, optional): Name of the source dataset to use for\n                the title of the output chart.\n            as_dict (bool, optional): If True, return a dict version of the chart.\n\n        Examples:\n            For the simplest code pipeline, load a pre-trained model\n            and run this against the test data.\n            ```py\n            df = pd.read_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\")\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.unlinkables_chart()\n            ```\n            For more complex code pipelines, you can run an entire pipeline\n            that estimates your m and u values, before `unlinkables_chart().\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n        \"\"\"\n\n        # Link our initial df on itself and calculate the % of unlinkable entries\n        records = unlinkables_data(self)\n        return unlinkables_chart(records, x_col, source_dataset, as_dict)\n\n    def comparison_viewer_dashboard(\n        self,\n        df_predict: SplinkDataFrame,\n        out_path: str,\n        overwrite=False,\n        num_example_rows=2,\n        return_html_as_string=False,\n    ):\n\"\"\"Generate an interactive html visualization of the linker's predictions and\n        save to `out_path`.  For more information see\n        [this video](https://www.youtube.com/watch?v=DNvCMqjipis)\n\n\n        Args:\n            df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n            out_path (str): The path (including filename) to save the html file to.\n            overwrite (bool, optional): Overwrite the html file if it already exists?\n                Defaults to False.\n            num_example_rows (int, optional): Number of example rows per comparison\n                vector. Defaults to 2.\n            return_html_as_string: If True, return the html as a string\n\n        Examples:\n            ```py\n            df_predictions = linker.predict()\n            linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n            ```\n\n            Optionally, in Jupyter, you can display the results inline\n            Otherwise you can just load the html file in your browser\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n            ```\n\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        sql = comparison_vector_distribution_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vector_distribution\")\n\n        sqls = comparison_viewer_table_sqls(self, num_example_rows)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        df = self._execute_sql_pipeline([df_predict])\n\n        rendered = render_splink_comparison_viewer_html(\n            df.as_record_dict(),\n            self._settings_obj._as_completed_dict(),\n            out_path,\n            overwrite,\n        )\n        if return_html_as_string:\n            return rendered\n\n    def parameter_estimate_comparisons_chart(self, include_m=True, include_u=True):\n\"\"\"Show a chart that shows how parameter estimates have differed across\n        the different estimation methods you have used.\n\n        For example, if you have run two EM estimation sessions, blocking on\n        different variables, and both result in parameter estimates for\n        first_name, this chart will enable easy comparison of the different\n        estimates\n\n        Args:\n            include_m (bool, optional): Show different estimates of m values. Defaults\n                to True.\n            include_u (bool, optional): Show different estimates of u values. Defaults\n                to True.\n\n        \"\"\"\n        records = self._settings_obj._parameter_estimates_as_records\n\n        to_retain = []\n        if include_m:\n            to_retain.append(\"m\")\n        if include_u:\n            to_retain.append(\"u\")\n\n        records = [r for r in records if r[\"m_or_u\"] in to_retain]\n\n        return parameter_estimate_comparisons(records)\n\n    def missingness_chart(self, input_dataset: str = None):\n\"\"\"Generate a summary chart of the missingness (prevalence of nulls) of\n        columns in the input datasets.  By default, missingness is assessed across\n        all input datasets\n\n        Args:\n            input_dataset (str, optional): Name of one of the input tables in the\n            database.  If provided, missingness will be computed for this table alone.\n            Defaults to None.\n\n        Examples:\n            ```py\n            linker.missingness_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.missingness_chart()\n            save_offline_chart(c.spec, \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500\n            ```\n        \"\"\"\n        records = missingness_data(self, input_dataset)\n        return missingness_chart(records)\n\n    def completeness_chart(self, input_dataset: str = None, cols: list[str] = None):\n\"\"\"Generate a summary chart of the completeness (proportion of non-nulls) of\n        columns in each of the input datasets. By default, completeness is assessed for\n        all column in the input data.\n\n        Args:\n            input_dataset (str, optional): Name of one of the input tables in the\n                database.  If provided, completeness will be computed for this table\n                alone. Defaults to None.\n            cols (List[str], optional): List of column names to calculate completeness.\n                Default to None.\n\n        Examples:\n            ```py\n            linker.completeness_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.completeness_chart()\n            save_offline_chart(c.spec, \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500\n            ```\n        \"\"\"\n        records = completeness_data(self, input_dataset, cols)\n        return completeness_chart(records)\n\n    def count_num_comparisons_from_blocking_rule(\n        self,\n        blocking_rule: str,\n    ) -&gt; int:\n\"\"\"Compute the number of pairwise record comparisons that would be generated by\n        a blocking rule\n\n        Args:\n            blocking_rule (str): The blocking rule to analyse\n            link_type (str, optional): The link type.  This is needed only if the\n                linker has not yet been provided with a settings dictionary.  Defaults\n                to None.\n            unique_id_column_name (str, optional):  This is needed only if the\n                linker has not yet been provided with a settings dictionary.  Defaults\n                to None.\n\n        Examples:\n            ```py\n            br = \"l.first_name = r.first_name\"\n            linker.count_num_comparisons_from_blocking_rule(br)\n            ```\n            &gt; 19387\n            ```py\n            br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            linker.count_num_comparisons_from_blocking_rule(br)\n            ```\n            &gt; 394\n\n        Returns:\n            int: The number of comparisons generated by the blocking rule\n        \"\"\"\n\n        sql = vertically_concatenate_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_concat\")\n\n        sql = number_of_comparisons_generated_by_blocking_rule_sql(self, blocking_rule)\n        self._enqueue_sql(sql, \"__splink__analyse_blocking_rule\")\n        res = self._execute_sql_pipeline().as_record_dict()[0]\n        return res[\"count_of_pairwise_comparisons_generated\"]\n\n    def cumulative_comparisons_from_blocking_rules_records(\n        self,\n        blocking_rules: str or list = None,\n    ):\n\"\"\"Output the number of comparisons generated by each successive blocking rule.\n\n        This is equivalent to the output size of df_predict and details how many\n        comparisons each of your individual blocking rules will contribute to the\n        total.\n\n        Args:\n            blocking_rules (str or list): The blocking rule(s) to compute comparisons\n                for. If null, the rules set out in your settings object will be used.\n\n        Examples:\n            ```py\n            linker_settings = DuckDBLinker(df, settings)\n            # Compute the cumulative number of comparisons generated by the rules\n            # in your settings object.\n            linker_settings.cumulative_comparisons_from_blocking_rules_records()\n            &gt;&gt;&gt;\n            # Generate total comparisons with custom blocking rules.\n            blocking_rules = [\n               \"l.surname = r.surname\",\n               \"l.first_name = r.first_name\n                and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            ]\n            &gt;&gt;&gt;\n            linker_settings.cumulative_comparisons_from_blocking_rules_records(\n                blocking_rules\n             )\n            ```\n\n        Returns:\n            List: A list of blocking rules and the corresponding number of\n                comparisons it is forecast to generate.\n        \"\"\"\n        if blocking_rules:\n            blocking_rules = ensure_is_list(blocking_rules)\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self, blocking_rules, output_chart=False\n        )\n\n        return records\n\n    def cumulative_num_comparisons_from_blocking_rules_chart(\n        self,\n        blocking_rules: str or list = None,\n    ):\n\"\"\"Display a chart with the cumulative number of comparisons generated by a\n        selection of blocking rules.\n\n        This is equivalent to the output size of df_predict and details how many\n        comparisons each of your individual blocking rules will contribute to the\n        total.\n\n        Args:\n            blocking_rules (str or list): The blocking rule(s) to compute comparisons\n                for. If null, the rules set out in your settings object will be used.\n\n        Examples:\n            ```py\n            linker_settings = DuckDBLinker(df, settings)\n            # Compute the cumulative number of comparisons generated by the rules\n            # in your settings object.\n            linker_settings.cumulative_num_comparisons_from_blocking_rules_chart()\n            &gt;&gt;&gt;\n            # Generate total comparisons with custom blocking rules.\n            blocking_rules = [\n               \"l.surname = r.surname\",\n               \"l.first_name = r.first_name\n                and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            ]\n            &gt;&gt;&gt;\n            linker_settings.cumulative_num_comparisons_from_blocking_rules_chart(\n                blocking_rules\n             )\n            ```\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n        \"\"\"\n\n        if blocking_rules:\n            blocking_rules = ensure_is_list(blocking_rules)\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self, blocking_rules, output_chart=True\n        )\n\n        return cumulative_blocking_rule_comparisons_generated(records)\n\n    def count_num_comparisons_from_blocking_rules_for_prediction(self, df_predict):\n\"\"\"Counts the maginal number of edges created from each of the blocking rules\n        in `blocking_rules_to_generate_predictions`\n\n        This is different to `count_num_comparisons_from_blocking_rule`\n        because it (a) analyses multiple blocking rules rather than a single rule, and\n        (b) deduplicates any comparisons that are generated, to tell you the\n        marginal effect of each entry in `blocking_rules_to_generate_predictions`\n\n        Args:\n            df_predict (SplinkDataFrame): SplinkDataFrame with match weights\n            and probabilities of rows matching\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df, connection=\":memory:\")\n            linker.load_settings(\"saved_settings.json\")\n            df_predict = linker.predict(threshold_match_probability=0.95)\n            count_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict)\n            count_pairwise.as_pandas_dataframe(limit=5)\n            ```\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons and\n                estimated pairwise comparisons generated by the blocking rules.\n        \"\"\"  # noqa: E501\n        sql = count_num_comparisons_from_blocking_rules_for_prediction_sql(\n            self, df_predict\n        )\n        match_key_analysis = self._sql_to_splink_dataframe_checking_cache(\n            sql, \"__splink__match_key_analysis\"\n        )\n        return match_key_analysis\n\n    def match_weights_chart(self):\n\"\"\"Display a chart of the (partial) match weights of the linkage model\n\n        Examples:\n            ```py\n            linker.match_weights_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.match_weights_chart()\n            save_offline_chart(c.spec, \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500)\n            ```\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n        \"\"\"\n        return self._settings_obj.match_weights_chart()\n\n    def tf_adjustment_chart(\n        self,\n        output_column_name: str,\n        n_most_freq: int = 10,\n        n_least_freq: int = 10,\n        vals_to_include: str | list = None,\n        as_dict: bool = False,\n    ):\n\"\"\"Display a chart showing the impact of term frequency adjustments on a\n        specific comparison level.\n        Each value\n\n        Args:\n            output_column_name (str): Name of an output column for which term frequency\n                 adjustment has been applied.\n            n_most_freq (int, optional): Number of most frequent values to show. If this\n                 or `n_least_freq` set to None, all values will be shown.\n                Default to 10.\n            n_least_freq (int, optional): Number of least frequent values to show. If\n                this or `n_most_freq` set to None, all values will be shown.\n                Default to 10.\n            vals_to_include (list, optional): Specific values for which to show term\n                sfrequency adjustments.\n                Defaults to None.\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n        \"\"\"\n\n        # Comparisons with TF adjustments\n        tf_comparisons = [\n            c._output_column_name\n            for c in self._settings_obj.comparisons\n            if any([cl._has_tf_adjustments for cl in c.comparison_levels])\n        ]\n        if output_column_name not in tf_comparisons:\n            raise ValueError(\n                f\"{output_column_name} is not a valid comparison column, or does not\"\n                f\" have term frequency adjustment activated\"\n            )\n\n        vals_to_include = ensure_is_list(vals_to_include)\n\n        return tf_adjustment_chart(\n            self,\n            output_column_name,\n            n_most_freq,\n            n_least_freq,\n            vals_to_include,\n            as_dict,\n        )\n\n    def m_u_parameters_chart(self):\n\"\"\"Display a chart of the m and u parameters of the linkage model\n\n        Examples:\n            ```py\n            linker.m_u_parameters_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.match_weights_chart()\n            save_offline_chart(c.spec, \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500)\n            ```\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n        \"\"\"\n\n        return self._settings_obj.m_u_parameters_chart()\n\n    def cluster_studio_dashboard(\n        self,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        out_path: str,\n        sampling_method=\"random\",\n        sample_size: int = 10,\n        cluster_ids: list = None,\n        cluster_names: list = None,\n        overwrite: bool = False,\n        return_html_as_string=False,\n    ):\n\"\"\"Generate an interactive html visualization of the predicted cluster and\n        save to `out_path`.\n\n        Args:\n            df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n            df_clustered (SplinkDataFrame): The outputs of\n                `linker.cluster_pairwise_predictions_at_threshold()`\n            out_path (str): The path (including filename) to save the html file to.\n            sampling_method (str, optional): `random` or `by_cluster_size`. Defaults to\n                `random`.\n            sample_size (int, optional): Number of clusters to show in the dahboard.\n                Defaults to 10.\n            cluster_ids (list): The IDs of the clusters that will be displayed in the\n                dashboard.  If provided, ignore the `sampling_method` and `sample_size`\n                arguments. Defaults to None.\n            overwrite (bool, optional): Overwrite the html file if it already exists?\n                Defaults to False.\n            cluster_names (list, optional): If provided, the dashboard will display\n                these names in the selection box. Ony works in conjunction with\n                `cluster_ids`.  Defaults to None.\n            return_html_as_string: If True, return the html as a string\n\n        Examples:\n            ```py\n            df_p = linker.predict()\n            df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\n            linker.cluster_studio_dashboard(\n                df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n            )\n            ```\n            Optionally, in Jupyter, you can display the results inline\n            Otherwise you can just load the html file in your browser\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n            ```\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        rendered = render_splink_cluster_studio_html(\n            self,\n            df_predict,\n            df_clustered,\n            out_path,\n            sampling_method=sampling_method,\n            sample_size=sample_size,\n            cluster_ids=cluster_ids,\n            overwrite=overwrite,\n            cluster_names=cluster_names,\n        )\n\n        if return_html_as_string:\n            return rendered\n\n    def save_model_to_json(\n        self, out_path: str | None = None, overwrite: bool = False\n    ) -&gt; dict:\n\"\"\"Save the configuration and parameters of the linkage model to a `.json` file.\n\n        The model can later be loaded back in using `linker.load_model()`.\n        The settings dict is also returned in case you want to save it a different way.\n\n        Examples:\n            ```py\n            linker.save_model_to_json(\"my_settings.json\", overwrite=True)\n            ```\n        Args:\n            out_path (str, optional): File path for json file. If None, don't save to\n                file. Defaults to None.\n            overwrite (bool, optional): Overwrite if already exists? Defaults to False.\n\n        Returns:\n            dict: The settings as a dictionary.\n        \"\"\"\n        model_dict = self._settings_obj.as_dict()\n        if out_path:\n            if os.path.isfile(out_path) and not overwrite:\n                raise ValueError(\n                    f\"The path {out_path} already exists. Please provide a different \"\n                    \"path or set overwrite=True\"\n                )\n            with open(out_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(model_dict, f, indent=4)\n        return model_dict\n\n    def save_settings_to_json(\n        self, out_path: str | None = None, overwrite: bool = False\n    ) -&gt; dict:\n\"\"\"\n        This function is deprecated. Use save_model_to_json() instead.\n        \"\"\"\n        warnings.warn(\n            \"This function is deprecated. Use save_model_to_json() instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return self.save_model_to_json(out_path, overwrite)\n\n    def estimate_probability_two_random_records_match(\n        self, deterministic_matching_rules, recall\n    ):\n\"\"\"Estimate the model parameter `probability_two_random_records_match` using\n        a direct estimation approach.\n\n        See [here](https://github.com/moj-analytical-services/splink/issues/462)\n        for discussion of methodology\n\n        Args:\n            deterministic_matching_rules (list): A list of deterministic matching\n                rules that should be designed to admit very few (none if possible)\n                false positives\n            recall (float): A guess at the recall the deterministic matching rules\n                will attain.  i.e. what proportion of true matches will be recovered\n                by these deterministic rules\n        \"\"\"\n\n        if (recall &gt; 1) or (recall &lt;= 0):\n            raise ValueError(\n                f\"Estimated recall must be greater than 0 \"\n                f\"and no more than 1. Supplied value {recall}.\"\n            )\n\n        # If user, by error, provides a single rule as a string\n        if isinstance(deterministic_matching_rules, str):\n            deterministic_matching_rules = [deterministic_matching_rules]\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self,\n            deterministic_matching_rules,\n        )\n\n        summary_record = records[-1]\n        num_observed_matches = summary_record[\"cumulative_rows\"]\n        num_total_comparisons = summary_record[\"cartesian\"]\n\n        if num_observed_matches &gt; num_total_comparisons * recall:\n            raise ValueError(\n                f\"Deterministic matching rules led to more \"\n                f\"observed matches than is consistent with supplied recall. \"\n                f\"With these rules, recall must be at least \"\n                f\"{num_observed_matches/num_total_comparisons:,.2f}.\"\n            )\n\n        num_expected_matches = num_observed_matches / recall\n        prob = num_expected_matches / num_total_comparisons\n\n        # warn about boundary values, as these will usually be in error\n        if num_observed_matches == 0:\n            logger.warning(\n                f\"WARNING: Deterministic matching rules led to no observed matches! \"\n                f\"This means that no possible record pairs are matches, \"\n                f\"and no records are linked to one another.\\n\"\n                f\"If this is truly the case then you do not need \"\n                f\"to run the linkage model.\\n\"\n                f\"However this is usually in error; \"\n                f\"expected rules to have recall of {100*recall:,.0f}%. \"\n                f\"Consider revising rules as they may have an error.\"\n            )\n        if prob == 1:\n            logger.warning(\n                \"WARNING: Probability two random records match is estimated to be 1.\\n\"\n                \"This means that all possible record pairs are matches, \"\n                \"and all records are linked to one another.\\n\"\n                \"If this is truly the case then you do not need \"\n                \"to run the linkage model.\\n\"\n                \"However, it is more likely that this estimate is faulty. \"\n                \"Perhaps your deterministic matching rules include \"\n                \"too many false positives?\"\n            )\n\n        self._settings_obj._probability_two_random_records_match = prob\n\n        reciprocal_prob = \"Infinity\" if prob == 0 else f\"{1/prob:,.2f}\"\n        logger.info(\n            f\"Probability two random records match is estimated to be  {prob:.3g}.\\n\"\n            f\"This means that amongst all possible pairwise record comparisons, one in \"\n            f\"{reciprocal_prob} are expected to match.  \"\n            f\"With {num_total_comparisons:,.0f} total\"\n            \" possible comparisons, we expect a total of around \"\n            f\"{num_expected_matches:,.2f} matching pairs\"\n        )\n\n    def invalidate_cache(self):\n\"\"\"Invalidate the Splink cache.  Any previously-computed tables\n        will be recomputed.\n        This is useful, for example, if the input data tables have changed.\n        \"\"\"\n        # Before Splink executes a SQL command, it checks the cache to see\n        # whether a table already exists with the name of the output table\n\n        # This function has the effect of changing the names of the output tables\n        # to include a different unique id\n\n        # As a result, any previously cached tables will not be found\n        self._cache_uid = ascii_uid(8)\n\n        # As a result, any previously cached tables will not be found\n        self._intermediate_table_cache.invalidate_cache()\n\n        # Also drop any existing splink tables from the database\n        # Note, this is not actually necessary, it's just good housekeeping\n        self._delete_tables_created_by_splink_from_db()\n\n    def register_table_input_nodes_concat_with_tf(self, input_data, overwrite=False):\n\"\"\"Register a pre-computed version of the input_nodes_concat_with_tf table that\n        you want to re-use e.g. that you created in a previous run\n\n        This method allowed you to register this table in the Splink cache\n        so it will be used rather than Splink computing this table anew.\n\n        Args:\n            input_data: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            overwrite (bool): Overwrite the table in the underlying database if it\n                exists\n        \"\"\"\n\n        table_name_physical = \"__splink__df_concat_with_tf_\" + self._cache_uid\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        self._intermediate_table_cache[\"__splink__df_concat_with_tf\"] = splink_dataframe\n        return splink_dataframe\n\n    def register_table_predict(self, input_data, overwrite=False):\n        table_name_physical = \"__splink__df_predict_\" + self._cache_uid\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        self._intermediate_table_cache[\"__splink__df_predict\"] = splink_dataframe\n        return splink_dataframe\n\n    def register_term_frequency_lookup(self, input_data, col_name, overwrite=False):\n        input_col = InputColumn(col_name, settings_obj=self._settings_obj)\n        table_name_templated = colname_to_tf_tablename(input_col)\n        table_name_physical = f\"{table_name_templated}_{self._cache_uid}\"\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        self._intermediate_table_cache[table_name_templated] = splink_dataframe\n        return splink_dataframe\n\n    def register_labels_table(self, input_data, overwrite=False):\n        table_name_physical = \"__splink__df_labels_\" + ascii_uid(8)\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        return splink_dataframe\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.cluster_pairwise_predictions_at_threshold","title":"<code>cluster_pairwise_predictions_at_threshold(df_predict, threshold_match_probability=None, pairwise_formatting=False, filter_pairwise_format_for_clusters=True)</code>","text":"<p>Clusters the pairwise match predictions that result from <code>linker.predict()</code> into groups of connected record using the connected components graph clustering algorithm</p> <p>Records with an estimated <code>match_probability</code> above <code>threshold_match_probability</code> are considered to be a match (i.e. they represent the same entity).</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The results of <code>linker.predict()</code></p> required <code>threshold_match_probability</code> <code>float</code> <p>Filter the pairwise match predictions to include only pairwise comparisons with a match_probability above this threshold. This dataframe is then fed into the clustering algorithm.</p> <code>None</code> <code>pairwise_formatting</code> <code>bool</code> <p>Whether to output the pairwise match predictions from linker.predict() with cluster IDs. If this is set to false, the output will be a list of all IDs, clustered into groups based on the desired match threshold.</p> <code>False</code> <code>filter_pairwise_format_for_clusters</code> <code>bool</code> <p>If pairwise formatting has been selected, whether to output all columns found within linker.predict(), or just return clusters.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold.</p> Source code in <code>splink/linker.py</code> <pre><code>def cluster_pairwise_predictions_at_threshold(\n    self,\n    df_predict: SplinkDataFrame,\n    threshold_match_probability: float = None,\n    pairwise_formatting: bool = False,\n    filter_pairwise_format_for_clusters: bool = True,\n) -&gt; SplinkDataFrame:\n\"\"\"Clusters the pairwise match predictions that result from `linker.predict()`\n    into groups of connected record using the connected components graph clustering\n    algorithm\n\n    Records with an estimated `match_probability` above\n    `threshold_match_probability` are considered to be a match (i.e. they represent\n    the same entity).\n\n    Args:\n        df_predict (SplinkDataFrame): The results of `linker.predict()`\n        threshold_match_probability (float): Filter the pairwise match predictions\n            to include only pairwise comparisons with a match_probability above this\n            threshold. This dataframe is then fed into the clustering\n            algorithm.\n        pairwise_formatting (bool): Whether to output the pairwise match predictions\n            from linker.predict() with cluster IDs.\n            If this is set to false, the output will be a list of all IDs, clustered\n            into groups based on the desired match threshold.\n        filter_pairwise_format_for_clusters (bool): If pairwise formatting has been\n            selected, whether to output all columns found within linker.predict(),\n            or just return clusters.\n\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered\n            into groups based on the desired match threshold.\n\n    \"\"\"\n\n    # Feeding in df_predict forces materiailisation, if it exists in your database\n    concat_with_tf = self._initialise_df_concat_with_tf(df_predict)\n\n    edges_table = _cc_create_unique_id_cols(\n        self,\n        concat_with_tf.physical_name,\n        df_predict.physical_name,\n        threshold_match_probability,\n    )\n\n    cc = solve_connected_components(\n        self,\n        edges_table,\n        df_predict,\n        concat_with_tf,\n        pairwise_formatting,\n        filter_pairwise_format_for_clusters,\n    )\n\n    return cc\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.compare_two_records","title":"<code>compare_two_records(record_1, record_2)</code>","text":"<p>Use the linkage model to compare and score a pairwise record comparison based on the two input records provided</p> <p>Parameters:</p> Name Type Description Default <code>record_1</code> <code>dict</code> <p>dictionary representing the first record.  Columns names and data types must be the same as the columns in the settings object</p> required <code>record_2</code> <code>dict</code> <p>dictionary representing the second record.  Columns names and data types must be the same as the columns in the settings object</p> required <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.compare_two_records(record_left, record_right)\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Pairwise comparison with scored prediction</p> Source code in <code>splink/linker.py</code> <pre><code>def compare_two_records(self, record_1: dict, record_2: dict):\n\"\"\"Use the linkage model to compare and score a pairwise record comparison\n    based on the two input records provided\n\n    Args:\n        record_1 (dict): dictionary representing the first record.  Columns names\n            and data types must be the same as the columns in the settings object\n        record_2 (dict): dictionary representing the second record.  Columns names\n            and data types must be the same as the columns in the settings object\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        linker.compare_two_records(record_left, record_right)\n        ```\n\n    Returns:\n        SplinkDataFrame: Pairwise comparison with scored prediction\n    \"\"\"\n    original_blocking_rules = (\n        self._settings_obj._blocking_rules_to_generate_predictions\n    )\n    original_link_type = self._settings_obj._link_type\n\n    self._compare_two_records_mode = True\n    self._settings_obj._blocking_rules_to_generate_predictions = []\n\n    uid = ascii_uid(8)\n    df_records_left = self.register_table(\n        [record_1], f\"__splink__compare_two_records_left_{uid}\", overwrite=True\n    )\n    df_records_left.templated_name = \"__splink__compare_two_records_left\"\n\n    df_records_right = self.register_table(\n        [record_2], f\"__splink__compare_two_records_right_{uid}\", overwrite=True\n    )\n    df_records_right.templated_name = \"__splink__compare_two_records_right\"\n\n    sql_join_tf = _join_tf_to_input_df_sql(self)\n\n    sql_join_tf = sql_join_tf.replace(\n        \"__splink__df_concat\", \"__splink__compare_two_records_left\"\n    )\n    self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_left_with_tf\")\n\n    sql_join_tf = sql_join_tf.replace(\n        \"__splink__compare_two_records_left\", \"__splink__compare_two_records_right\"\n    )\n\n    self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_right_with_tf\")\n\n    sql = block_using_rules_sql(self)\n    self._enqueue_sql(sql, \"__splink__df_blocked\")\n\n    sql = compute_comparison_vector_values_sql(self._settings_obj)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n    sqls = predict_from_comparison_vectors_sqls(\n        self._settings_obj,\n        sql_infinity_expression=self._infinity_expression,\n    )\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    predictions = self._execute_sql_pipeline(\n        [df_records_left, df_records_right], use_cache=False\n    )\n\n    self._settings_obj._blocking_rules_to_generate_predictions = (\n        original_blocking_rules\n    )\n    self._settings_obj._link_type = original_link_type\n    self._compare_two_records_mode = False\n\n    return predictions\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.compute_tf_table","title":"<code>compute_tf_table(column_name)</code>","text":"<p>Compute a term frequency table for a given column and persist to the database</p> <p>This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time</p> <p>Examples:</p> DuckDBSpark <p>Real time linkage <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.compute_tf_table(\"surname\")\nlinker.compare_two_records(record_left, record_right)\n</code></pre> Pre-computed term frequency tables <pre><code>linker = DuckDBLinker(df)\ndf_first_name_tf = linker.compute_tf_table(\"first_name\")\ndf_first_name_tf.write.parquet(\"folder/first_name_tf\")\n&gt;&gt;&gt;\n# On subsequent data linking job, read this table rather than recompute\ndf_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\ndf_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n</code></pre></p> <p>Real time linkage <pre><code>linker = SparkLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.compute_tf_table(\"surname\")\nlinker.compare_two_records(record_left, record_right)\n</code></pre> Pre-computed term frequency tables <pre><code>linker = SparkLinker(df)\ndf_first_name_tf = linker.compute_tf_table(\"first_name\")\ndf_first_name_tf.write.parquet(\"folder/first_name_tf\")\n&gt;&gt;&gt;\n# On subsequent data linking job, read this table rather than recompute\ndf_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\ndf_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <code>str</code> <p>The column name in the input table</p> required <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>The resultant table as a splink data frame</p> Source code in <code>splink/linker.py</code> <pre><code>def compute_tf_table(self, column_name: str) -&gt; SplinkDataFrame:\n\"\"\"Compute a term frequency table for a given column and persist to the database\n\n    This method is useful if you want to pre-compute term frequency tables e.g.\n    so that real time linkage executes faster, or so that you can estimate\n    various models without having to recompute term frequency tables each time\n\n    Examples:\n        === \"DuckDB\"\n            Real time linkage\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compute_tf_table(\"surname\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n            Pre-computed term frequency tables\n            ```py\n            linker = DuckDBLinker(df)\n            df_first_name_tf = linker.compute_tf_table(\"first_name\")\n            df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n            &gt;&gt;&gt;\n            # On subsequent data linking job, read this table rather than recompute\n            df_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\n            df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n            ```\n        === \"Spark\"\n            Real time linkage\n            ```py\n            linker = SparkLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compute_tf_table(\"surname\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n            Pre-computed term frequency tables\n            ```py\n            linker = SparkLinker(df)\n            df_first_name_tf = linker.compute_tf_table(\"first_name\")\n            df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n            &gt;&gt;&gt;\n            # On subsequent data linking job, read this table rather than recompute\n            df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\n            df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n            ```\n\n    Args:\n        column_name (str): The column name in the input table\n\n    Returns:\n        SplinkDataFrame: The resultant table as a splink data frame\n    \"\"\"\n\n    input_col = InputColumn(column_name, settings_obj=self._settings_obj)\n    tf_tablename = colname_to_tf_tablename(input_col)\n    cache = self._intermediate_table_cache\n    concat_tf_tables = [\n        remove_quotes_from_identifiers(tf_col.input_name_as_tree).sql()\n        for tf_col in self._settings_obj._term_frequency_columns\n    ]\n\n    if tf_tablename in cache:\n        tf_df = cache[tf_tablename]\n    elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables:\n        self._pipeline.reset()\n        # If our df_concat_with_tf table already exists, use backwards inference to\n        # find a given tf table\n        colname = InputColumn(column_name)\n        sql = term_frequencies_from_concat_with_tf(colname)\n        self._enqueue_sql(sql, colname_to_tf_tablename(colname))\n        tf_df = self._execute_sql_pipeline(\n            [cache[\"__splink__df_concat_with_tf\"]], materialise_as_hash=True\n        )\n        self._intermediate_table_cache[tf_tablename] = tf_df\n    else:\n        # Clear the pipeline if we are materialising\n        self._pipeline.reset()\n        df_concat = self._initialise_df_concat()\n        input_dfs = []\n        if df_concat:\n            input_dfs.append(df_concat)\n        sql = term_frequencies_for_single_column_sql(input_col)\n        self._enqueue_sql(sql, tf_tablename)\n        tf_df = self._execute_sql_pipeline(input_dfs, materialise_as_hash=True)\n        self._intermediate_table_cache[tf_tablename] = tf_df\n\n    return tf_df\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.deterministic_link","title":"<code>deterministic_link()</code>","text":"<p>Uses the blocking rules specified by <code>blocking_rules_to_generate_predictions</code> in the settings dictionary to generate pairwise record comparisons.</p> <p>For deterministic linkage, this should be a list of blocking rules which are strict enough to generate only true links.</p> <p>Deterministic linkage, however, is likely to result in missed links (false negatives).</p> <p>Examples:</p> DuckDB <pre><code>from splink.duckdb.duckdb_linker import DuckDBLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = DuckDBLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> Spark <pre><code>from splink.spark.spark_linker import SparkLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = SparkLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> Athena <pre><code>from splink.athena.athena_linker import AthenaLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = AthenaLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> SQLite <pre><code>from splink.sqlite.sqlite_linker import SQLiteLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": []\n}\n&gt;&gt;&gt;\nlinker = SQLiteLinker(df, settings)\ndf = linker.deterministic_link()\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>A SplinkDataFrame of the pairwise comparisons.  This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data.</p> Source code in <code>splink/linker.py</code> <pre><code>def deterministic_link(self) -&gt; SplinkDataFrame:\n\"\"\"Uses the blocking rules specified by\n    `blocking_rules_to_generate_predictions` in the settings dictionary to\n    generate pairwise record comparisons.\n\n    For deterministic linkage, this should be a list of blocking rules which\n    are strict enough to generate only true links.\n\n    Deterministic linkage, however, is likely to result in missed links\n    (false negatives).\n\n    Examples:\n        === \"DuckDB\"\n        ```py\n        from splink.duckdb.duckdb_linker import DuckDBLinker\n\n        settings = {\n            \"link_type\": \"dedupe_only\",\n            \"blocking_rules_to_generate_predictions\": [\n                \"l.first_name = r.first_name\",\n                \"l.surname = r.surname\",\n            ],\n            \"comparisons\": []\n        }\n        &gt;&gt;&gt;\n        linker = DuckDBLinker(df, settings)\n        df = linker.deterministic_link()\n        ```\n        === \"Spark\"\n        ```py\n        from splink.spark.spark_linker import SparkLinker\n\n        settings = {\n            \"link_type\": \"dedupe_only\",\n            \"blocking_rules_to_generate_predictions\": [\n                \"l.first_name = r.first_name\",\n                \"l.surname = r.surname\",\n            ],\n            \"comparisons\": []\n        }\n        &gt;&gt;&gt;\n        linker = SparkLinker(df, settings)\n        df = linker.deterministic_link()\n        ```\n        === \"Athena\"\n        ```py\n        from splink.athena.athena_linker import AthenaLinker\n\n        settings = {\n            \"link_type\": \"dedupe_only\",\n            \"blocking_rules_to_generate_predictions\": [\n                \"l.first_name = r.first_name\",\n                \"l.surname = r.surname\",\n            ],\n            \"comparisons\": []\n        }\n        &gt;&gt;&gt;\n        linker = AthenaLinker(df, settings)\n        df = linker.deterministic_link()\n        ```\n        === \"SQLite\"\n        ```py\n        from splink.sqlite.sqlite_linker import SQLiteLinker\n\n        settings = {\n            \"link_type\": \"dedupe_only\",\n            \"blocking_rules_to_generate_predictions\": [\n                \"l.first_name = r.first_name\",\n                \"l.surname = r.surname\",\n            ],\n            \"comparisons\": []\n        }\n        &gt;&gt;&gt;\n        linker = SQLiteLinker(df, settings)\n        df = linker.deterministic_link()\n        ```\n\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n            represents a table materialised in the database. Methods on the\n            SplinkDataFrame allow you to access the underlying data.\n    \"\"\"\n\n    # Allows clustering during a deterministic linkage.\n    # This is used in `cluster_pairwise_predictions_at_threshold`\n    # to set the cluster threshold to 1\n    self._deterministic_link_mode = True\n\n    concat_with_tf = self._initialise_df_concat_with_tf()\n    sql = block_using_rules_sql(self)\n    self._enqueue_sql(sql, \"__splink__df_blocked\")\n    return self._execute_sql_pipeline([concat_with_tf])\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.find_matches_to_new_records","title":"<code>find_matches_to_new_records(records_or_tablename, blocking_rules=[], match_weight_threshold=-4)</code>","text":"<p>Given one or more records, find records in the input dataset(s) which match and return in order of the splink prediction score.</p> <p>This effectively provides a way of searching the input datasets for given record(s)</p> <p>Parameters:</p> Name Type Description Default <code>records_or_tablename</code> <code>List[dict]</code> <p>Input search record(s) as list of dict, or a table registered to the database.</p> required <code>blocking_rules</code> <code>list</code> <p>Blocking rules to select which records to find and score. If [], do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to [].</p> <code>[]</code> <code>match_weight_threshold</code> <code>int</code> <p>Return matches with a match weight above this threshold. Defaults to -4.</p> <code>-4</code> <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\n# Pre-compute tf tables for any tables with\n# term frequency adjustments\nlinker.compute_tf_table(\"first_name\")\nrecord = {'unique_id': 1,\n    'first_name': \"John\",\n    'surname': \"Smith\",\n    'dob': \"1971-05-24\",\n    'city': \"London\",\n    'email': \"john@smith.net\"\n    }\ndf = linker.find_matches_to_new_records([record], blocking_rules=[])\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>The pairwise comparisons.</p> Source code in <code>splink/linker.py</code> <pre><code>def find_matches_to_new_records(\n    self,\n    records_or_tablename,\n    blocking_rules=[],\n    match_weight_threshold=-4,\n) -&gt; SplinkDataFrame:\n\"\"\"Given one or more records, find records in the input dataset(s) which match\n    and return in order of the splink prediction score.\n\n    This effectively provides a way of searching the input datasets\n    for given record(s)\n\n    Args:\n        records_or_tablename (List[dict]): Input search record(s) as list of dict,\n            or a table registered to the database.\n        blocking_rules (list, optional): Blocking rules to select\n            which records to find and score. If [], do not use a blocking\n            rule - meaning the input records will be compared to all records\n            provided to the linker when it was instantiated. Defaults to [].\n        match_weight_threshold (int, optional): Return matches with a match weight\n            above this threshold. Defaults to -4.\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        # Pre-compute tf tables for any tables with\n        # term frequency adjustments\n        linker.compute_tf_table(\"first_name\")\n        record = {'unique_id': 1,\n            'first_name': \"John\",\n            'surname': \"Smith\",\n            'dob': \"1971-05-24\",\n            'city': \"London\",\n            'email': \"john@smith.net\"\n            }\n        df = linker.find_matches_to_new_records([record], blocking_rules=[])\n        ```\n\n    Returns:\n        SplinkDataFrame: The pairwise comparisons.\n    \"\"\"\n\n    original_blocking_rules = (\n        self._settings_obj._blocking_rules_to_generate_predictions\n    )\n    original_link_type = self._settings_obj._link_type\n\n    if not isinstance(records_or_tablename, str):\n        uid = ascii_uid(8)\n        self.register_table(\n            records_or_tablename, f\"__splink__df_new_records_{uid}\", overwrite=True\n        )\n        new_records_tablename = f\"__splink__df_new_records_{uid}\"\n    else:\n        new_records_tablename = records_or_tablename\n\n    cache = self._intermediate_table_cache\n    input_dfs = []\n    # If our df_concat_with_tf table already exists, use backwards inference to\n    # find all underlying term frequency tables.\n    if \"__splink__df_concat_with_tf\" in cache:\n        concat_with_tf = cache[\"__splink__df_concat_with_tf\"]\n        tf_tables = compute_term_frequencies_from_concat_with_tf(self)\n        # This queues up our tf tables, rather materialising them\n        for tf in tf_tables:\n            # if tf is a SplinkDataFrame, then the table already exists\n            if isinstance(tf, SplinkDataFrame):\n                input_dfs.append(tf)\n            else:\n                self._enqueue_sql(tf[\"sql\"], tf[\"output_table_name\"])\n    else:\n        # This queues up our cols_with_tf and df_concat_with_tf tables.\n        concat_with_tf = self._initialise_df_concat_with_tf(materialise=False)\n\n    if concat_with_tf:\n        input_dfs.append(concat_with_tf)\n\n    rules = []\n    for r in blocking_rules:\n        br_as_obj = BlockingRule(r) if not isinstance(r, BlockingRule) else r\n        br_as_obj.preceding_rules = rules.copy()\n        rules.append(br_as_obj)\n    blocking_rules = rules\n\n    self._settings_obj._blocking_rules_to_generate_predictions = blocking_rules\n\n    self._settings_obj._link_type = \"link_only_find_matches_to_new_records\"\n    self._find_new_matches_mode = True\n\n    sql = _join_tf_to_input_df_sql(self)\n    sql = sql.replace(\"__splink__df_concat\", new_records_tablename)\n    self._enqueue_sql(sql, \"__splink__df_new_records_with_tf\")\n\n    sql = block_using_rules_sql(self)\n    self._enqueue_sql(sql, \"__splink__df_blocked\")\n\n    sql = compute_comparison_vector_values_sql(self._settings_obj)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n    sqls = predict_from_comparison_vectors_sqls(\n        self._settings_obj,\n        sql_infinity_expression=self._infinity_expression,\n    )\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    sql = f\"\"\"\n    select * from __splink__df_predict\n    where match_weight &gt; {match_weight_threshold}\n    \"\"\"\n\n    self._enqueue_sql(sql, \"__splink__find_matches_predictions\")\n\n    predictions = self._execute_sql_pipeline(\n        input_dataframes=input_dfs, use_cache=False\n    )\n\n    self._settings_obj._blocking_rules_to_generate_predictions = (\n        original_blocking_rules\n    )\n    self._settings_obj._link_type = original_link_type\n    self._find_new_matches_mode = False\n\n    return predictions\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.load_settings","title":"<code>load_settings(settings_dict)</code>","text":"<p>Initialise settings for the linker.  To be used if settings were not passed to the linker on creation. This can either be in the form of a settings dictionary or a filepath to a json file containing a valid settings dictionary.</p> <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.profile_columns([\"first_name\", \"surname\"])\nlinker.load_settings(settings_dict)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>settings_dict</code> <code>dict | str | Path</code> <p>A Splink settings dictionary or the path to your settings json file.</p> required Source code in <code>splink/linker.py</code> <pre><code>def load_settings(self, settings_dict: dict | str | Path):\n\"\"\"Initialise settings for the linker.  To be used if settings were\n    not passed to the linker on creation. This can either be in the form\n    of a settings dictionary or a filepath to a json file containing a\n    valid settings dictionary.\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.profile_columns([\"first_name\", \"surname\"])\n        linker.load_settings(settings_dict)\n        ```\n\n    Args:\n        settings_dict (dict | str | Path): A Splink settings dictionary or\n            the path to your settings json file.\n    \"\"\"\n\n    if not isinstance(settings_dict, dict):\n        p = Path(settings_dict)\n        if not p.is_file():  # check if it's a valid file/filepath\n            raise FileNotFoundError(\n                \"The filepath you have provided is either not a valid file \"\n                \"or doesn't exist along the path provided.\"\n            )\n        settings_dict = json.loads(p.read_text())\n\n    # Store the cache ID so it can be reloaded after cache invalidation\n    cache_id = self._cache_uid\n    # So we don't run into any issues with generated tables having\n    # invalid columns as settings have been tweaked, invalidate\n    # the cache and allow these tables to be recomputed.\n\n    # This is less efficient, but triggers infrequently and ensures we don't\n    # run into issues where the defaults used conflict with the actual values\n    # supplied in settings.\n\n    # This is particularly relevant with `source_dataset`, which appears within\n    # concat_with_tf.\n    self.invalidate_cache()\n\n    # If a uid already exists in your settings object, prioritise this\n    settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", cache_id)\n    settings_dict[\"sql_dialect\"] = settings_dict.get(\n        \"sql_dialect\", self._sql_dialect\n    )\n    self._settings_dict = settings_dict\n    self._settings_obj_ = Settings(settings_dict)\n    self._validate_input_dfs()\n    self._validate_dialect()\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.load_model","title":"<code>load_model(model_path)</code>","text":"<p>Load a pre-defined model from a json file into the linker. This is intended to be used with the output of <code>save_model_to_json()</code>.</p> <p>Examples:</p> <pre><code>linker.load_model(\"my_settings.json\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>Path</code> <p>A path to your model settings json file.</p> required Source code in <code>splink/linker.py</code> <pre><code>def load_model(self, model_path: Path):\n\"\"\"\n    Load a pre-defined model from a json file into the linker.\n    This is intended to be used with the output of\n    `save_model_to_json()`.\n\n    Examples:\n        ```py\n        linker.load_model(\"my_settings.json\")\n        ```\n\n    Args:\n        model_path (Path): A path to your model settings json file.\n    \"\"\"\n\n    return self.load_settings(model_path)\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.load_settings_from_json","title":"<code>load_settings_from_json(in_path)</code>","text":"<p>This method is now deprecated. Please use <code>load_settings</code> when loading existing settings or <code>load_model</code> when loading  a pre-trained model.</p> <p>Load settings from a <code>.json</code> file. This <code>.json</code> file would usually be the output of <code>linker.save_model_to_json()</code></p> <p>Examples:</p> <pre><code>linker.load_settings_from_json(\"my_settings.json\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>in_path</code> <code>str</code> <p>Path to settings json file</p> required Source code in <code>splink/linker.py</code> <pre><code>def load_settings_from_json(self, in_path: str | Path):\n\"\"\"*This method is now deprecated. Please use `load_settings`\n    when loading existing settings or `load_model` when loading\n     a pre-trained model.*\n\n    Load settings from a `.json` file.\n    This `.json` file would usually be the output of\n    `linker.save_model_to_json()`\n    Examples:\n        ```py\n        linker.load_settings_from_json(\"my_settings.json\")\n        ```\n    Args:\n        in_path (str): Path to settings json file\n    \"\"\"\n    self.load_settings(in_path)\n\n    warnings.warn(\n        \"`load_settings_from_json` is deprecated. We advise you use \"\n        \"`linker.load_settings()` when loading in your settings or a previously \"\n        \"trained model.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerpred.html#splink.linker.Linker.predict","title":"<code>predict(threshold_match_probability=None, threshold_match_weight=None, materialise_after_computing_term_frequencies=True)</code>","text":"<p>Create a dataframe of scored pairwise comparisons using the parameters of the linkage model.</p> <p>Uses the blocking rules specified in the <code>blocking_rules_to_generate_predictions</code> of the settings dictionary to generate the pairwise comparisons.</p> <p>Parameters:</p> Name Type Description Default <code>threshold_match_probability</code> <code>float</code> <p>If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None.</p> <code>None</code> <code>threshold_match_weight</code> <code>float</code> <p>If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None.</p> <code>None</code> <code>materialise_after_computing_term_frequencies</code> <code>bool</code> <p>If true, Splink will materialise the table containing the input nodes (rows) joined to any term frequencies which have been asked for in the settings object.  If False, this will be computed as part of one possibly gigantic CTE pipeline.   Defaults to True</p> <code>True</code> <p>Examples:</p> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\ndf = linker.predict(threshold_match_probability=0.95)\ndf.as_pandas_dataframe(limit=5)\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>A SplinkDataFrame of the pairwise comparisons.  This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data.</p> Source code in <code>splink/linker.py</code> <pre><code>def predict(\n    self,\n    threshold_match_probability: float = None,\n    threshold_match_weight: float = None,\n    materialise_after_computing_term_frequencies=True,\n) -&gt; SplinkDataFrame:\n\"\"\"Create a dataframe of scored pairwise comparisons using the parameters\n    of the linkage model.\n\n    Uses the blocking rules specified in the\n    `blocking_rules_to_generate_predictions` of the settings dictionary to\n    generate the pairwise comparisons.\n\n    Args:\n        threshold_match_probability (float, optional): If specified,\n            filter the results to include only pairwise comparisons with a\n            match_probability above this threshold. Defaults to None.\n        threshold_match_weight (float, optional): If specified,\n            filter the results to include only pairwise comparisons with a\n            match_weight above this threshold. Defaults to None.\n        materialise_after_computing_term_frequencies (bool): If true, Splink\n            will materialise the table containing the input nodes (rows)\n            joined to any term frequencies which have been asked\n            for in the settings object.  If False, this will be\n            computed as part of one possibly gigantic CTE\n            pipeline.   Defaults to True\n\n    Examples:\n        ```py\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        df = linker.predict(threshold_match_probability=0.95)\n        df.as_pandas_dataframe(limit=5)\n        ```\n    Returns:\n        SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n            represents a table materialised in the database. Methods on the\n            SplinkDataFrame allow you to access the underlying data.\n\n    \"\"\"\n\n    # If materialise_after_computing_term_frequencies=False and the user only\n    # calls predict, it runs as a single pipeline with no materialisation\n    # of anything.\n\n    # _initialise_df_concat_with_tf returns None if the table doesn't exist\n    # and only SQL is queued in this step.\n    nodes_with_tf = self._initialise_df_concat_with_tf(\n        materialise=materialise_after_computing_term_frequencies\n    )\n\n    input_dataframes = []\n    if nodes_with_tf:\n        input_dataframes.append(nodes_with_tf)\n\n    sql = block_using_rules_sql(self)\n    self._enqueue_sql(sql, \"__splink__df_blocked\")\n\n    repartition_after_blocking = getattr(self, \"repartition_after_blocking\", False)\n\n    # repartition after blocking only exists on the SparkLinker\n    if repartition_after_blocking:\n        df_blocked = self._execute_sql_pipeline(input_dataframes)\n        input_dataframes.append(df_blocked)\n\n    sql = compute_comparison_vector_values_sql(self._settings_obj)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n    sqls = predict_from_comparison_vectors_sqls(\n        self._settings_obj,\n        threshold_match_probability,\n        threshold_match_weight,\n        sql_infinity_expression=self._infinity_expression,\n    )\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    predictions = self._execute_sql_pipeline(input_dataframes)\n    self._predict_warning()\n    return predictions\n</code></pre>","tags":["API","Prediction"]},{"location":"linkerqa.html","title":"Documentation for <code>Linker</code> object methods related to QA","text":"<p>The Linker object manages the data linkage process and holds the data linkage model.</p> <p>Most of Splink's functionality can  be accessed by calling methods (functions) on the linker, such as <code>linker.predict()</code>, <code>linker.profile_columns()</code> etc.</p> <p>The Linker class is intended for subclassing for specific backends, e.g. a <code>DuckDBLinker</code>.</p> Source code in <code>splink/linker.py</code> <pre><code>class Linker:\n\"\"\"The Linker object manages the data linkage process and holds the data linkage\n    model.\n\n    Most of Splink's functionality can  be accessed by calling methods (functions)\n    on the linker, such as `linker.predict()`, `linker.profile_columns()` etc.\n\n    The Linker class is intended for subclassing for specific backends, e.g.\n    a `DuckDBLinker`.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_table_or_tables: str | list,\n        settings_dict: dict | Path,\n        accepted_df_dtypes,\n        set_up_basic_logging: bool = True,\n        input_table_aliases: str | list = None,\n    ):\n\"\"\"Initialise the linker object, which manages the data linkage process and\n        holds the data linkage model.\n\n        Examples:\n            === \"DuckDB\"\n                Dedupe\n                ```py\n                df = pd.read_csv(\"data_to_dedupe.csv\")\n                linker = DuckDBLinker(df, settings_dict)\n                ```\n                Link\n                ```py\n                df_1 = pd.read_parquet(\"table_1/\")\n                df_2 = pd.read_parquet(\"table_2/\")\n                linker = DuckDBLinker(\n                    [df_1, df_2],\n                    settings_dict,\n                    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                    )\n                ```\n                Dedupe with a pre-trained model read from a json file\n                ```py\n                df = pd.read_csv(\"data_to_dedupe.csv\")\n                linker = DuckDBLinker(df, \"model.json\")\n                ```\n            === \"Spark\"\n                Dedupe\n                ```py\n                df = spark.read.csv(\"data_to_dedupe.csv\")\n                linker = SparkLinker(df, settings_dict)\n                ```\n                Link\n                ```py\n                df_1 = spark.read.parquet(\"table_1/\")\n                df_2 = spark.read.parquet(\"table_2/\")\n                linker = SparkLinker(\n                    [df_1, df_2],\n                    settings_dict,\n                    input_table_aliases=[\"customers\", \"contact_center_callers\"]\n                    )\n                ```\n                Dedupe with a pre-trained model read from a json file\n                ```py\n                df = spark.read.csv(\"data_to_dedupe.csv\")\n                linker = SparkLinker(df, \"model.json\")\n                ```\n\n        Args:\n            input_table_or_tables (Union[str, list]): Input data into the linkage model.\n                Either a single string (the name of a table in a database) for\n                deduplication jobs, or a list of strings  (the name of tables in a\n                database) for link_only or link_and_dedupe.  For some linkers, such as\n                the DuckDBLinker and the SparkLinker, it's also possible to pass in\n                dataframes (Pandas and Spark respectively) rather than strings.\n            settings_dict (dict | Path, optional): A Splink settings dictionary, or a\n                path to a json defining a settingss dictionary or pre-trained model.\n                If not provided when the object is created, can later be added using\n                `linker.load_settings()` or `linker.load_model()` Defaults to None.\n            set_up_basic_logging (bool, optional): If true, sets ups up basic logging\n                so that Splink sends messages at INFO level to stdout. Defaults to True.\n            input_table_aliases (Union[str, list], optional): Labels assigned to\n                input tables in Splink outputs.  If the names of the tables in the\n                input database are long or unspecific, this argument can be used\n                to attach more easily readable/interpretable names. Defaults to None.\n        \"\"\"\n\n        if set_up_basic_logging:\n            logging.basicConfig(\n                format=\"%(message)s\",\n            )\n            splink_logger = logging.getLogger(\"splink\")\n            splink_logger.setLevel(logging.INFO)\n\n        self._pipeline = SQLPipeline()\n\n        self._names_of_tables_created_by_splink: set = set()\n        self._intermediate_table_cache: dict = CacheDictWithLogging()\n\n        if not isinstance(settings_dict, (dict, type(None))):\n            # Run if you've entered a filepath\n            # feed it a blank settings dictionary\n            self._setup_settings_objs(None)\n            self.load_settings(settings_dict)\n        else:\n            settings_dict = deepcopy(settings_dict)\n            self._setup_settings_objs(settings_dict)\n\n        homogenised_tables, homogenised_aliases = self._register_input_tables(\n            input_table_or_tables,\n            input_table_aliases,\n            accepted_df_dtypes,\n        )\n\n        self._input_tables_dict = self._get_input_tables_dict(\n            homogenised_tables, homogenised_aliases\n        )\n\n        self._validate_input_dfs()\n        self._em_training_sessions = []\n\n        self._find_new_matches_mode = False\n        self._train_u_using_random_sample_mode = False\n        self._compare_two_records_mode = False\n        self._self_link_mode = False\n        self._analyse_blocking_mode = False\n        self._deterministic_link_mode = False\n\n        self.debug_mode = False\n\n    @property\n    def _cache_uid(self):\n        if self._settings_dict:\n            return self._settings_obj._cache_uid\n        else:\n            return self._cache_uid_no_settings\n\n    @_cache_uid.setter\n    def _cache_uid(self, value):\n        if self._settings_dict:\n            self._settings_obj._cache_uid = value\n        else:\n            self._cache_uid_no_settings = value\n\n    @property\n    def _settings_obj(self) -&gt; Settings:\n        if self._settings_obj_ is None:\n            raise ValueError(\n                \"You did not provide a settings dictionary when you \"\n                \"created the linker.  To continue, you need to provide a settings \"\n                \"dictionary using the `load_settings()` method on your linker \"\n                \"object. i.e. linker.load_settings(settings_dict)\"\n            )\n        return self._settings_obj_\n\n    @property\n    def _input_tablename_l(self):\n        if self._find_new_matches_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._self_link_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._compare_two_records_mode:\n            return \"__splink__compare_two_records_left_with_tf\"\n\n        if self._train_u_using_random_sample_mode:\n            return \"__splink__df_concat_with_tf_sample\"\n\n        if self._analyse_blocking_mode:\n            return \"__splink__df_concat\"\n\n        if self._two_dataset_link_only:\n            return \"__splink__df_concat_with_tf_left\"\n\n        return \"__splink__df_concat_with_tf\"\n\n    @property\n    def _input_tablename_r(self):\n        if self._find_new_matches_mode:\n            return \"__splink__df_new_records_with_tf\"\n\n        if self._self_link_mode:\n            return \"__splink__df_concat_with_tf\"\n\n        if self._compare_two_records_mode:\n            return \"__splink__compare_two_records_right_with_tf\"\n\n        if self._train_u_using_random_sample_mode:\n            return \"__splink__df_concat_with_tf_sample\"\n\n        if self._analyse_blocking_mode:\n            return \"__splink__df_concat\"\n\n        if self._two_dataset_link_only:\n            return \"__splink_df_concat_with_tf_right\"\n        return \"__splink__df_concat_with_tf\"\n\n    @property\n    def _source_dataset_column_name(self):\n        if self._settings_obj_ is None:\n            return None\n\n        # Used throughout the scripts to feed our SQL\n        if self._settings_obj._source_dataset_column_name_is_required:\n            df_obj = next(iter(self._input_tables_dict.values()))\n            columns = df_obj.columns_escaped\n\n            input_column, src_ds_col = self._settings_obj_._source_dataset_col\n            return \"__splink_source_dataset\" if src_ds_col in columns else input_column\n        else:\n            return None\n\n    @property\n    def _two_dataset_link_only(self):\n        # Two dataset link only join is a special case where an inner join of the\n        # two datasets is much more efficient than self-joining the vertically\n        # concatenation of all input datasets\n        if self._find_new_matches_mode:\n            return True\n\n        if self._compare_two_records_mode:\n            return True\n\n        # in u-train sample mode we are joining the concatenated table mixing\n        # both data sets - hence if we inner join on True we will end up with\n        # samples which both originate from the same dataset\n        if self._train_u_using_random_sample_mode:\n            return False\n\n        if self._analyse_blocking_mode:\n            return False\n\n        if (\n            len(self._input_tables_dict) == 2\n            and self._settings_obj._link_type == \"link_only\"\n        ):\n            return True\n        else:\n            return False\n\n    @property\n    def _sql_dialect(self):\n        if self._sql_dialect_ is None:\n            raise NotImplementedError(\n                f\"No SQL dialect set on object of type {type(self)}. \"\n                \"Did you make sure to create a dialect-specific Linker?\"\n            )\n        return self._sql_dialect_\n\n    @property\n    def _infinity_expression(self):\n        raise NotImplementedError(\n            f\"infinity sql expression not available for {type(self)}\"\n        )\n\n    @property\n    def _verify_link_only_job(self):\n        cache = self._intermediate_table_cache\n        if \"__splink__df_concat_with_tf\" not in cache:\n            return\n\n        if self._settings_obj._link_type == \"link_only\":\n            # if input datasets &gt; 1 then skip\n            if len(self._input_tables_dict) &gt; 1:\n                return\n\n            # else, check if source dataset column is populated...\n            src_ds = self._source_dataset_column_name\n            if src_ds == \"__splink_source_dataset\":\n                _, src_ds = self._settings_obj_._source_dataset_col\n\n            sql = find_unique_source_dataset(src_ds)\n            self._enqueue_sql(sql, \"source_ds_distinct\")\n            src_ds_distinct = self._execute_sql_pipeline(\n                [cache[\"__splink__df_concat_with_tf\"]]\n            )\n            if len(src_ds_distinct.as_record_dict()) == 1:\n                raise SplinkException(\n                    \"if `link_type` is `link_only`, it should have at least two \"\n                    \"input dataframes, or one dataframe with a `source_dataset` \"\n                    \"column outlining which dataset each record belongs to.\"\n                )\n\n    def _register_input_tables(self, input_tables, input_aliases, accepted_df_dtypes):\n        # 'homogenised' means all entries are strings representing tables\n        homogenised_tables = []\n        homogenised_aliases = []\n        accepted_df_dtypes = ensure_is_tuple(accepted_df_dtypes)\n\n        existing_tables = []\n        for alias in input_aliases:\n            # Check if alias is a string (indicating a table name) and that it is not\n            # a file path.\n            if not isinstance(alias, str) or re.match(pattern=r\".*\", string=alias):\n                continue\n            exists = self._table_exists_in_database(alias)\n            if exists:\n                existing_tables.append(f\"'{alias}'\")\n        if existing_tables:\n            input_tables = \", \".join(existing_tables)\n            raise ValueError(\n                f\"Table(s): {input_tables} already exists in database. \"\n                \"Please remove or rename it/them before retrying\"\n            )\n\n        for i, (table, alias) in enumerate(zip(input_tables, input_aliases)):\n            if isinstance(alias, accepted_df_dtypes):\n                alias = f\"__splink__input_table_{i}\"\n\n            if isinstance(table, accepted_df_dtypes):\n                self._table_registration(table, alias)\n                table = alias\n\n            homogenised_tables.append(table)\n            homogenised_aliases.append(alias)\n\n        return homogenised_tables, homogenised_aliases\n\n    def _setup_settings_objs(self, settings_dict):\n        # Setup the linker class's required settings\n        self._settings_dict = settings_dict\n\n        # if settings_dict is passed, set sql_dialect on it if missing, and make sure\n        # incompatible dialect not passed\n        if settings_dict is not None and settings_dict.get(\"sql_dialect\", None) is None:\n            settings_dict[\"sql_dialect\"] = self._sql_dialect\n\n        if settings_dict is None:\n            self._cache_uid_no_settings = ascii_uid(8)\n        else:\n            uid = settings_dict.get(\"linker_uid\", ascii_uid(8))\n            settings_dict[\"linker_uid\"] = uid\n\n        if settings_dict is None:\n            self._settings_obj_ = None\n        else:\n            self._settings_obj_ = Settings(settings_dict)\n\n            self._validate_dialect()\n\n    def _initialise_df_concat(self, materialise=False):\n        cache = self._intermediate_table_cache\n        concat_df = None\n        if \"__splink__df_concat\" in cache:\n            concat_df = cache[\"__splink__df_concat\"]\n        elif \"__splink__df_concat_with_tf\" in cache:\n            concat_df = cache[\"__splink__df_concat_with_tf\"]\n            concat_df.templated_name = \"__splink__df_concat\"\n        else:\n            if materialise:\n                # Clear the pipeline if we are materialising\n                # There's no reason not to do this, since when\n                # we execute the pipeline, it'll get cleared anyway\n                self._pipeline.reset()\n            sql = vertically_concatenate_sql(self)\n            self._enqueue_sql(sql, \"__splink__df_concat\")\n            if materialise:\n                concat_df = self._execute_sql_pipeline()\n                cache[\"__splink__df_concat\"] = concat_df\n\n        return concat_df\n\n    def _initialise_df_concat_with_tf(self, materialise=True):\n        cache = self._intermediate_table_cache\n        nodes_with_tf = None\n        if \"__splink__df_concat_with_tf\" in cache:\n            nodes_with_tf = cache[\"__splink__df_concat_with_tf\"]\n\n        else:\n            if materialise:\n                # Clear the pipeline if we are materialising\n                # There's no reason not to do this, since when\n                # we execute the pipeline, it'll get cleared anyway\n                self._pipeline.reset()\n\n            sql = vertically_concatenate_sql(self)\n            self._enqueue_sql(sql, \"__splink__df_concat\")\n\n            sqls = compute_all_term_frequencies_sqls(self)\n            for sql in sqls:\n                self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n            if materialise:\n                nodes_with_tf = self._execute_sql_pipeline()\n                cache[\"__splink__df_concat_with_tf\"] = nodes_with_tf\n\n        # verify the link job\n        if self._settings_obj_ is not None:\n            self._verify_link_only_job\n\n        return nodes_with_tf\n\n    def _table_to_splink_dataframe(\n        self, templated_name, physical_name\n    ) -&gt; SplinkDataFrame:\n\"\"\"Create a SplinkDataframe from a table in the underlying database called\n        `physical_name`.\n\n        Associate a `templated_name` with this table, which signifies the purpose\n        or 'meaning' of this table to splink. (e.g. `__splink__df_blocked`)\n\n        Args:\n            templated_name (str): The purpose of the table to Splink\n            physical_name (str): The name of the table in the underlying databse\n        \"\"\"\n        raise NotImplementedError(\n            \"_table_to_splink_dataframe not implemented on this linker\"\n        )\n\n    def _enqueue_sql(self, sql, output_table_name):\n\"\"\"Add sql to the current pipeline, but do not execute the pipeline.\"\"\"\n        self._pipeline.enqueue_sql(sql, output_table_name)\n\n    def _execute_sql_pipeline(\n        self,\n        input_dataframes: list[SplinkDataFrame] = [],\n        materialise_as_hash=True,\n        use_cache=True,\n    ) -&gt; SplinkDataFrame:\n\"\"\"Execute the SQL queued in the current pipeline as a single statement\n        e.g. `with a as (), b as , c as (), select ... from c`, then execute the\n        pipeline, returning the resultant table as a SplinkDataFrame\n\n        Args:\n            input_dataframes (List[SplinkDataFrame], optional): A 'starting point' of\n                SplinkDataFrames if needed. Defaults to [].\n            materialise_as_hash (bool, optional): If true, the output tablename will end\n                in a unique identifer. Defaults to True.\n            use_cache (bool, optional): If true, look at whether the SQL pipeline has\n                been executed before, and if so, use the existing result. Defaults to\n                True.\n\n        Returns:\n            SplinkDataFrame: An abstraction representing the table created by the sql\n                pipeline\n        \"\"\"\n\n        if not self.debug_mode:\n            sql_gen = self._pipeline._generate_pipeline(input_dataframes)\n\n            output_tablename_templated = self._pipeline.queue[-1].output_table_name\n\n            try:\n                dataframe = self._sql_to_splink_dataframe_checking_cache(\n                    sql_gen,\n                    output_tablename_templated,\n                    materialise_as_hash,\n                    use_cache,\n                )\n            except Exception as e:\n                raise e\n            finally:\n                self._pipeline.reset()\n\n            return dataframe\n        else:\n            # In debug mode, we do not pipeline the sql and print the\n            # results of each part of the pipeline\n            for task in self._pipeline._generate_pipeline_parts(input_dataframes):\n                output_tablename = task.output_table_name\n                sql = task.sql\n                print(\"------\")\n                print(f\"--------Creating table: {output_tablename}--------\")\n\n                dataframe = self._sql_to_splink_dataframe_checking_cache(\n                    sql,\n                    output_tablename,\n                    materialise_as_hash=False,\n                    use_cache=False,\n                )\n            self._pipeline.reset()\n            return dataframe\n\n    def _execute_sql_against_backend(\n        self, sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n\"\"\"Execute a single sql SELECT statement, returning a SplinkDataFrame.\n\n        Subclasses should implement this, using _log_and_run_sql_execution() within\n        their implementation, maybe doing some SQL translation or other prep/cleanup\n        work before/after.\n        \"\"\"\n        raise NotImplementedError(\n            f\"_execute_sql_against_backend not implemented for {type(self)}\"\n        )\n\n    def _run_sql_execution(\n        self, final_sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n\"\"\"**Actually** execute the sql against the backend database.\n\n        This is intended to be implemented by a subclass, but not actually called\n        directly. Instead, call _log_and_run_sql_execution, and that will call\n        this method.\n\n        This could return something, or not. It's up to the Linker subclass to decide.\n        \"\"\"\n        raise NotImplementedError(\n            f\"_run_sql_execution not implemented for {type(self)}\"\n        )\n\n    def _log_and_run_sql_execution(\n        self, final_sql: str, templated_name: str, physical_name: str\n    ) -&gt; SplinkDataFrame:\n\"\"\"Log the sql, then call _run_sql_execution(), wrapping any errors\"\"\"\n        logger.debug(execute_sql_logging_message_info(templated_name, physical_name))\n        logger.log(5, log_sql(final_sql))\n        try:\n            return self._run_sql_execution(final_sql, templated_name, physical_name)\n        except Exception as e:\n            # Parse our SQL through sqlglot to pretty print\n            try:\n                final_sql = sqlglot.parse_one(\n                    final_sql,\n                    read=self._sql_dialect,\n                ).sql(pretty=True)\n                # if sqlglot produces any errors, just report the raw SQL\n            except Exception:\n                pass\n\n            raise SplinkException(\n                f\"Error executing the following sql for table \"\n                f\"`{templated_name}` ({physical_name}):\\n{final_sql}\"\n            ) from e\n\n    def register_table(self, input, table_name, overwrite=False):\n\"\"\"\n        Register a table to your backend database, to be used in one of the\n        splink methods, or simply to allow querying.\n\n        Tables can be of type: dictionary, record level dictionary,\n        pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n        Examples:\n            ```py\n            test_dict = {\"a\": [666,777,888],\"b\": [4,5,6]}\n            linker.register_table(test_dict, \"test_dict\")\n            linker.query_sql(\"select * from test_dict\")\n            ```\n\n        Args:\n            input: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            table_name (str): The name you wish to assign to the table.\n            overwrite (bool): Overwrite the table in the underlying database if it\n                exists\n\n        Returns:\n            SplinkDataFrame: An abstraction representing the table created by the sql\n                pipeline\n        \"\"\"\n\n        raise NotImplementedError(f\"register_table not implemented for {type(self)}\")\n\n    def _table_registration(self, input, table_name):\n\"\"\"\n        Register a table to your backend database, to be used in one of the\n        splink methods, or simply to allow querying.\n\n        Tables can be of type: dictionary, record level dictionary,\n        pandas dataframe, pyarrow table and in the spark case, a spark df.\n\n        This function is contains no overwrite functionality, so it can be used\n        where we don't want to allow for overwriting.\n\n        Args:\n            input: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            table_name (str): The name you wish to assign to the table.\n\n        Returns:\n            None\n        \"\"\"\n\n        raise NotImplementedError(\n            f\"_table_registration not implemented for {type(self)}\"\n        )\n\n    def query_sql(self, sql, output_type=\"pandas\"):\n\"\"\"\n        Run a SQL query against your backend database and return\n        the resulting output.\n\n        Examples:\n            === \"DuckDB\"\n                ```py\n                linker = DuckDBLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \"Spark\"\n                ```py\n                linker = SparkLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \"Athena\"\n                ```py\n                linker = AthenaLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n                ```\n            === \"SQLite\"\n                ```py\n                linker = SQLiteLinker(df, settings)\n                df_predict = linker.predict()\n                linker.query_sql(f\"select * from {df_predict.physical_name} limit 10\")\n            ```\n\n        Args:\n            sql (str): The SQL to be queried.\n            output_type (str): One of splink_df/splinkdf or pandas.\n                This determines the type of table that your results are output in.\n        \"\"\"\n\n        output_tablename_templated = \"__splink__df_sql_query\"\n\n        splink_dataframe = self._sql_to_splink_dataframe_checking_cache(\n            sql,\n            output_tablename_templated,\n            materialise_as_hash=False,\n            use_cache=False,\n        )\n\n        if output_type in (\"splink_df\", \"splinkdf\"):\n            return splink_dataframe\n        elif output_type == \"pandas\":\n            out = splink_dataframe.as_pandas_dataframe()\n            # If pandas, drop the table to cleanup the db\n            splink_dataframe.drop_table_from_database()\n            return out\n        else:\n            raise ValueError(\n                f\"output_type '{output_type}' is not supported.\",\n                \"Must be one of 'splink_df'/'splinkdf' or 'pandas'\",\n            )\n\n    def _sql_to_splink_dataframe_checking_cache(\n        self,\n        sql,\n        output_tablename_templated,\n        materialise_as_hash=True,\n        use_cache=True,\n    ) -&gt; SplinkDataFrame:\n\"\"\"Execute sql, or if identical sql has been run before, return cached results.\n\n        This function\n            - is used by _execute_sql_pipeline to to execute SQL\n            - or can be used directly if you have a single SQL statement that's\n              not in a pipeline\n\n        Return a SplinkDataFrame representing the results of the SQL\n        \"\"\"\n\n        to_hash = (sql + self._cache_uid).encode(\"utf-8\")\n        hash = hashlib.sha256(to_hash).hexdigest()[:9]\n        # Ensure hash is valid sql table name\n        table_name_hash = f\"{output_tablename_templated}_{hash}\"\n\n        if use_cache:\n            if self._table_exists_in_database(output_tablename_templated):\n                logger.debug(f\"Using existing table {output_tablename_templated}\")\n                return self._table_to_splink_dataframe(\n                    output_tablename_templated, output_tablename_templated\n                )\n\n            if self._table_exists_in_database(table_name_hash):\n                logger.debug(\n                    f\"Using cache for {output_tablename_templated}\"\n                    f\" with physical name {table_name_hash}\"\n                )\n                return self._table_to_splink_dataframe(\n                    output_tablename_templated, table_name_hash\n                )\n\n        if self.debug_mode:\n            print(sql)\n\n        if materialise_as_hash:\n            splink_dataframe = self._execute_sql_against_backend(\n                sql, output_tablename_templated, table_name_hash\n            )\n        else:\n            splink_dataframe = self._execute_sql_against_backend(\n                sql,\n                output_tablename_templated,\n                output_tablename_templated,\n            )\n\n        self._names_of_tables_created_by_splink.add(splink_dataframe.physical_name)\n\n        if self.debug_mode:\n            df_pd = splink_dataframe.as_pandas_dataframe()\n            try:\n                from IPython.display import display\n\n                display(df_pd)\n            except ModuleNotFoundError:\n                print(df_pd)\n\n        return splink_dataframe\n\n    def __deepcopy__(self, memo):\n\"\"\"When we do EM training, we need a copy of the linker which is independent\n        of the main linker e.g. setting parameters on the copy will not affect the\n        main linker.  This method implements ensures linker can be deepcopied.\n        \"\"\"\n        new_linker = copy(self)\n        new_linker._em_training_sessions = []\n        new_settings = deepcopy(self._settings_obj_)\n        new_linker._settings_obj_ = new_settings\n        return new_linker\n\n    def _ensure_aliases_populated_and_is_list(\n        self, input_table_or_tables, input_table_aliases\n    ):\n        if input_table_aliases is None:\n            input_table_aliases = input_table_or_tables\n\n        input_table_aliases = ensure_is_list(input_table_aliases)\n\n        return input_table_aliases\n\n    def _get_input_tables_dict(self, input_table_or_tables, input_table_aliases):\n        input_table_or_tables = ensure_is_list(input_table_or_tables)\n\n        input_table_aliases = self._ensure_aliases_populated_and_is_list(\n            input_table_or_tables, input_table_aliases\n        )\n\n        d = {}\n        for table_name, table_alias in zip(input_table_or_tables, input_table_aliases):\n            d[table_alias] = self._table_to_splink_dataframe(table_alias, table_name)\n        return d\n\n    def _get_input_tf_dict(self, df_dict):\n        d = {}\n        for df_name, df_value in df_dict.items():\n            renamed = colname_to_tf_tablename(df_name)\n            d[renamed] = self._table_to_splink_dataframe(renamed, df_value)\n        return d\n\n    def _predict_warning(self):\n        if not self._settings_obj._is_fully_trained:\n            msg = (\n                \"\\n -- WARNING --\\n\"\n                \"You have called predict(), but there are some parameter \"\n                \"estimates which have neither been estimated or specified in your \"\n                \"settings dictionary.  To produce predictions the following\"\n                \" untrained trained parameters will use default values.\"\n            )\n            messages = self._settings_obj._not_trained_messages()\n\n            warn_message = \"\\n\".join([msg] + messages)\n\n            logger.warning(warn_message)\n\n    def _table_exists_in_database(self, table_name):\n        raise NotImplementedError(\n            f\"table_exists_in_database not implemented for {type(self)}\"\n        )\n\n    def _validate_input_dfs(self):\n        if not hasattr(self, \"_input_tables_dict\"):\n            # This is only triggered where a user loads a settings dict from a\n            # given file path.\n            return\n\n        for df in self._input_tables_dict.values():\n            df.validate()\n\n        if self._settings_obj_ is not None:\n            if self._settings_obj._link_type == \"dedupe_only\":\n                if len(self._input_tables_dict) &gt; 1:\n                    raise ValueError(\n                        'If link_type = \"dedupe only\" then input tables must contain '\n                        \"only a single input table\",\n                    )\n\n    def _validate_dialect(self):\n        settings_dialect = self._settings_obj._sql_dialect\n        if settings_dialect != self._sql_dialect:\n            raise ValueError(\n                f\"Incompatible SQL dialect! `settings` dictionary uses \"\n                f\"dialect {settings_dialect}, but expecting \"\n                f\"'{self._sql_dialect}' for Linker of type {type(self)}\"\n            )\n\n    def _populate_probability_two_random_records_match_from_trained_values(self):\n        recip_prop_matches_estimates = []\n\n        logger.log(\n            15,\n            (\n                \"---- Using training sessions to compute \"\n                \"probability two random records match ----\"\n            ),\n        )\n        for em_training_session in self._em_training_sessions:\n            training_lambda = (\n                em_training_session._settings_obj._probability_two_random_records_match\n            )\n            training_lambda_bf = prob_to_bayes_factor(training_lambda)\n            reverse_levels = (\n                em_training_session._comparison_levels_to_reverse_blocking_rule\n            )\n\n            logger.log(\n                15,\n                \"\\n\"\n                f\"Probability two random records match from trained model blocking on \"\n                f\"{em_training_session._blocking_rule_for_training.blocking_rule}: \"\n                f\"{training_lambda:,.3f}\",\n            )\n\n            for reverse_level in reverse_levels:\n                # Get comparison level on current settings obj\n                cc = self._settings_obj._get_comparison_by_output_column_name(\n                    reverse_level.comparison._output_column_name\n                )\n\n                cl = cc._get_comparison_level_by_comparison_vector_value(\n                    reverse_level._comparison_vector_value\n                )\n\n                if cl._has_estimated_values:\n                    bf = cl._trained_m_median / cl._trained_u_median\n                else:\n                    bf = cl._bayes_factor\n\n                logger.log(\n                    15,\n                    f\"Reversing comparison level {cc._output_column_name}\"\n                    f\" using bayes factor {bf:,.3f}\",\n                )\n\n                training_lambda_bf = training_lambda_bf / bf\n\n                as_prob = bayes_factor_to_prob(training_lambda_bf)\n\n                logger.log(\n                    15,\n                    (\n                        \"This estimate of probability two random records match now: \"\n                        f\" {as_prob:,.3f} \"\n                        f\"with reciprocal {(1/as_prob):,.3f}\"\n                    ),\n                )\n            logger.log(15, \"\\n---------\")\n            p = bayes_factor_to_prob(training_lambda_bf)\n            recip_prop_matches_estimates.append(1 / p)\n\n        prop_matches_estimate = 1 / median(recip_prop_matches_estimates)\n\n        self._settings_obj._probability_two_random_records_match = prop_matches_estimate\n        logger.log(\n            15,\n            \"\\nMedian of prop of matches estimates: \"\n            f\"{self._settings_obj._probability_two_random_records_match:,.3f} \"\n            \"reciprocal \"\n            f\"{1/self._settings_obj._probability_two_random_records_match:,.3f}\",\n        )\n\n    def _populate_m_u_from_trained_values(self):\n        ccs = self._settings_obj.comparisons\n\n        for cc in ccs:\n            for cl in cc._comparison_levels_excluding_null:\n                if cl._has_estimated_u_values:\n                    cl.u_probability = cl._trained_u_median\n                if cl._has_estimated_m_values:\n                    cl.m_probability = cl._trained_m_median\n\n    def _delete_tables_created_by_splink_from_db(\n        self, retain_term_frequency=True, retain_df_concat_with_tf=True\n    ):\n        to_remove = set()\n        for name in self._names_of_tables_created_by_splink:\n            # Only delete tables explicitly marked as having been created by splink\n            if \"__splink__\" not in name:\n                continue\n            if name == \"__splink__df_concat_with_tf\":\n                if not retain_df_concat_with_tf:\n                    self._delete_table_from_database(name)\n                    to_remove.add(name)\n            elif name.startswith(\"__splink__df_tf_\"):\n                if not retain_term_frequency:\n                    self._delete_table_from_database(name)\n                    to_remove.add(name)\n            else:\n                self._delete_table_from_database(name)\n                to_remove.add(name)\n\n        self._names_of_tables_created_by_splink = (\n            self._names_of_tables_created_by_splink - to_remove\n        )\n\n    def _raise_error_if_necessary_waterfall_columns_not_computed(self):\n        ricc = self._settings_obj._retain_intermediate_calculation_columns\n        rmc = self._settings_obj._retain_matching_columns\n        if not (ricc and rmc):\n            raise ValueError(\n                \"retain_intermediate_calculation_columns and \"\n                \"retain_matching_columns must both be set to True in your settings\"\n                \" dictionary to use this function, because otherwise the necessary \"\n                \"columns will not be available in the input records.\"\n                f\" Their current values are {ricc} and {rmc}, respectively. \"\n                \"Please re-run your linkage with them both set to True.\"\n            )\n\n    def _raise_error_if_necessary_accuracy_columns_not_computed(self):\n        rmc = self._settings_obj._retain_matching_columns\n        if not (rmc):\n            raise ValueError(\n                \"retain_matching_columns must be set to True in your settings\"\n                \" dictionary to use this function, because otherwise the necessary \"\n                \"columns will not be available in the input records.\"\n                f\" Its current value is {rmc}. \"\n                \"Please re-run your linkage with it set to True.\"\n            )\n\n    def load_settings(self, settings_dict: dict | str | Path):\n\"\"\"Initialise settings for the linker.  To be used if settings were\n        not passed to the linker on creation. This can either be in the form\n        of a settings dictionary or a filepath to a json file containing a\n        valid settings dictionary.\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.profile_columns([\"first_name\", \"surname\"])\n            linker.load_settings(settings_dict)\n            ```\n\n        Args:\n            settings_dict (dict | str | Path): A Splink settings dictionary or\n                the path to your settings json file.\n        \"\"\"\n\n        if not isinstance(settings_dict, dict):\n            p = Path(settings_dict)\n            if not p.is_file():  # check if it's a valid file/filepath\n                raise FileNotFoundError(\n                    \"The filepath you have provided is either not a valid file \"\n                    \"or doesn't exist along the path provided.\"\n                )\n            settings_dict = json.loads(p.read_text())\n\n        # Store the cache ID so it can be reloaded after cache invalidation\n        cache_id = self._cache_uid\n        # So we don't run into any issues with generated tables having\n        # invalid columns as settings have been tweaked, invalidate\n        # the cache and allow these tables to be recomputed.\n\n        # This is less efficient, but triggers infrequently and ensures we don't\n        # run into issues where the defaults used conflict with the actual values\n        # supplied in settings.\n\n        # This is particularly relevant with `source_dataset`, which appears within\n        # concat_with_tf.\n        self.invalidate_cache()\n\n        # If a uid already exists in your settings object, prioritise this\n        settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", cache_id)\n        settings_dict[\"sql_dialect\"] = settings_dict.get(\n            \"sql_dialect\", self._sql_dialect\n        )\n        self._settings_dict = settings_dict\n        self._settings_obj_ = Settings(settings_dict)\n        self._validate_input_dfs()\n        self._validate_dialect()\n\n    def load_model(self, model_path: Path):\n\"\"\"\n        Load a pre-defined model from a json file into the linker.\n        This is intended to be used with the output of\n        `save_model_to_json()`.\n\n        Examples:\n            ```py\n            linker.load_model(\"my_settings.json\")\n            ```\n\n        Args:\n            model_path (Path): A path to your model settings json file.\n        \"\"\"\n\n        return self.load_settings(model_path)\n\n    def initialise_settings(self, settings_dict: dict):\n\"\"\"*This method is now deprecated. Please use `load_settings`\n        when loading existing settings or `load_model` when loading\n         a pre-trained model.*\n\n        Initialise settings for the linker.  To be used if settings were\n        not passed to the linker on creation.\n        Examples:\n            === \"DuckDB\"\n                ```py\n                linker = DuckDBLinker(df\")\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \"Spark\"\n                ```py\n                linker = SparkLinker(df\")\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \"Athena\"\n                ```py\n                linker = AthenaLinker(df\")\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n            === \"SQLite\"\n                ```py\n                linker = SQLiteLinker(df\")\n                linker.profile_columns([\"first_name\", \"surname\"])\n                linker.initialise_settings(settings_dict)\n                ```\n        Args:\n            settings_dict (dict): A Splink settings dictionary\n        \"\"\"\n        # If a uid already exists in your settings object, prioritise this\n        settings_dict[\"linker_uid\"] = settings_dict.get(\"linker_uid\", self._cache_uid)\n        settings_dict[\"sql_dialect\"] = settings_dict.get(\n            \"sql_dialect\", self._sql_dialect\n        )\n        self._settings_dict = settings_dict\n        self._settings_obj_ = Settings(settings_dict)\n        self._validate_input_dfs()\n        self._validate_dialect()\n\n        warnings.warn(\n            \"`initialise_settings` is deprecated. We advise you use \"\n            \"`linker.load_settings()` when loading in your settings or a previously \"\n            \"trained model.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    def load_settings_from_json(self, in_path: str | Path):\n\"\"\"*This method is now deprecated. Please use `load_settings`\n        when loading existing settings or `load_model` when loading\n         a pre-trained model.*\n\n        Load settings from a `.json` file.\n        This `.json` file would usually be the output of\n        `linker.save_model_to_json()`\n        Examples:\n            ```py\n            linker.load_settings_from_json(\"my_settings.json\")\n            ```\n        Args:\n            in_path (str): Path to settings json file\n        \"\"\"\n        self.load_settings(in_path)\n\n        warnings.warn(\n            \"`load_settings_from_json` is deprecated. We advise you use \"\n            \"`linker.load_settings()` when loading in your settings or a previously \"\n            \"trained model.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    def compute_tf_table(self, column_name: str) -&gt; SplinkDataFrame:\n\"\"\"Compute a term frequency table for a given column and persist to the database\n\n        This method is useful if you want to pre-compute term frequency tables e.g.\n        so that real time linkage executes faster, or so that you can estimate\n        various models without having to recompute term frequency tables each time\n\n        Examples:\n            === \"DuckDB\"\n                Real time linkage\n                ```py\n                linker = DuckDBLinker(df)\n                linker.load_settings(\"saved_settings.json\")\n                linker.compute_tf_table(\"surname\")\n                linker.compare_two_records(record_left, record_right)\n                ```\n                Pre-computed term frequency tables\n                ```py\n                linker = DuckDBLinker(df)\n                df_first_name_tf = linker.compute_tf_table(\"first_name\")\n                df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n                &gt;&gt;&gt;\n                # On subsequent data linking job, read this table rather than recompute\n                df_first_name_tf = pd.read_parquet(\"folder/first_name_tf\")\n                df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n                ```\n            === \"Spark\"\n                Real time linkage\n                ```py\n                linker = SparkLinker(df)\n                linker.load_settings(\"saved_settings.json\")\n                linker.compute_tf_table(\"surname\")\n                linker.compare_two_records(record_left, record_right)\n                ```\n                Pre-computed term frequency tables\n                ```py\n                linker = SparkLinker(df)\n                df_first_name_tf = linker.compute_tf_table(\"first_name\")\n                df_first_name_tf.write.parquet(\"folder/first_name_tf\")\n                &gt;&gt;&gt;\n                # On subsequent data linking job, read this table rather than recompute\n                df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\")\n                df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\")\n                ```\n\n        Args:\n            column_name (str): The column name in the input table\n\n        Returns:\n            SplinkDataFrame: The resultant table as a splink data frame\n        \"\"\"\n\n        input_col = InputColumn(column_name, settings_obj=self._settings_obj)\n        tf_tablename = colname_to_tf_tablename(input_col)\n        cache = self._intermediate_table_cache\n        concat_tf_tables = [\n            remove_quotes_from_identifiers(tf_col.input_name_as_tree).sql()\n            for tf_col in self._settings_obj._term_frequency_columns\n        ]\n\n        if tf_tablename in cache:\n            tf_df = cache[tf_tablename]\n        elif \"__splink__df_concat_with_tf\" in cache and column_name in concat_tf_tables:\n            self._pipeline.reset()\n            # If our df_concat_with_tf table already exists, use backwards inference to\n            # find a given tf table\n            colname = InputColumn(column_name)\n            sql = term_frequencies_from_concat_with_tf(colname)\n            self._enqueue_sql(sql, colname_to_tf_tablename(colname))\n            tf_df = self._execute_sql_pipeline(\n                [cache[\"__splink__df_concat_with_tf\"]], materialise_as_hash=True\n            )\n            self._intermediate_table_cache[tf_tablename] = tf_df\n        else:\n            # Clear the pipeline if we are materialising\n            self._pipeline.reset()\n            df_concat = self._initialise_df_concat()\n            input_dfs = []\n            if df_concat:\n                input_dfs.append(df_concat)\n            sql = term_frequencies_for_single_column_sql(input_col)\n            self._enqueue_sql(sql, tf_tablename)\n            tf_df = self._execute_sql_pipeline(input_dfs, materialise_as_hash=True)\n            self._intermediate_table_cache[tf_tablename] = tf_df\n\n        return tf_df\n\n    def deterministic_link(self) -&gt; SplinkDataFrame:\n\"\"\"Uses the blocking rules specified by\n        `blocking_rules_to_generate_predictions` in the settings dictionary to\n        generate pairwise record comparisons.\n\n        For deterministic linkage, this should be a list of blocking rules which\n        are strict enough to generate only true links.\n\n        Deterministic linkage, however, is likely to result in missed links\n        (false negatives).\n\n        Examples:\n            === \"DuckDB\"\n            ```py\n            from splink.duckdb.duckdb_linker import DuckDBLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = DuckDBLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n            === \"Spark\"\n            ```py\n            from splink.spark.spark_linker import SparkLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = SparkLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n            === \"Athena\"\n            ```py\n            from splink.athena.athena_linker import AthenaLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = AthenaLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n            === \"SQLite\"\n            ```py\n            from splink.sqlite.sqlite_linker import SQLiteLinker\n\n            settings = {\n                \"link_type\": \"dedupe_only\",\n                \"blocking_rules_to_generate_predictions\": [\n                    \"l.first_name = r.first_name\",\n                    \"l.surname = r.surname\",\n                ],\n                \"comparisons\": []\n            }\n            &gt;&gt;&gt;\n            linker = SQLiteLinker(df, settings)\n            df = linker.deterministic_link()\n            ```\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n                represents a table materialised in the database. Methods on the\n                SplinkDataFrame allow you to access the underlying data.\n        \"\"\"\n\n        # Allows clustering during a deterministic linkage.\n        # This is used in `cluster_pairwise_predictions_at_threshold`\n        # to set the cluster threshold to 1\n        self._deterministic_link_mode = True\n\n        concat_with_tf = self._initialise_df_concat_with_tf()\n        sql = block_using_rules_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_blocked\")\n        return self._execute_sql_pipeline([concat_with_tf])\n\n    def estimate_u_using_random_sampling(\n        self, max_pairs: int = None, seed: int = None, *, target_rows=None\n    ):\n\"\"\"Estimate the u parameters of the linkage model using random sampling.\n\n        The u parameters represent the proportion of record comparisons that fall\n        into each comparison level amongst truly non-matching records.\n\n        This procedure takes a sample of the data and generates the cartesian\n        product of pairwise record comparisons amongst the sampled records.\n        The validity of the u values rests on the assumption that the resultant\n        pairwise comparisons are non-matches (or at least, they are very unlikely to be\n        matches). For large datasets, this is typically true.\n\n        The results of estimate_u_using_random_sampling, and therefore an entire splink\n        model, can be made reproducible by setting the seed parameter. Setting the seed\n        will have performance implications as additional processing is required.\n\n        Args:\n            max_pairs (int): The maximum number of pairwise record comparisons to\n            sample. Larger will give more accurate estimates\n            but lead to longer runtimes.  In our experience at least 1e9 (one billion)\n            gives best results but can take a long time to compute. 1e7 (ten million)\n            is often adequate whilst testing different model specifications, before\n            the final model is estimated.\n            seed (int): Seed for random sampling. Assign to get reproducible u\n            probabilities. Note, seed for random sampling is only supported for\n            DuckDB and Spark, for Athena and SQLite set to None.\n\n        Examples:\n            ```py\n            linker.estimate_u_using_random_sampling(1e8)\n            ```\n\n        Returns:\n            None: Updates the estimated u parameters within the linker object\n            and returns nothing.\n        \"\"\"\n        # TODO: Remove this compatibility code in a future release once we drop\n        # support for \"target_rows\". Deprecation warning added in 3.7.0\n        if max_pairs is not None and target_rows is not None:\n            # user supplied both\n            raise TypeError(\"Just use max_pairs\")\n        elif max_pairs is not None:\n            # user is doing it correctly\n            pass\n        elif target_rows is not None:\n            # user is using deprecated argument\n            warnings.warn(\n                \"target_rows is deprecated; use max_pairs\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            max_pairs = target_rows\n        else:\n            raise TypeError(\"Missing argument max_pairs\")\n\n        estimate_u_values(self, max_pairs, seed)\n        self._populate_m_u_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n    def estimate_m_from_label_column(self, label_colname: str):\n\"\"\"Estimate the m parameters of the linkage model from a label (ground truth)\n        column in the input dataframe(s).\n\n        The m parameters represent the proportion of record comparisons that fall\n        into each comparison level amongst truly matching records.\n\n        The ground truth column is used to generate pairwise record comparisons\n        which are then assumed to be matches.\n\n        For example, if the entity being matched is persons, and your input dataset(s)\n        contain social security number, this could be used to estimate the m values\n        for the model.\n\n        Note that this column does not need to be fully populated.  A common case is\n        where a unique identifier such as social security number is only partially\n        populated.\n\n        Args:\n            label_colname (str): The name of the column containing the ground truth\n                label in the input data.\n\n        Examples:\n            ```py\n            linker.estimate_m_from_label_column(\"social_security_number\")\n            ```\n\n        Returns:\n            Updates the estimated m parameters within the linker object\n            and returns nothing.\n        \"\"\"\n\n        # Ensure this has been run on the main linker so that it can be used by\n        # training linked when it checks the cache\n        self._initialise_df_concat_with_tf()\n        estimate_m_values_from_label_column(\n            self,\n            self._input_tables_dict,\n            label_colname,\n        )\n        self._populate_m_u_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n    def estimate_parameters_using_expectation_maximisation(\n        self,\n        blocking_rule: str,\n        comparisons_to_deactivate: list[str | Comparison] = None,\n        comparison_levels_to_reverse_blocking_rule: list[ComparisonLevel] = None,\n        fix_probability_two_random_records_match: bool = False,\n        fix_m_probabilities=False,\n        fix_u_probabilities=True,\n        populate_probability_two_random_records_match_from_trained_values=False,\n    ) -&gt; EMTrainingSession:\n\"\"\"Estimate the parameters of the linkage model using expectation maximisation.\n\n        By default, the m probabilities are estimated, but not the u probabilities,\n        because good estimates for the u probabilities can be obtained from\n        `linker.estimate_u_using_random_sampling()`.  You can change this by setting\n        `fix_u_probabilities` to False.\n\n        The blocking rule provided is used to generate pairwise record comparisons.\n        Usually, this should be a blocking rule that results in a dataframe where\n        matches are between about 1% and 99% of the comparisons.\n\n        By default, m parameters are estimated for all comparisons except those which\n        are included in the blocking rule.\n\n        For example, if the blocking rule is `l.first_name = r.first_name`, then\n        parameter esimates will be made for all comparison except those which use\n        `first_name` in their sql_condition\n\n        By default, the probability two random records match is estimated for the\n        blocked data, and then the m and u parameters for the columns specified in the\n        blocking rules are used to estiamte the global probability two random records\n        match.\n\n        To control which comparisons should have their parameter estimated, and the\n        process of 'reversing out' the global probability two random records match, the\n        user may specify `comparisons_to_deactivate` and\n        `comparison_levels_to_reverse_blocking_rule`.   This is useful, for example\n        if you block on the dmetaphone of a column but match on the original column.\n\n        Examples:\n            Default behaviour\n            ```py\n            br_training = \"l.first_name = r.first_name and l.dob = r.dob\"\n            linker.estimate_parameters_using_expectation_maximisation(br_training)\n            ```\n            Specify which comparisons to deactivate\n            ```py\n            br_training = \"l.dmeta_first_name = r.dmeta_first_name\"\n            settings_obj = linker._settings_obj\n            comp = settings_obj._get_comparison_by_output_column_name(\"first_name\")\n            dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1)\n            linker.estimate_parameters_using_expectation_maximisation(\n                br_training,\n                comparisons_to_deactivate=[\"first_name\"],\n                comparison_levels_to_reverse_blocking_rule=[dmeta_level],\n            )\n            ```\n\n        Args:\n            blocking_rule (str): The blocking rule used to generate pairwise record\n                comparisons.\n            comparisons_to_deactivate (list, optional): By default, splink will\n                analyse the blocking rule provided and estimate the m parameters for\n                all comaprisons except those included in the blocking rule.  If\n                comparisons_to_deactivate are provided, spink will instead\n                estimate m parameters for all comparison except those specified\n                in the comparisons_to_deactivate list.  This list can either contain\n                the output_column_name of the Comparison as a string, or Comparison\n                objects.  Defaults to None.\n            comparison_levels_to_reverse_blocking_rule (list, optional): By default,\n                splink will analyse the blocking rule provided and adjust the\n                global probability two random records match to account for the matches\n                specified in the blocking rule. If provided, this argument will overrule\n                this default behaviour. The user must provide a list of ComparisonLevel\n                objects.  Defaults to None.\n            fix_probability_two_random_records_match (bool, optional): If True, do not\n                update the probability two random records match after each iteration.\n                Defaults to False.\n            fix_m_probabilities (bool, optional): If True, do not update the m\n                probabilities after each iteration. Defaults to False.\n            fix_u_probabilities (bool, optional): If True, do not update the u\n                probabilities after each iteration. Defaults to True.\n            populate_probability_two_random_records_match_from_trained_values\n                (bool, optional): If True, derive this parameter from\n                the blocked value. Defaults to False.\n\n        Examples:\n            ```py\n            blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\"\n            linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n            ```\n\n        Returns:\n            EMTrainingSession:  An object containing information about the training\n                session such as how parameters changed during the iteration history\n\n        \"\"\"\n        # Ensure this has been run on the main linker so that it's in the cache\n        # to be used by the training linkers\n        self._initialise_df_concat_with_tf()\n\n        if comparisons_to_deactivate:\n            # If user provided a string, convert to Comparison object\n            comparisons_to_deactivate = [\n                self._settings_obj._get_comparison_by_output_column_name(n)\n                if isinstance(n, str)\n                else n\n                for n in comparisons_to_deactivate\n            ]\n            if comparison_levels_to_reverse_blocking_rule is None:\n                logger.warning(\n                    \"\\nWARNING: \\n\"\n                    \"You have provided comparisons_to_deactivate but not \"\n                    \"comparison_levels_to_reverse_blocking_rule.\\n\"\n                    \"If comparisons_to_deactivate is provided, then \"\n                    \"you usually need to provide corresponding \"\n                    \"comparison_levels_to_reverse_blocking_rule \"\n                    \"because each comparison to deactivate is effectively treated \"\n                    \"as an exact match.\"\n                )\n\n        em_training_session = EMTrainingSession(\n            self,\n            blocking_rule,\n            fix_u_probabilities=fix_u_probabilities,\n            fix_m_probabilities=fix_m_probabilities,\n            fix_probability_two_random_records_match=fix_probability_two_random_records_match,  # noqa 501\n            comparisons_to_deactivate=comparisons_to_deactivate,\n            comparison_levels_to_reverse_blocking_rule=comparison_levels_to_reverse_blocking_rule,  # noqa 501\n        )\n\n        em_training_session._train()\n\n        self._populate_m_u_from_trained_values()\n\n        if populate_probability_two_random_records_match_from_trained_values:\n            self._populate_probability_two_random_records_match_from_trained_values()\n\n        self._settings_obj._columns_without_estimated_parameters_message()\n\n        return em_training_session\n\n    def predict(\n        self,\n        threshold_match_probability: float = None,\n        threshold_match_weight: float = None,\n        materialise_after_computing_term_frequencies=True,\n    ) -&gt; SplinkDataFrame:\n\"\"\"Create a dataframe of scored pairwise comparisons using the parameters\n        of the linkage model.\n\n        Uses the blocking rules specified in the\n        `blocking_rules_to_generate_predictions` of the settings dictionary to\n        generate the pairwise comparisons.\n\n        Args:\n            threshold_match_probability (float, optional): If specified,\n                filter the results to include only pairwise comparisons with a\n                match_probability above this threshold. Defaults to None.\n            threshold_match_weight (float, optional): If specified,\n                filter the results to include only pairwise comparisons with a\n                match_weight above this threshold. Defaults to None.\n            materialise_after_computing_term_frequencies (bool): If true, Splink\n                will materialise the table containing the input nodes (rows)\n                joined to any term frequencies which have been asked\n                for in the settings object.  If False, this will be\n                computed as part of one possibly gigantic CTE\n                pipeline.   Defaults to True\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            df = linker.predict(threshold_match_probability=0.95)\n            df.as_pandas_dataframe(limit=5)\n            ```\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons.  This\n                represents a table materialised in the database. Methods on the\n                SplinkDataFrame allow you to access the underlying data.\n\n        \"\"\"\n\n        # If materialise_after_computing_term_frequencies=False and the user only\n        # calls predict, it runs as a single pipeline with no materialisation\n        # of anything.\n\n        # _initialise_df_concat_with_tf returns None if the table doesn't exist\n        # and only SQL is queued in this step.\n        nodes_with_tf = self._initialise_df_concat_with_tf(\n            materialise=materialise_after_computing_term_frequencies\n        )\n\n        input_dataframes = []\n        if nodes_with_tf:\n            input_dataframes.append(nodes_with_tf)\n\n        sql = block_using_rules_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_blocked\")\n\n        repartition_after_blocking = getattr(self, \"repartition_after_blocking\", False)\n\n        # repartition after blocking only exists on the SparkLinker\n        if repartition_after_blocking:\n            df_blocked = self._execute_sql_pipeline(input_dataframes)\n            input_dataframes.append(df_blocked)\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            threshold_match_probability,\n            threshold_match_weight,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        predictions = self._execute_sql_pipeline(input_dataframes)\n        self._predict_warning()\n        return predictions\n\n    def find_matches_to_new_records(\n        self,\n        records_or_tablename,\n        blocking_rules=[],\n        match_weight_threshold=-4,\n    ) -&gt; SplinkDataFrame:\n\"\"\"Given one or more records, find records in the input dataset(s) which match\n        and return in order of the splink prediction score.\n\n        This effectively provides a way of searching the input datasets\n        for given record(s)\n\n        Args:\n            records_or_tablename (List[dict]): Input search record(s) as list of dict,\n                or a table registered to the database.\n            blocking_rules (list, optional): Blocking rules to select\n                which records to find and score. If [], do not use a blocking\n                rule - meaning the input records will be compared to all records\n                provided to the linker when it was instantiated. Defaults to [].\n            match_weight_threshold (int, optional): Return matches with a match weight\n                above this threshold. Defaults to -4.\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            # Pre-compute tf tables for any tables with\n            # term frequency adjustments\n            linker.compute_tf_table(\"first_name\")\n            record = {'unique_id': 1,\n                'first_name': \"John\",\n                'surname': \"Smith\",\n                'dob': \"1971-05-24\",\n                'city': \"London\",\n                'email': \"john@smith.net\"\n                }\n            df = linker.find_matches_to_new_records([record], blocking_rules=[])\n            ```\n\n        Returns:\n            SplinkDataFrame: The pairwise comparisons.\n        \"\"\"\n\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        if not isinstance(records_or_tablename, str):\n            uid = ascii_uid(8)\n            self.register_table(\n                records_or_tablename, f\"__splink__df_new_records_{uid}\", overwrite=True\n            )\n            new_records_tablename = f\"__splink__df_new_records_{uid}\"\n        else:\n            new_records_tablename = records_or_tablename\n\n        cache = self._intermediate_table_cache\n        input_dfs = []\n        # If our df_concat_with_tf table already exists, use backwards inference to\n        # find all underlying term frequency tables.\n        if \"__splink__df_concat_with_tf\" in cache:\n            concat_with_tf = cache[\"__splink__df_concat_with_tf\"]\n            tf_tables = compute_term_frequencies_from_concat_with_tf(self)\n            # This queues up our tf tables, rather materialising them\n            for tf in tf_tables:\n                # if tf is a SplinkDataFrame, then the table already exists\n                if isinstance(tf, SplinkDataFrame):\n                    input_dfs.append(tf)\n                else:\n                    self._enqueue_sql(tf[\"sql\"], tf[\"output_table_name\"])\n        else:\n            # This queues up our cols_with_tf and df_concat_with_tf tables.\n            concat_with_tf = self._initialise_df_concat_with_tf(materialise=False)\n\n        if concat_with_tf:\n            input_dfs.append(concat_with_tf)\n\n        rules = []\n        for r in blocking_rules:\n            br_as_obj = BlockingRule(r) if not isinstance(r, BlockingRule) else r\n            br_as_obj.preceding_rules = rules.copy()\n            rules.append(br_as_obj)\n        blocking_rules = rules\n\n        self._settings_obj._blocking_rules_to_generate_predictions = blocking_rules\n\n        self._settings_obj._link_type = \"link_only_find_matches_to_new_records\"\n        self._find_new_matches_mode = True\n\n        sql = _join_tf_to_input_df_sql(self)\n        sql = sql.replace(\"__splink__df_concat\", new_records_tablename)\n        self._enqueue_sql(sql, \"__splink__df_new_records_with_tf\")\n\n        sql = block_using_rules_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_blocked\")\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        sql = f\"\"\"\n        select * from __splink__df_predict\n        where match_weight &gt; {match_weight_threshold}\n        \"\"\"\n\n        self._enqueue_sql(sql, \"__splink__find_matches_predictions\")\n\n        predictions = self._execute_sql_pipeline(\n            input_dataframes=input_dfs, use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._find_new_matches_mode = False\n\n        return predictions\n\n    def compare_two_records(self, record_1: dict, record_2: dict):\n\"\"\"Use the linkage model to compare and score a pairwise record comparison\n        based on the two input records provided\n\n        Args:\n            record_1 (dict): dictionary representing the first record.  Columns names\n                and data types must be the same as the columns in the settings object\n            record_2 (dict): dictionary representing the second record.  Columns names\n                and data types must be the same as the columns in the settings object\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.compare_two_records(record_left, record_right)\n            ```\n\n        Returns:\n            SplinkDataFrame: Pairwise comparison with scored prediction\n        \"\"\"\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        self._compare_two_records_mode = True\n        self._settings_obj._blocking_rules_to_generate_predictions = []\n\n        uid = ascii_uid(8)\n        df_records_left = self.register_table(\n            [record_1], f\"__splink__compare_two_records_left_{uid}\", overwrite=True\n        )\n        df_records_left.templated_name = \"__splink__compare_two_records_left\"\n\n        df_records_right = self.register_table(\n            [record_2], f\"__splink__compare_two_records_right_{uid}\", overwrite=True\n        )\n        df_records_right.templated_name = \"__splink__compare_two_records_right\"\n\n        sql_join_tf = _join_tf_to_input_df_sql(self)\n\n        sql_join_tf = sql_join_tf.replace(\n            \"__splink__df_concat\", \"__splink__compare_two_records_left\"\n        )\n        self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_left_with_tf\")\n\n        sql_join_tf = sql_join_tf.replace(\n            \"__splink__compare_two_records_left\", \"__splink__compare_two_records_right\"\n        )\n\n        self._enqueue_sql(sql_join_tf, \"__splink__compare_two_records_right_with_tf\")\n\n        sql = block_using_rules_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_blocked\")\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        predictions = self._execute_sql_pipeline(\n            [df_records_left, df_records_right], use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._compare_two_records_mode = False\n\n        return predictions\n\n    def _self_link(self) -&gt; SplinkDataFrame:\n\"\"\"Use the linkage model to compare and score all records in our input df with\n            themselves.\n\n        Returns:\n            SplinkDataFrame: Scored pairwise comparisons of the input records to\n                themselves.\n        \"\"\"\n\n        original_blocking_rules = (\n            self._settings_obj._blocking_rules_to_generate_predictions\n        )\n        original_link_type = self._settings_obj._link_type\n\n        # Changes our sql to allow for a self link.\n        # This is used in `_sql_gen_where_condition` in blocking.py\n        # to remove any 'where' clauses when blocking (normally when blocking\n        # we want to *remove* self links!)\n        self._self_link_mode = True\n\n        # Block on uid i.e. create pairwise record comparisons where the uid matches\n        uid_cols = self._settings_obj._unique_id_input_columns\n        uid_l = _composite_unique_id_from_edges_sql(uid_cols, None, \"l\")\n        uid_r = _composite_unique_id_from_edges_sql(uid_cols, None, \"r\")\n\n        self._settings_obj._blocking_rules_to_generate_predictions = [\n            BlockingRule(f\"{uid_l} = {uid_r}\")\n        ]\n\n        nodes_with_tf = self._initialise_df_concat_with_tf()\n\n        sql = block_using_rules_sql(self)\n\n        self._enqueue_sql(sql, \"__splink__df_blocked\")\n\n        sql = compute_comparison_vector_values_sql(self._settings_obj)\n\n        self._enqueue_sql(sql, \"__splink__df_comparison_vectors\")\n\n        sqls = predict_from_comparison_vectors_sqls(\n            self._settings_obj,\n            sql_infinity_expression=self._infinity_expression,\n        )\n        for sql in sqls:\n            output_table_name = sql[\"output_table_name\"]\n            output_table_name = output_table_name.replace(\"predict\", \"self_link\")\n            self._enqueue_sql(sql[\"sql\"], output_table_name)\n\n        predictions = self._execute_sql_pipeline(\n            input_dataframes=[nodes_with_tf], use_cache=False\n        )\n\n        self._settings_obj._blocking_rules_to_generate_predictions = (\n            original_blocking_rules\n        )\n        self._settings_obj._link_type = original_link_type\n        self._self_link_mode = False\n\n        return predictions\n\n    def cluster_pairwise_predictions_at_threshold(\n        self,\n        df_predict: SplinkDataFrame,\n        threshold_match_probability: float = None,\n        pairwise_formatting: bool = False,\n        filter_pairwise_format_for_clusters: bool = True,\n    ) -&gt; SplinkDataFrame:\n\"\"\"Clusters the pairwise match predictions that result from `linker.predict()`\n        into groups of connected record using the connected components graph clustering\n        algorithm\n\n        Records with an estimated `match_probability` above\n        `threshold_match_probability` are considered to be a match (i.e. they represent\n        the same entity).\n\n        Args:\n            df_predict (SplinkDataFrame): The results of `linker.predict()`\n            threshold_match_probability (float): Filter the pairwise match predictions\n                to include only pairwise comparisons with a match_probability above this\n                threshold. This dataframe is then fed into the clustering\n                algorithm.\n            pairwise_formatting (bool): Whether to output the pairwise match predictions\n                from linker.predict() with cluster IDs.\n                If this is set to false, the output will be a list of all IDs, clustered\n                into groups based on the desired match threshold.\n            filter_pairwise_format_for_clusters (bool): If pairwise formatting has been\n                selected, whether to output all columns found within linker.predict(),\n                or just return clusters.\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered\n                into groups based on the desired match threshold.\n\n        \"\"\"\n\n        # Feeding in df_predict forces materiailisation, if it exists in your database\n        concat_with_tf = self._initialise_df_concat_with_tf(df_predict)\n\n        edges_table = _cc_create_unique_id_cols(\n            self,\n            concat_with_tf.physical_name,\n            df_predict.physical_name,\n            threshold_match_probability,\n        )\n\n        cc = solve_connected_components(\n            self,\n            edges_table,\n            df_predict,\n            concat_with_tf,\n            pairwise_formatting,\n            filter_pairwise_format_for_clusters,\n        )\n\n        return cc\n\n    def profile_columns(\n        self, column_expressions: str | list[str], top_n=10, bottom_n=10\n    ):\n        return profile_columns(self, column_expressions, top_n=top_n, bottom_n=bottom_n)\n\n    def _get_labels_tablename_from_input(\n        self, labels_splinkdataframe_or_table_name: str | SplinkDataFrame\n    ):\n        if isinstance(labels_splinkdataframe_or_table_name, SplinkDataFrame):\n            labels_tablename = labels_splinkdataframe_or_table_name.physical_name\n        elif isinstance(labels_splinkdataframe_or_table_name, str):\n            labels_tablename = labels_splinkdataframe_or_table_name\n        else:\n            raise ValueError(\n                \"The 'labels_splinkdataframe_or_table_name' argument\"\n                \" must be of type SplinkDataframe or a string representing a tablename\"\n                \" in the input database\"\n            )\n        return labels_tablename\n\n    def estimate_m_from_pairwise_labels(self, labels_splinkdataframe_or_table_name):\n\"\"\"Estimate the m parameters of the linkage model from a dataframe of pairwise\n        labels.\n\n        The table of labels should be in the following format, and should\n        be registered with your database:\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|\n        |----------------|-----------|----------------|-----------|\n        |df_1            |1          |df_2            |2          |\n        |df_1            |1          |df_2            |3          |\n\n        Note that `source_dataset` and `unique_id` should correspond to the\n        values specified in the settings dict, and the `input_table_aliases`\n        passed to the `linker` object. Note that at the moment, this method does\n        not respect values in a `clerical_match_score` column.  If provided, these\n        are ignored and it is assumed that every row in the table of labels is a score\n        of 1, i.e. a perfect match.\n\n        Args:\n          labels_splinkdataframe_or_table_name (str): Name of table containing labels\n            in the database or SplinkDataframe\n\n        Examples:\n            ```py\n            pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\")\n            linker.register_table(pairwise_labels, \"labels\", overwrite=True)\n            linker.estimate_m_from_pairwise_labels(\"labels\")\n            ```\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        estimate_m_from_pairwise_labels(self, labels_tablename)\n\n    def truth_space_table_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ) -&gt; SplinkDataFrame:\n\"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n        match_probability, suitable for plotting a ROC chart.\n\n        The table of labels should be in the following format, and should be registered\n        with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            === \"DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.truth_space_table_from_labels_table(\"labels\")\n                ```\n            === \"Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.truth_space_table_from_labels_table(\"labels\")\n                ```\n        Returns:\n            SplinkDataFrame:  Table of truth statistics\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        return truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n\n    def roc_chart_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name: str | SplinkDataFrame,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n\"\"\"Generate a ROC chart from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            === \"DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.roc_chart_from_labels_table(\"labels\")\n                ```\n            === \"Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.roc_chart_from_labels_table(\"labels\")\n                ```\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return roc_chart(recs)\n\n    def precision_recall_chart_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n\"\"\"Generate a precision-recall chart from labelled (ground truth) data.\n\n        The table of labels should be in the following format, and should be registered\n        as a table with your database:\n\n        |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n        |----------------|-----------|----------------|-----------|--------------------|\n        |df_1            |1          |df_2            |2          |0.99                |\n        |df_1            |1          |df_2            |3          |0.2                 |\n\n        Note that `source_dataset` and `unique_id` should correspond to the values\n        specified in the settings dict, and the `input_table_aliases` passed to the\n        `linker` object.\n\n        For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n        Examples:\n            === \"DuckDB\"\n                ```py\n                labels = pd.read_csv(\"my_labels.csv\")\n                linker.register_table(labels, \"labels\")\n                linker.precision_recall_chart_from_labels_table(\"labels\")\n                ```\n            === \"Spark\"\n                ```py\n                labels = spark.read.csv(\"my_labels.csv\", header=True)\n                labels.createDataFrame(\"labels\")\n                linker.precision_recall_chart_from_labels_table(\"labels\")\n                ```\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        self._raise_error_if_necessary_accuracy_columns_not_computed()\n        df_truth_space = truth_space_table_from_labels_table(\n            self,\n            labels_tablename,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return precision_recall_chart(recs)\n\n    def prediction_errors_from_labels_table(\n        self,\n        labels_splinkdataframe_or_table_name,\n        include_false_positives=True,\n        include_false_negatives=True,\n        threshold=0.5,\n    ):\n\"\"\"Generate a dataframe containing false positives and false negatives\n        based on the comparison between the clerical_match_score in the labels\n        table compared with the splink predicted match probability\n\n        Args:\n            labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n                containing labels in the database\n            include_false_positives (bool, optional): Defaults to True.\n            include_false_negatives (bool, optional): Defaults to True.\n            threshold (float, optional): Threshold above which a score is considered\n                to be a match. Defaults to 0.5.\n\n        Returns:\n            SplinkDataFrame:  Table containing false positives and negatives\n        \"\"\"\n        labels_tablename = self._get_labels_tablename_from_input(\n            labels_splinkdataframe_or_table_name\n        )\n        return prediction_errors_from_labels_table(\n            self,\n            labels_tablename,\n            include_false_positives,\n            include_false_negatives,\n            threshold,\n        )\n\n    def truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n\"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n        match_probability, suitable for plotting a ROC chart.\n\n        Your labels_column_name should include the ground truth cluster (unique\n        identifier) that groups entities which are the same\n\n        Args:\n            labels_tablename (str): Name of table containing labels in the database\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            ```py\n            linker.truth_space_table_from_labels_column(\"cluster\")\n            ```\n\n        Returns:\n            SplinkDataFrame:  Table of truth statistics\n        \"\"\"\n\n        return truth_space_table_from_labels_column(\n            self, labels_column_name, threshold_actual, match_weight_round_to_nearest\n        )\n\n    def roc_chart_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n\"\"\"Generate a ROC chart from ground truth data, whereby the ground truth\n        is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n\n        Examples:\n            ```py\n            linker.roc_chart_from_labels_column(\"labels\")\n            ```\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n        \"\"\"\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return roc_chart(recs)\n\n    def precision_recall_chart_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=0.5,\n        match_weight_round_to_nearest: float = None,\n    ):\n\"\"\"Generate a precision-recall chart from ground truth data, whereby the ground\n        truth is in a column in the input dataset called `labels_column_name`\n\n        Args:\n            labels_column_name (str): Column name containing labels in the input table\n            threshold_actual (float, optional): Where the `clerical_match_score`\n                provided by the user is a probability rather than binary, this value\n                is used as the threshold to classify `clerical_match_score`s as binary\n                matches or non matches. Defaults to 0.5.\n            match_weight_round_to_nearest (float, optional): When provided, thresholds\n                are rounded.  When large numbers of labels are provided, this is\n                sometimes necessary to reduce the size of the ROC table, and therefore\n                the number of points plotted on the ROC chart. Defaults to None.\n        Examples:\n            ```py\n            linker.precision_recall_chart_from_labels_column(\"ground_truth\")\n            ```\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n        \"\"\"\n\n        df_truth_space = truth_space_table_from_labels_column(\n            self,\n            labels_column_name,\n            threshold_actual=threshold_actual,\n            match_weight_round_to_nearest=match_weight_round_to_nearest,\n        )\n        recs = df_truth_space.as_record_dict()\n        return precision_recall_chart(recs)\n\n    def prediction_errors_from_labels_column(\n        self,\n        label_colname,\n        include_false_positives=True,\n        include_false_negatives=True,\n        threshold=0.5,\n    ):\n\"\"\"Generate a dataframe containing false positives and false negatives\n        based on the comparison between the splink match probability and the\n        labels column.  A label column is a column in the input dataset that contains\n        the 'ground truth' cluster to which the record belongs\n\n        Args:\n            label_colname (str): Name of labels column in input data\n            include_false_positives (bool, optional): Defaults to True.\n            include_false_negatives (bool, optional): Defaults to True.\n            threshold (float, optional): Threshold above which a score is considered\n                to be a match. Defaults to 0.5.\n\n        Returns:\n            SplinkDataFrame:  Table containing false positives and negatives\n        \"\"\"\n        return prediction_errors_from_label_column(\n            self,\n            label_colname,\n            include_false_positives,\n            include_false_negatives,\n            threshold,\n        )\n\n    def match_weights_histogram(\n        self, df_predict: SplinkDataFrame, target_bins: int = 30, width=600, height=250\n    ):\n\"\"\"Generate a histogram that shows the distribution of match weights in\n        `df_predict`\n\n        Args:\n            df_predict (SplinkDataFrame): Output of `linker.predict()`\n            target_bins (int, optional): Target number of bins in histogram. Defaults to\n                30.\n            width (int, optional): Width of output. Defaults to 600.\n            height (int, optional): Height of output chart. Defaults to 250.\n\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n\n        \"\"\"\n        df = histogram_data(self, df_predict, target_bins)\n        recs = df.as_record_dict()\n        return match_weights_histogram(recs, width=width, height=height)\n\n    def waterfall_chart(self, records: list[dict], filter_nulls=True):\n\"\"\"Visualise how the final match weight is computed for the provided pairwise\n        record comparisons.\n\n        Records must be provided as a list of dictionaries. This would usually be\n        obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame.\n\n        Examples:\n            ```py\n            df = linker.predict(threshold_match_weight=2)\n            records = df.as_record_dict(limit=10)\n            linker.waterfall_chart(records)\n            ```\n\n        Args:\n            records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)`\n                where `df` is a SplinkDataFrame.\n            filter_nulls (bool, optional): Whether the visualiation shows null\n                comparisons, which have no effect on final match weight. Defaults to\n                True.\n\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        return waterfall_chart(records, self._settings_obj, filter_nulls)\n\n    def unlinkables_chart(\n        self,\n        x_col=\"match_weight\",\n        source_dataset=None,\n        as_dict=False,\n    ):\n\"\"\"Generate an interactive chart displaying the proportion of records that\n        are \"unlinkable\" for a given splink score threshold and model parameters.\n\n        Unlinkable records are those that, even when compared with themselves, do not\n        contain enough information to confirm a match.\n\n        Args:\n            x_col (str, optional): Column to use for the x-axis.\n                Defaults to \"match_weight\".\n            source_dataset (str, optional): Name of the source dataset to use for\n                the title of the output chart.\n            as_dict (bool, optional): If True, return a dict version of the chart.\n\n        Examples:\n            For the simplest code pipeline, load a pre-trained model\n            and run this against the test data.\n            ```py\n            df = pd.read_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\")\n            linker = DuckDBLinker(df)\n            linker.load_settings(\"saved_settings.json\")\n            linker.unlinkables_chart()\n            ```\n            For more complex code pipelines, you can run an entire pipeline\n            that estimates your m and u values, before `unlinkables_chart().\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n        \"\"\"\n\n        # Link our initial df on itself and calculate the % of unlinkable entries\n        records = unlinkables_data(self)\n        return unlinkables_chart(records, x_col, source_dataset, as_dict)\n\n    def comparison_viewer_dashboard(\n        self,\n        df_predict: SplinkDataFrame,\n        out_path: str,\n        overwrite=False,\n        num_example_rows=2,\n        return_html_as_string=False,\n    ):\n\"\"\"Generate an interactive html visualization of the linker's predictions and\n        save to `out_path`.  For more information see\n        [this video](https://www.youtube.com/watch?v=DNvCMqjipis)\n\n\n        Args:\n            df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n            out_path (str): The path (including filename) to save the html file to.\n            overwrite (bool, optional): Overwrite the html file if it already exists?\n                Defaults to False.\n            num_example_rows (int, optional): Number of example rows per comparison\n                vector. Defaults to 2.\n            return_html_as_string: If True, return the html as a string\n\n        Examples:\n            ```py\n            df_predictions = linker.predict()\n            linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n            ```\n\n            Optionally, in Jupyter, you can display the results inline\n            Otherwise you can just load the html file in your browser\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n            ```\n\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        sql = comparison_vector_distribution_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_comparison_vector_distribution\")\n\n        sqls = comparison_viewer_table_sqls(self, num_example_rows)\n        for sql in sqls:\n            self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n        df = self._execute_sql_pipeline([df_predict])\n\n        rendered = render_splink_comparison_viewer_html(\n            df.as_record_dict(),\n            self._settings_obj._as_completed_dict(),\n            out_path,\n            overwrite,\n        )\n        if return_html_as_string:\n            return rendered\n\n    def parameter_estimate_comparisons_chart(self, include_m=True, include_u=True):\n\"\"\"Show a chart that shows how parameter estimates have differed across\n        the different estimation methods you have used.\n\n        For example, if you have run two EM estimation sessions, blocking on\n        different variables, and both result in parameter estimates for\n        first_name, this chart will enable easy comparison of the different\n        estimates\n\n        Args:\n            include_m (bool, optional): Show different estimates of m values. Defaults\n                to True.\n            include_u (bool, optional): Show different estimates of u values. Defaults\n                to True.\n\n        \"\"\"\n        records = self._settings_obj._parameter_estimates_as_records\n\n        to_retain = []\n        if include_m:\n            to_retain.append(\"m\")\n        if include_u:\n            to_retain.append(\"u\")\n\n        records = [r for r in records if r[\"m_or_u\"] in to_retain]\n\n        return parameter_estimate_comparisons(records)\n\n    def missingness_chart(self, input_dataset: str = None):\n\"\"\"Generate a summary chart of the missingness (prevalence of nulls) of\n        columns in the input datasets.  By default, missingness is assessed across\n        all input datasets\n\n        Args:\n            input_dataset (str, optional): Name of one of the input tables in the\n            database.  If provided, missingness will be computed for this table alone.\n            Defaults to None.\n\n        Examples:\n            ```py\n            linker.missingness_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.missingness_chart()\n            save_offline_chart(c.spec, \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500\n            ```\n        \"\"\"\n        records = missingness_data(self, input_dataset)\n        return missingness_chart(records)\n\n    def completeness_chart(self, input_dataset: str = None, cols: list[str] = None):\n\"\"\"Generate a summary chart of the completeness (proportion of non-nulls) of\n        columns in each of the input datasets. By default, completeness is assessed for\n        all column in the input data.\n\n        Args:\n            input_dataset (str, optional): Name of one of the input tables in the\n                database.  If provided, completeness will be computed for this table\n                alone. Defaults to None.\n            cols (List[str], optional): List of column names to calculate completeness.\n                Default to None.\n\n        Examples:\n            ```py\n            linker.completeness_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.completeness_chart()\n            save_offline_chart(c.spec, \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500\n            ```\n        \"\"\"\n        records = completeness_data(self, input_dataset, cols)\n        return completeness_chart(records)\n\n    def count_num_comparisons_from_blocking_rule(\n        self,\n        blocking_rule: str,\n    ) -&gt; int:\n\"\"\"Compute the number of pairwise record comparisons that would be generated by\n        a blocking rule\n\n        Args:\n            blocking_rule (str): The blocking rule to analyse\n            link_type (str, optional): The link type.  This is needed only if the\n                linker has not yet been provided with a settings dictionary.  Defaults\n                to None.\n            unique_id_column_name (str, optional):  This is needed only if the\n                linker has not yet been provided with a settings dictionary.  Defaults\n                to None.\n\n        Examples:\n            ```py\n            br = \"l.first_name = r.first_name\"\n            linker.count_num_comparisons_from_blocking_rule(br)\n            ```\n            &gt; 19387\n            ```py\n            br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            linker.count_num_comparisons_from_blocking_rule(br)\n            ```\n            &gt; 394\n\n        Returns:\n            int: The number of comparisons generated by the blocking rule\n        \"\"\"\n\n        sql = vertically_concatenate_sql(self)\n        self._enqueue_sql(sql, \"__splink__df_concat\")\n\n        sql = number_of_comparisons_generated_by_blocking_rule_sql(self, blocking_rule)\n        self._enqueue_sql(sql, \"__splink__analyse_blocking_rule\")\n        res = self._execute_sql_pipeline().as_record_dict()[0]\n        return res[\"count_of_pairwise_comparisons_generated\"]\n\n    def cumulative_comparisons_from_blocking_rules_records(\n        self,\n        blocking_rules: str or list = None,\n    ):\n\"\"\"Output the number of comparisons generated by each successive blocking rule.\n\n        This is equivalent to the output size of df_predict and details how many\n        comparisons each of your individual blocking rules will contribute to the\n        total.\n\n        Args:\n            blocking_rules (str or list): The blocking rule(s) to compute comparisons\n                for. If null, the rules set out in your settings object will be used.\n\n        Examples:\n            ```py\n            linker_settings = DuckDBLinker(df, settings)\n            # Compute the cumulative number of comparisons generated by the rules\n            # in your settings object.\n            linker_settings.cumulative_comparisons_from_blocking_rules_records()\n            &gt;&gt;&gt;\n            # Generate total comparisons with custom blocking rules.\n            blocking_rules = [\n               \"l.surname = r.surname\",\n               \"l.first_name = r.first_name\n                and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            ]\n            &gt;&gt;&gt;\n            linker_settings.cumulative_comparisons_from_blocking_rules_records(\n                blocking_rules\n             )\n            ```\n\n        Returns:\n            List: A list of blocking rules and the corresponding number of\n                comparisons it is forecast to generate.\n        \"\"\"\n        if blocking_rules:\n            blocking_rules = ensure_is_list(blocking_rules)\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self, blocking_rules, output_chart=False\n        )\n\n        return records\n\n    def cumulative_num_comparisons_from_blocking_rules_chart(\n        self,\n        blocking_rules: str or list = None,\n    ):\n\"\"\"Display a chart with the cumulative number of comparisons generated by a\n        selection of blocking rules.\n\n        This is equivalent to the output size of df_predict and details how many\n        comparisons each of your individual blocking rules will contribute to the\n        total.\n\n        Args:\n            blocking_rules (str or list): The blocking rule(s) to compute comparisons\n                for. If null, the rules set out in your settings object will be used.\n\n        Examples:\n            ```py\n            linker_settings = DuckDBLinker(df, settings)\n            # Compute the cumulative number of comparisons generated by the rules\n            # in your settings object.\n            linker_settings.cumulative_num_comparisons_from_blocking_rules_chart()\n            &gt;&gt;&gt;\n            # Generate total comparisons with custom blocking rules.\n            blocking_rules = [\n               \"l.surname = r.surname\",\n               \"l.first_name = r.first_name\n                and substr(l.dob,1,4) = substr(r.dob,1,4)\"\n            ]\n            &gt;&gt;&gt;\n            linker_settings.cumulative_num_comparisons_from_blocking_rules_chart(\n                blocking_rules\n             )\n            ```\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n        \"\"\"\n\n        if blocking_rules:\n            blocking_rules = ensure_is_list(blocking_rules)\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self, blocking_rules, output_chart=True\n        )\n\n        return cumulative_blocking_rule_comparisons_generated(records)\n\n    def count_num_comparisons_from_blocking_rules_for_prediction(self, df_predict):\n\"\"\"Counts the maginal number of edges created from each of the blocking rules\n        in `blocking_rules_to_generate_predictions`\n\n        This is different to `count_num_comparisons_from_blocking_rule`\n        because it (a) analyses multiple blocking rules rather than a single rule, and\n        (b) deduplicates any comparisons that are generated, to tell you the\n        marginal effect of each entry in `blocking_rules_to_generate_predictions`\n\n        Args:\n            df_predict (SplinkDataFrame): SplinkDataFrame with match weights\n            and probabilities of rows matching\n\n        Examples:\n            ```py\n            linker = DuckDBLinker(df, connection=\":memory:\")\n            linker.load_settings(\"saved_settings.json\")\n            df_predict = linker.predict(threshold_match_probability=0.95)\n            count_pairwise = linker.count_num_comparisons_from_blocking_rules_for_prediction(df_predict)\n            count_pairwise.as_pandas_dataframe(limit=5)\n            ```\n\n        Returns:\n            SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons and\n                estimated pairwise comparisons generated by the blocking rules.\n        \"\"\"  # noqa: E501\n        sql = count_num_comparisons_from_blocking_rules_for_prediction_sql(\n            self, df_predict\n        )\n        match_key_analysis = self._sql_to_splink_dataframe_checking_cache(\n            sql, \"__splink__match_key_analysis\"\n        )\n        return match_key_analysis\n\n    def match_weights_chart(self):\n\"\"\"Display a chart of the (partial) match weights of the linkage model\n\n        Examples:\n            ```py\n            linker.match_weights_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.match_weights_chart()\n            save_offline_chart(c.spec, \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500)\n            ```\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n        \"\"\"\n        return self._settings_obj.match_weights_chart()\n\n    def tf_adjustment_chart(\n        self,\n        output_column_name: str,\n        n_most_freq: int = 10,\n        n_least_freq: int = 10,\n        vals_to_include: str | list = None,\n        as_dict: bool = False,\n    ):\n\"\"\"Display a chart showing the impact of term frequency adjustments on a\n        specific comparison level.\n        Each value\n\n        Args:\n            output_column_name (str): Name of an output column for which term frequency\n                 adjustment has been applied.\n            n_most_freq (int, optional): Number of most frequent values to show. If this\n                 or `n_least_freq` set to None, all values will be shown.\n                Default to 10.\n            n_least_freq (int, optional): Number of least frequent values to show. If\n                this or `n_most_freq` set to None, all values will be shown.\n                Default to 10.\n            vals_to_include (list, optional): Specific values for which to show term\n                sfrequency adjustments.\n                Defaults to None.\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n        \"\"\"\n\n        # Comparisons with TF adjustments\n        tf_comparisons = [\n            c._output_column_name\n            for c in self._settings_obj.comparisons\n            if any([cl._has_tf_adjustments for cl in c.comparison_levels])\n        ]\n        if output_column_name not in tf_comparisons:\n            raise ValueError(\n                f\"{output_column_name} is not a valid comparison column, or does not\"\n                f\" have term frequency adjustment activated\"\n            )\n\n        vals_to_include = ensure_is_list(vals_to_include)\n\n        return tf_adjustment_chart(\n            self,\n            output_column_name,\n            n_most_freq,\n            n_least_freq,\n            vals_to_include,\n            as_dict,\n        )\n\n    def m_u_parameters_chart(self):\n\"\"\"Display a chart of the m and u parameters of the linkage model\n\n        Examples:\n            ```py\n            linker.m_u_parameters_chart()\n            ```\n            To view offline (if you don't have an internet connection):\n            ```py\n            from splink.charts import save_offline_chart\n            c = linker.match_weights_chart()\n            save_offline_chart(c.spec, \"test_chart.html\")\n            ```\n            View resultant html file in Jupyter (or just load it in your browser)\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./test_chart.html\", width=1000, height=500)\n            ```\n\n        Returns:\n            VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n                The vegalite spec is available as a dictionary using the `spec`\n                attribute.\n        \"\"\"\n\n        return self._settings_obj.m_u_parameters_chart()\n\n    def cluster_studio_dashboard(\n        self,\n        df_predict: SplinkDataFrame,\n        df_clustered: SplinkDataFrame,\n        out_path: str,\n        sampling_method=\"random\",\n        sample_size: int = 10,\n        cluster_ids: list = None,\n        cluster_names: list = None,\n        overwrite: bool = False,\n        return_html_as_string=False,\n    ):\n\"\"\"Generate an interactive html visualization of the predicted cluster and\n        save to `out_path`.\n\n        Args:\n            df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n            df_clustered (SplinkDataFrame): The outputs of\n                `linker.cluster_pairwise_predictions_at_threshold()`\n            out_path (str): The path (including filename) to save the html file to.\n            sampling_method (str, optional): `random` or `by_cluster_size`. Defaults to\n                `random`.\n            sample_size (int, optional): Number of clusters to show in the dahboard.\n                Defaults to 10.\n            cluster_ids (list): The IDs of the clusters that will be displayed in the\n                dashboard.  If provided, ignore the `sampling_method` and `sample_size`\n                arguments. Defaults to None.\n            overwrite (bool, optional): Overwrite the html file if it already exists?\n                Defaults to False.\n            cluster_names (list, optional): If provided, the dashboard will display\n                these names in the selection box. Ony works in conjunction with\n                `cluster_ids`.  Defaults to None.\n            return_html_as_string: If True, return the html as a string\n\n        Examples:\n            ```py\n            df_p = linker.predict()\n            df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\n            linker.cluster_studio_dashboard(\n                df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n            )\n            ```\n            Optionally, in Jupyter, you can display the results inline\n            Otherwise you can just load the html file in your browser\n            ```py\n            from IPython.display import IFrame\n            IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n            ```\n        \"\"\"\n        self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n        rendered = render_splink_cluster_studio_html(\n            self,\n            df_predict,\n            df_clustered,\n            out_path,\n            sampling_method=sampling_method,\n            sample_size=sample_size,\n            cluster_ids=cluster_ids,\n            overwrite=overwrite,\n            cluster_names=cluster_names,\n        )\n\n        if return_html_as_string:\n            return rendered\n\n    def save_model_to_json(\n        self, out_path: str | None = None, overwrite: bool = False\n    ) -&gt; dict:\n\"\"\"Save the configuration and parameters of the linkage model to a `.json` file.\n\n        The model can later be loaded back in using `linker.load_model()`.\n        The settings dict is also returned in case you want to save it a different way.\n\n        Examples:\n            ```py\n            linker.save_model_to_json(\"my_settings.json\", overwrite=True)\n            ```\n        Args:\n            out_path (str, optional): File path for json file. If None, don't save to\n                file. Defaults to None.\n            overwrite (bool, optional): Overwrite if already exists? Defaults to False.\n\n        Returns:\n            dict: The settings as a dictionary.\n        \"\"\"\n        model_dict = self._settings_obj.as_dict()\n        if out_path:\n            if os.path.isfile(out_path) and not overwrite:\n                raise ValueError(\n                    f\"The path {out_path} already exists. Please provide a different \"\n                    \"path or set overwrite=True\"\n                )\n            with open(out_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(model_dict, f, indent=4)\n        return model_dict\n\n    def save_settings_to_json(\n        self, out_path: str | None = None, overwrite: bool = False\n    ) -&gt; dict:\n\"\"\"\n        This function is deprecated. Use save_model_to_json() instead.\n        \"\"\"\n        warnings.warn(\n            \"This function is deprecated. Use save_model_to_json() instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return self.save_model_to_json(out_path, overwrite)\n\n    def estimate_probability_two_random_records_match(\n        self, deterministic_matching_rules, recall\n    ):\n\"\"\"Estimate the model parameter `probability_two_random_records_match` using\n        a direct estimation approach.\n\n        See [here](https://github.com/moj-analytical-services/splink/issues/462)\n        for discussion of methodology\n\n        Args:\n            deterministic_matching_rules (list): A list of deterministic matching\n                rules that should be designed to admit very few (none if possible)\n                false positives\n            recall (float): A guess at the recall the deterministic matching rules\n                will attain.  i.e. what proportion of true matches will be recovered\n                by these deterministic rules\n        \"\"\"\n\n        if (recall &gt; 1) or (recall &lt;= 0):\n            raise ValueError(\n                f\"Estimated recall must be greater than 0 \"\n                f\"and no more than 1. Supplied value {recall}.\"\n            )\n\n        # If user, by error, provides a single rule as a string\n        if isinstance(deterministic_matching_rules, str):\n            deterministic_matching_rules = [deterministic_matching_rules]\n\n        records = cumulative_comparisons_generated_by_blocking_rules(\n            self,\n            deterministic_matching_rules,\n        )\n\n        summary_record = records[-1]\n        num_observed_matches = summary_record[\"cumulative_rows\"]\n        num_total_comparisons = summary_record[\"cartesian\"]\n\n        if num_observed_matches &gt; num_total_comparisons * recall:\n            raise ValueError(\n                f\"Deterministic matching rules led to more \"\n                f\"observed matches than is consistent with supplied recall. \"\n                f\"With these rules, recall must be at least \"\n                f\"{num_observed_matches/num_total_comparisons:,.2f}.\"\n            )\n\n        num_expected_matches = num_observed_matches / recall\n        prob = num_expected_matches / num_total_comparisons\n\n        # warn about boundary values, as these will usually be in error\n        if num_observed_matches == 0:\n            logger.warning(\n                f\"WARNING: Deterministic matching rules led to no observed matches! \"\n                f\"This means that no possible record pairs are matches, \"\n                f\"and no records are linked to one another.\\n\"\n                f\"If this is truly the case then you do not need \"\n                f\"to run the linkage model.\\n\"\n                f\"However this is usually in error; \"\n                f\"expected rules to have recall of {100*recall:,.0f}%. \"\n                f\"Consider revising rules as they may have an error.\"\n            )\n        if prob == 1:\n            logger.warning(\n                \"WARNING: Probability two random records match is estimated to be 1.\\n\"\n                \"This means that all possible record pairs are matches, \"\n                \"and all records are linked to one another.\\n\"\n                \"If this is truly the case then you do not need \"\n                \"to run the linkage model.\\n\"\n                \"However, it is more likely that this estimate is faulty. \"\n                \"Perhaps your deterministic matching rules include \"\n                \"too many false positives?\"\n            )\n\n        self._settings_obj._probability_two_random_records_match = prob\n\n        reciprocal_prob = \"Infinity\" if prob == 0 else f\"{1/prob:,.2f}\"\n        logger.info(\n            f\"Probability two random records match is estimated to be  {prob:.3g}.\\n\"\n            f\"This means that amongst all possible pairwise record comparisons, one in \"\n            f\"{reciprocal_prob} are expected to match.  \"\n            f\"With {num_total_comparisons:,.0f} total\"\n            \" possible comparisons, we expect a total of around \"\n            f\"{num_expected_matches:,.2f} matching pairs\"\n        )\n\n    def invalidate_cache(self):\n\"\"\"Invalidate the Splink cache.  Any previously-computed tables\n        will be recomputed.\n        This is useful, for example, if the input data tables have changed.\n        \"\"\"\n        # Before Splink executes a SQL command, it checks the cache to see\n        # whether a table already exists with the name of the output table\n\n        # This function has the effect of changing the names of the output tables\n        # to include a different unique id\n\n        # As a result, any previously cached tables will not be found\n        self._cache_uid = ascii_uid(8)\n\n        # As a result, any previously cached tables will not be found\n        self._intermediate_table_cache.invalidate_cache()\n\n        # Also drop any existing splink tables from the database\n        # Note, this is not actually necessary, it's just good housekeeping\n        self._delete_tables_created_by_splink_from_db()\n\n    def register_table_input_nodes_concat_with_tf(self, input_data, overwrite=False):\n\"\"\"Register a pre-computed version of the input_nodes_concat_with_tf table that\n        you want to re-use e.g. that you created in a previous run\n\n        This method allowed you to register this table in the Splink cache\n        so it will be used rather than Splink computing this table anew.\n\n        Args:\n            input_data: The data you wish to register. This can be either a dictionary,\n                pandas dataframe, pyarrow table or a spark dataframe.\n            overwrite (bool): Overwrite the table in the underlying database if it\n                exists\n        \"\"\"\n\n        table_name_physical = \"__splink__df_concat_with_tf_\" + self._cache_uid\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        self._intermediate_table_cache[\"__splink__df_concat_with_tf\"] = splink_dataframe\n        return splink_dataframe\n\n    def register_table_predict(self, input_data, overwrite=False):\n        table_name_physical = \"__splink__df_predict_\" + self._cache_uid\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        self._intermediate_table_cache[\"__splink__df_predict\"] = splink_dataframe\n        return splink_dataframe\n\n    def register_term_frequency_lookup(self, input_data, col_name, overwrite=False):\n        input_col = InputColumn(col_name, settings_obj=self._settings_obj)\n        table_name_templated = colname_to_tf_tablename(input_col)\n        table_name_physical = f\"{table_name_templated}_{self._cache_uid}\"\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        self._intermediate_table_cache[table_name_templated] = splink_dataframe\n        return splink_dataframe\n\n    def register_labels_table(self, input_data, overwrite=False):\n        table_name_physical = \"__splink__df_labels_\" + ascii_uid(8)\n        splink_dataframe = self.register_table(\n            input_data, table_name_physical, overwrite=overwrite\n        )\n        return splink_dataframe\n</code></pre>","tags":["API","QA","Clusters","Labels"]},{"location":"linkerqa.html#splink.linker.Linker.cluster_studio_dashboard","title":"<code>cluster_studio_dashboard(df_predict, df_clustered, out_path, sampling_method='random', sample_size=10, cluster_ids=None, cluster_names=None, overwrite=False, return_html_as_string=False)</code>","text":"<p>Generate an interactive html visualization of the predicted cluster and save to <code>out_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.predict()</code></p> required <code>df_clustered</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.cluster_pairwise_predictions_at_threshold()</code></p> required <code>out_path</code> <code>str</code> <p>The path (including filename) to save the html file to.</p> required <code>sampling_method</code> <code>str</code> <p><code>random</code> or <code>by_cluster_size</code>. Defaults to <code>random</code>.</p> <code>'random'</code> <code>sample_size</code> <code>int</code> <p>Number of clusters to show in the dahboard. Defaults to 10.</p> <code>10</code> <code>cluster_ids</code> <code>list</code> <p>The IDs of the clusters that will be displayed in the dashboard.  If provided, ignore the <code>sampling_method</code> and <code>sample_size</code> arguments. Defaults to None.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Overwrite the html file if it already exists? Defaults to False.</p> <code>False</code> <code>cluster_names</code> <code>list</code> <p>If provided, the dashboard will display these names in the selection box. Ony works in conjunction with <code>cluster_ids</code>.  Defaults to None.</p> <code>None</code> <code>return_html_as_string</code> <p>If True, return the html as a string</p> <code>False</code> <p>Examples:</p> <p><pre><code>df_p = linker.predict()\ndf_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\nlinker.cluster_studio_dashboard(\n    df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n)\n</code></pre> Optionally, in Jupyter, you can display the results inline Otherwise you can just load the html file in your browser <pre><code>from IPython.display import IFrame\nIFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n</code></pre></p> Source code in <code>splink/linker.py</code> <pre><code>def cluster_studio_dashboard(\n    self,\n    df_predict: SplinkDataFrame,\n    df_clustered: SplinkDataFrame,\n    out_path: str,\n    sampling_method=\"random\",\n    sample_size: int = 10,\n    cluster_ids: list = None,\n    cluster_names: list = None,\n    overwrite: bool = False,\n    return_html_as_string=False,\n):\n\"\"\"Generate an interactive html visualization of the predicted cluster and\n    save to `out_path`.\n\n    Args:\n        df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n        df_clustered (SplinkDataFrame): The outputs of\n            `linker.cluster_pairwise_predictions_at_threshold()`\n        out_path (str): The path (including filename) to save the html file to.\n        sampling_method (str, optional): `random` or `by_cluster_size`. Defaults to\n            `random`.\n        sample_size (int, optional): Number of clusters to show in the dahboard.\n            Defaults to 10.\n        cluster_ids (list): The IDs of the clusters that will be displayed in the\n            dashboard.  If provided, ignore the `sampling_method` and `sample_size`\n            arguments. Defaults to None.\n        overwrite (bool, optional): Overwrite the html file if it already exists?\n            Defaults to False.\n        cluster_names (list, optional): If provided, the dashboard will display\n            these names in the selection box. Ony works in conjunction with\n            `cluster_ids`.  Defaults to None.\n        return_html_as_string: If True, return the html as a string\n\n    Examples:\n        ```py\n        df_p = linker.predict()\n        df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5)\n        linker.cluster_studio_dashboard(\n            df_p, df_c, [0, 4, 7], \"cluster_studio.html\"\n        )\n        ```\n        Optionally, in Jupyter, you can display the results inline\n        Otherwise you can just load the html file in your browser\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200)\n        ```\n    \"\"\"\n    self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n    rendered = render_splink_cluster_studio_html(\n        self,\n        df_predict,\n        df_clustered,\n        out_path,\n        sampling_method=sampling_method,\n        sample_size=sample_size,\n        cluster_ids=cluster_ids,\n        overwrite=overwrite,\n        cluster_names=cluster_names,\n    )\n\n    if return_html_as_string:\n        return rendered\n</code></pre>","tags":["API","QA","Clusters","Labels"]},{"location":"linkerqa.html#splink.linker.Linker.comparison_viewer_dashboard","title":"<code>comparison_viewer_dashboard(df_predict, out_path, overwrite=False, num_example_rows=2, return_html_as_string=False)</code>","text":"<p>Generate an interactive html visualization of the linker's predictions and save to <code>out_path</code>.  For more information see this video</p> <p>Parameters:</p> Name Type Description Default <code>df_predict</code> <code>SplinkDataFrame</code> <p>The outputs of <code>linker.predict()</code></p> required <code>out_path</code> <code>str</code> <p>The path (including filename) to save the html file to.</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite the html file if it already exists? Defaults to False.</p> <code>False</code> <code>num_example_rows</code> <code>int</code> <p>Number of example rows per comparison vector. Defaults to 2.</p> <code>2</code> <code>return_html_as_string</code> <p>If True, return the html as a string</p> <code>False</code> <p>Examples:</p> <pre><code>df_predictions = linker.predict()\nlinker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n</code></pre> <p>Optionally, in Jupyter, you can display the results inline Otherwise you can just load the html file in your browser <pre><code>from IPython.display import IFrame\nIFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n</code></pre></p> Source code in <code>splink/linker.py</code> <pre><code>def comparison_viewer_dashboard(\n    self,\n    df_predict: SplinkDataFrame,\n    out_path: str,\n    overwrite=False,\n    num_example_rows=2,\n    return_html_as_string=False,\n):\n\"\"\"Generate an interactive html visualization of the linker's predictions and\n    save to `out_path`.  For more information see\n    [this video](https://www.youtube.com/watch?v=DNvCMqjipis)\n\n\n    Args:\n        df_predict (SplinkDataFrame): The outputs of `linker.predict()`\n        out_path (str): The path (including filename) to save the html file to.\n        overwrite (bool, optional): Overwrite the html file if it already exists?\n            Defaults to False.\n        num_example_rows (int, optional): Number of example rows per comparison\n            vector. Defaults to 2.\n        return_html_as_string: If True, return the html as a string\n\n    Examples:\n        ```py\n        df_predictions = linker.predict()\n        linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2)\n        ```\n\n        Optionally, in Jupyter, you can display the results inline\n        Otherwise you can just load the html file in your browser\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n        ```\n\n    \"\"\"\n    self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n    sql = comparison_vector_distribution_sql(self)\n    self._enqueue_sql(sql, \"__splink__df_comparison_vector_distribution\")\n\n    sqls = comparison_viewer_table_sqls(self, num_example_rows)\n    for sql in sqls:\n        self._enqueue_sql(sql[\"sql\"], sql[\"output_table_name\"])\n\n    df = self._execute_sql_pipeline([df_predict])\n\n    rendered = render_splink_comparison_viewer_html(\n        df.as_record_dict(),\n        self._settings_obj._as_completed_dict(),\n        out_path,\n        overwrite,\n    )\n    if return_html_as_string:\n        return rendered\n</code></pre>","tags":["API","QA","Clusters","Labels"]},{"location":"linkerqa.html#splink.linker.Linker.m_u_parameters_chart","title":"<code>m_u_parameters_chart()</code>","text":"<p>Display a chart of the m and u parameters of the linkage model</p> <p>Examples:</p> <p><pre><code>linker.m_u_parameters_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.match_weights_chart()\nsave_offline_chart(c.spec, \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500)\n</code></pre></p> <p>Returns:</p> Name Type Description <code>VegaLite</code> <p>A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the <code>spec</code> attribute.</p> Source code in <code>splink/linker.py</code> <pre><code>def m_u_parameters_chart(self):\n\"\"\"Display a chart of the m and u parameters of the linkage model\n\n    Examples:\n        ```py\n        linker.m_u_parameters_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.match_weights_chart()\n        save_offline_chart(c.spec, \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500)\n        ```\n\n    Returns:\n        VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n            The vegalite spec is available as a dictionary using the `spec`\n            attribute.\n    \"\"\"\n\n    return self._settings_obj.m_u_parameters_chart()\n</code></pre>","tags":["API","QA","Clusters","Labels"]},{"location":"linkerqa.html#splink.linker.Linker.match_weights_chart","title":"<code>match_weights_chart()</code>","text":"<p>Display a chart of the (partial) match weights of the linkage model</p> <p>Examples:</p> <p><pre><code>linker.match_weights_chart()\n</code></pre> To view offline (if you don't have an internet connection): <pre><code>from splink.charts import save_offline_chart\nc = linker.match_weights_chart()\nsave_offline_chart(c.spec, \"test_chart.html\")\n</code></pre> View resultant html file in Jupyter (or just load it in your browser) <pre><code>from IPython.display import IFrame\nIFrame(src=\"./test_chart.html\", width=1000, height=500)\n</code></pre></p> <p>Returns:</p> Name Type Description <code>VegaLite</code> <p>A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the <code>spec</code> attribute.</p> Source code in <code>splink/linker.py</code> <pre><code>def match_weights_chart(self):\n\"\"\"Display a chart of the (partial) match weights of the linkage model\n\n    Examples:\n        ```py\n        linker.match_weights_chart()\n        ```\n        To view offline (if you don't have an internet connection):\n        ```py\n        from splink.charts import save_offline_chart\n        c = linker.match_weights_chart()\n        save_offline_chart(c.spec, \"test_chart.html\")\n        ```\n        View resultant html file in Jupyter (or just load it in your browser)\n        ```py\n        from IPython.display import IFrame\n        IFrame(src=\"./test_chart.html\", width=1000, height=500)\n        ```\n\n    Returns:\n        VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n            The vegalite spec is available as a dictionary using the `spec`\n            attribute.\n    \"\"\"\n    return self._settings_obj.match_weights_chart()\n</code></pre>","tags":["API","QA","Clusters","Labels"]},{"location":"linkerqa.html#splink.linker.Linker.parameter_estimate_comparisons_chart","title":"<code>parameter_estimate_comparisons_chart(include_m=True, include_u=True)</code>","text":"<p>Show a chart that shows how parameter estimates have differed across the different estimation methods you have used.</p> <p>For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates</p> <p>Parameters:</p> Name Type Description Default <code>include_m</code> <code>bool</code> <p>Show different estimates of m values. Defaults to True.</p> <code>True</code> <code>include_u</code> <code>bool</code> <p>Show different estimates of u values. Defaults to True.</p> <code>True</code> Source code in <code>splink/linker.py</code> <pre><code>def parameter_estimate_comparisons_chart(self, include_m=True, include_u=True):\n\"\"\"Show a chart that shows how parameter estimates have differed across\n    the different estimation methods you have used.\n\n    For example, if you have run two EM estimation sessions, blocking on\n    different variables, and both result in parameter estimates for\n    first_name, this chart will enable easy comparison of the different\n    estimates\n\n    Args:\n        include_m (bool, optional): Show different estimates of m values. Defaults\n            to True.\n        include_u (bool, optional): Show different estimates of u values. Defaults\n            to True.\n\n    \"\"\"\n    records = self._settings_obj._parameter_estimates_as_records\n\n    to_retain = []\n    if include_m:\n        to_retain.append(\"m\")\n    if include_u:\n        to_retain.append(\"u\")\n\n    records = [r for r in records if r[\"m_or_u\"] in to_retain]\n\n    return parameter_estimate_comparisons(records)\n</code></pre>","tags":["API","QA","Clusters","Labels"]},{"location":"linkerqa.html#splink.linker.Linker.precision_recall_chart_from_labels_column","title":"<code>precision_recall_chart_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a precision-recall chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>linker.precision_recall_chart_from_labels_column(\"ground_truth\")\n</code></pre> <p>Returns:</p> Name Type Description <code>VegaLite</code> <p>A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the <code>spec</code> attribute.</p> Source code in <code>splink/linker.py</code> <pre><code>def precision_recall_chart_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n\"\"\"Generate a precision-recall chart from ground truth data, whereby the ground\n    truth is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n    Examples:\n        ```py\n        linker.precision_recall_chart_from_labels_column(\"ground_truth\")\n        ```\n\n    Returns:\n        VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n            The vegalite spec is available as a dictionary using the `spec`\n            attribute.\n    \"\"\"\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return precision_recall_chart(recs)\n</code></pre>","tags":["API","QA","Clusters","Labels"]},{"location":"linkerqa.html#splink.linker.Linker.precision_recall_chart_from_labels_table","title":"<code>precision_recall_chart_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a precision-recall chart from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered as a table with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSpark <pre><code>labels = pd.read_csv(\"my_labels.csv\")\nlinker.register_table(labels, \"labels\")\nlinker.precision_recall_chart_from_labels_table(\"labels\")\n</code></pre> <pre><code>labels = spark.read.csv(\"my_labels.csv\", header=True)\nlabels.createDataFrame(\"labels\")\nlinker.precision_recall_chart_from_labels_table(\"labels\")\n</code></pre> <p>Returns:</p> Name Type Description <code>VegaLite</code> <p>A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the <code>spec</code> attribute.</p> Source code in <code>splink/linker.py</code> <pre><code>def precision_recall_chart_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n\"\"\"Generate a precision-recall chart from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    as a table with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n    Examples:\n        === \"DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.precision_recall_chart_from_labels_table(\"labels\")\n            ```\n        === \"Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.precision_recall_chart_from_labels_table(\"labels\")\n            ```\n\n    Returns:\n        VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n            The vegalite spec is available as a dictionary using the `spec`\n            attribute.\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return precision_recall_chart(recs)\n</code></pre>","tags":["API","QA","Clusters","Labels"]},{"location":"linkerqa.html#splink.linker.Linker.prediction_errors_from_labels_table","title":"<code>prediction_errors_from_labels_table(labels_splinkdataframe_or_table_name, include_false_positives=True, include_false_negatives=True, threshold=0.5)</code>","text":"<p>Generate a dataframe containing false positives and false negatives based on the comparison between the clerical_match_score in the labels table compared with the splink predicted match probability</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>include_false_positives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>include_false_negatives</code> <code>bool</code> <p>Defaults to True.</p> <code>True</code> <code>threshold</code> <code>float</code> <p>Threshold above which a score is considered to be a match. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Table containing false positives and negatives</p> Source code in <code>splink/linker.py</code> <pre><code>def prediction_errors_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    include_false_positives=True,\n    include_false_negatives=True,\n    threshold=0.5,\n):\n\"\"\"Generate a dataframe containing false positives and false negatives\n    based on the comparison between the clerical_match_score in the labels\n    table compared with the splink predicted match probability\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        include_false_positives (bool, optional): Defaults to True.\n        include_false_negatives (bool, optional): Defaults to True.\n        threshold (float, optional): Threshold above which a score is considered\n            to be a match. Defaults to 0.5.\n\n    Returns:\n        SplinkDataFrame:  Table containing false positives and negatives\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n    return prediction_errors_from_labels_table(\n        self,\n        labels_tablename,\n        include_false_positives,\n        include_false_negatives,\n        threshold,\n    )\n</code></pre>","tags":["API","QA","Clusters","Labels"]},{"location":"linkerqa.html#splink.linker.Linker.roc_chart_from_labels_column","title":"<code>roc_chart_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a ROC chart from ground truth data, whereby the ground truth is in a column in the input dataset called <code>labels_column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>labels_column_name</code> <code>str</code> <p>Column name containing labels in the input table</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>linker.roc_chart_from_labels_column(\"labels\")\n</code></pre> <p>Returns:</p> Name Type Description <code>VegaLite</code> <p>A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the <code>spec</code> attribute.</p> Source code in <code>splink/linker.py</code> <pre><code>def roc_chart_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n\"\"\"Generate a ROC chart from ground truth data, whereby the ground truth\n    is in a column in the input dataset called `labels_column_name`\n\n    Args:\n        labels_column_name (str): Column name containing labels in the input table\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        ```py\n        linker.roc_chart_from_labels_column(\"labels\")\n        ```\n\n    Returns:\n        VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n            The vegalite spec is available as a dictionary using the `spec`\n            attribute.\n    \"\"\"\n\n    df_truth_space = truth_space_table_from_labels_column(\n        self,\n        labels_column_name,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return roc_chart(recs)\n</code></pre>","tags":["API","QA","Clusters","Labels"]},{"location":"linkerqa.html#splink.linker.Linker.roc_chart_from_labels_table","title":"<code>roc_chart_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate a ROC chart from labelled (ground truth) data.</p> <p>The table of labels should be in the following format, and should be registered with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSpark <pre><code>labels = pd.read_csv(\"my_labels.csv\")\nlinker.register_table(labels, \"labels\")\nlinker.roc_chart_from_labels_table(\"labels\")\n</code></pre> <pre><code>labels = spark.read.csv(\"my_labels.csv\", header=True)\nlabels.createDataFrame(\"labels\")\nlinker.roc_chart_from_labels_table(\"labels\")\n</code></pre> <p>Returns:</p> Name Type Description <code>VegaLite</code> <p>A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the <code>spec</code> attribute.</p> Source code in <code>splink/linker.py</code> <pre><code>def roc_chart_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name: str | SplinkDataFrame,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n\"\"\"Generate a ROC chart from labelled (ground truth) data.\n\n    The table of labels should be in the following format, and should be registered\n    with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        === \"DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.roc_chart_from_labels_table(\"labels\")\n            ```\n        === \"Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.roc_chart_from_labels_table(\"labels\")\n            ```\n\n    Returns:\n        VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n            The vegalite spec is available as a dictionary using the `spec`\n            attribute.\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    df_truth_space = truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n    recs = df_truth_space.as_record_dict()\n    return roc_chart(recs)\n</code></pre>","tags":["API","QA","Clusters","Labels"]},{"location":"linkerqa.html#splink.linker.Linker.truth_space_table_from_labels_column","title":"<code>truth_space_table_from_labels_column(labels_column_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart.</p> <p>Your labels_column_name should include the ground truth cluster (unique identifier) that groups entities which are the same</p> <p>Parameters:</p> Name Type Description Default <code>labels_tablename</code> <code>str</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>linker.truth_space_table_from_labels_column(\"cluster\")\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <p>Table of truth statistics</p> Source code in <code>splink/linker.py</code> <pre><code>def truth_space_table_from_labels_column(\n    self,\n    labels_column_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n):\n\"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n    match_probability, suitable for plotting a ROC chart.\n\n    Your labels_column_name should include the ground truth cluster (unique\n    identifier) that groups entities which are the same\n\n    Args:\n        labels_tablename (str): Name of table containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        ```py\n        linker.truth_space_table_from_labels_column(\"cluster\")\n        ```\n\n    Returns:\n        SplinkDataFrame:  Table of truth statistics\n    \"\"\"\n\n    return truth_space_table_from_labels_column(\n        self, labels_column_name, threshold_actual, match_weight_round_to_nearest\n    )\n</code></pre>","tags":["API","QA","Clusters","Labels"]},{"location":"linkerqa.html#splink.linker.Linker.truth_space_table_from_labels_table","title":"<code>truth_space_table_from_labels_table(labels_splinkdataframe_or_table_name, threshold_actual=0.5, match_weight_round_to_nearest=None)</code>","text":"<p>Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart.</p> <p>The table of labels should be in the following format, and should be registered with your database:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 <p>Note that <code>source_dataset</code> and <code>unique_id</code> should correspond to the values specified in the settings dict, and the <code>input_table_aliases</code> passed to the <code>linker</code> object.</p> <p>For <code>dedupe_only</code> links, the <code>source_dataset</code> columns can be ommitted.</p> <p>Parameters:</p> Name Type Description Default <code>labels_splinkdataframe_or_table_name</code> <code>str | SplinkDataFrame</code> <p>Name of table containing labels in the database</p> required <code>threshold_actual</code> <code>float</code> <p>Where the <code>clerical_match_score</code> provided by the user is a probability rather than binary, this value is used as the threshold to classify <code>clerical_match_score</code>s as binary matches or non matches. Defaults to 0.5.</p> <code>0.5</code> <code>match_weight_round_to_nearest</code> <code>float</code> <p>When provided, thresholds are rounded.  When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None.</p> <code>None</code> <p>Examples:</p> DuckDBSpark <pre><code>labels = pd.read_csv(\"my_labels.csv\")\nlinker.register_table(labels, \"labels\")\nlinker.truth_space_table_from_labels_table(\"labels\")\n</code></pre> <pre><code>labels = spark.read.csv(\"my_labels.csv\", header=True)\nlabels.createDataFrame(\"labels\")\nlinker.truth_space_table_from_labels_table(\"labels\")\n</code></pre> <p>Returns:</p> Name Type Description <code>SplinkDataFrame</code> <code>SplinkDataFrame</code> <p>Table of truth statistics</p> Source code in <code>splink/linker.py</code> <pre><code>def truth_space_table_from_labels_table(\n    self,\n    labels_splinkdataframe_or_table_name,\n    threshold_actual=0.5,\n    match_weight_round_to_nearest: float = None,\n) -&gt; SplinkDataFrame:\n\"\"\"Generate truth statistics (false positive etc.) for each threshold value of\n    match_probability, suitable for plotting a ROC chart.\n\n    The table of labels should be in the following format, and should be registered\n    with your database:\n\n    |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score|\n    |----------------|-----------|----------------|-----------|--------------------|\n    |df_1            |1          |df_2            |2          |0.99                |\n    |df_1            |1          |df_2            |3          |0.2                 |\n\n    Note that `source_dataset` and `unique_id` should correspond to the values\n    specified in the settings dict, and the `input_table_aliases` passed to the\n    `linker` object.\n\n    For `dedupe_only` links, the `source_dataset` columns can be ommitted.\n\n    Args:\n        labels_splinkdataframe_or_table_name (str | SplinkDataFrame): Name of table\n            containing labels in the database\n        threshold_actual (float, optional): Where the `clerical_match_score`\n            provided by the user is a probability rather than binary, this value\n            is used as the threshold to classify `clerical_match_score`s as binary\n            matches or non matches. Defaults to 0.5.\n        match_weight_round_to_nearest (float, optional): When provided, thresholds\n            are rounded.  When large numbers of labels are provided, this is\n            sometimes necessary to reduce the size of the ROC table, and therefore\n            the number of points plotted on the ROC chart. Defaults to None.\n\n    Examples:\n        === \"DuckDB\"\n            ```py\n            labels = pd.read_csv(\"my_labels.csv\")\n            linker.register_table(labels, \"labels\")\n            linker.truth_space_table_from_labels_table(\"labels\")\n            ```\n        === \"Spark\"\n            ```py\n            labels = spark.read.csv(\"my_labels.csv\", header=True)\n            labels.createDataFrame(\"labels\")\n            linker.truth_space_table_from_labels_table(\"labels\")\n            ```\n    Returns:\n        SplinkDataFrame:  Table of truth statistics\n    \"\"\"\n    labels_tablename = self._get_labels_tablename_from_input(\n        labels_splinkdataframe_or_table_name\n    )\n\n    self._raise_error_if_necessary_accuracy_columns_not_computed()\n    return truth_space_table_from_labels_table(\n        self,\n        labels_tablename,\n        threshold_actual=threshold_actual,\n        match_weight_round_to_nearest=match_weight_round_to_nearest,\n    )\n</code></pre>","tags":["API","QA","Clusters","Labels"]},{"location":"linkerqa.html#splink.linker.Linker.unlinkables_chart","title":"<code>unlinkables_chart(x_col='match_weight', source_dataset=None, as_dict=False)</code>","text":"<p>Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters.</p> <p>Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match.</p> <p>Parameters:</p> Name Type Description Default <code>x_col</code> <code>str</code> <p>Column to use for the x-axis. Defaults to \"match_weight\".</p> <code>'match_weight'</code> <code>source_dataset</code> <code>str</code> <p>Name of the source dataset to use for the title of the output chart.</p> <code>None</code> <code>as_dict</code> <code>bool</code> <p>If True, return a dict version of the chart.</p> <code>False</code> <p>Examples:</p> <p>For the simplest code pipeline, load a pre-trained model and run this against the test data. <pre><code>df = pd.read_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\")\nlinker = DuckDBLinker(df)\nlinker.load_settings(\"saved_settings.json\")\nlinker.unlinkables_chart()\n</code></pre> For more complex code pipelines, you can run an entire pipeline that estimates your m and u values, before `unlinkables_chart().</p> <p>Returns:</p> Name Type Description <code>VegaLite</code> <p>A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the <code>spec</code> attribute.</p> Source code in <code>splink/linker.py</code> <pre><code>def unlinkables_chart(\n    self,\n    x_col=\"match_weight\",\n    source_dataset=None,\n    as_dict=False,\n):\n\"\"\"Generate an interactive chart displaying the proportion of records that\n    are \"unlinkable\" for a given splink score threshold and model parameters.\n\n    Unlinkable records are those that, even when compared with themselves, do not\n    contain enough information to confirm a match.\n\n    Args:\n        x_col (str, optional): Column to use for the x-axis.\n            Defaults to \"match_weight\".\n        source_dataset (str, optional): Name of the source dataset to use for\n            the title of the output chart.\n        as_dict (bool, optional): If True, return a dict version of the chart.\n\n    Examples:\n        For the simplest code pipeline, load a pre-trained model\n        and run this against the test data.\n        ```py\n        df = pd.read_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\")\n        linker = DuckDBLinker(df)\n        linker.load_settings(\"saved_settings.json\")\n        linker.unlinkables_chart()\n        ```\n        For more complex code pipelines, you can run an entire pipeline\n        that estimates your m and u values, before `unlinkables_chart().\n\n    Returns:\n        VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n            The vegalite spec is available as a dictionary using the `spec`\n            attribute.\n    \"\"\"\n\n    # Link our initial df on itself and calculate the % of unlinkable entries\n    records = unlinkables_data(self)\n    return unlinkables_chart(records, x_col, source_dataset, as_dict)\n</code></pre>","tags":["API","QA","Clusters","Labels"]},{"location":"linkerqa.html#splink.linker.Linker.waterfall_chart","title":"<code>waterfall_chart(records, filter_nulls=True)</code>","text":"<p>Visualise how the final match weight is computed for the provided pairwise record comparisons.</p> <p>Records must be provided as a list of dictionaries. This would usually be obtained from <code>df.as_record_dict(limit=n)</code> where <code>df</code> is a SplinkDataFrame.</p> <p>Examples:</p> <pre><code>df = linker.predict(threshold_match_weight=2)\nrecords = df.as_record_dict(limit=10)\nlinker.waterfall_chart(records)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>records</code> <code>List[dict]</code> <p>Usually be obtained from <code>df.as_record_dict(limit=n)</code> where <code>df</code> is a SplinkDataFrame.</p> required <code>filter_nulls</code> <code>bool</code> <p>Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>VegaLite</code> <p>A VegaLite chart object. See altair.vegalite.v4.display.VegaLite. The vegalite spec is available as a dictionary using the <code>spec</code> attribute.</p> Source code in <code>splink/linker.py</code> <pre><code>def waterfall_chart(self, records: list[dict], filter_nulls=True):\n\"\"\"Visualise how the final match weight is computed for the provided pairwise\n    record comparisons.\n\n    Records must be provided as a list of dictionaries. This would usually be\n    obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame.\n\n    Examples:\n        ```py\n        df = linker.predict(threshold_match_weight=2)\n        records = df.as_record_dict(limit=10)\n        linker.waterfall_chart(records)\n        ```\n\n    Args:\n        records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)`\n            where `df` is a SplinkDataFrame.\n        filter_nulls (bool, optional): Whether the visualiation shows null\n            comparisons, which have no effect on final match weight. Defaults to\n            True.\n\n\n    Returns:\n        VegaLite: A VegaLite chart object. See altair.vegalite.v4.display.VegaLite.\n            The vegalite spec is available as a dictionary using the `spec`\n            attribute.\n\n    \"\"\"\n    self._raise_error_if_necessary_waterfall_columns_not_computed()\n\n    return waterfall_chart(records, self._settings_obj, filter_nulls)\n</code></pre>","tags":["API","QA","Clusters","Labels"]},{"location":"settings_dict_guide.html","title":"Settings dictionary reference","text":"","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#guide-to-splink-settings","title":"Guide to Splink settings","text":"<p>This document enumerates all the settings and configuration options available when developing your data linkage model.</p> <p>You can find an interative settings editor here.</p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#settings-keys-in-the-base-setting-dictionary","title":"Settings keys in the base setting dictionary","text":"","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#link_type","title":"link_type","text":"<p>The type of data linking task.  Required.</p> <ul> <li> <p>When <code>dedupe_only</code>, <code>splink</code> find duplicates.  User expected to provide a single input dataset.</p> </li> <li> <p>When <code>link_and_dedupe</code>, <code>splink</code> finds links within and between input datasets.  User is expected to provide two or more input datasets.</p> </li> <li> <p>When <code>link_only</code>,  <code>splink</code> finds links between datasets, but does not attempt to deduplicate the datasets (it does not try and find links within each input dataset.) User is expected to provide two or more input datasets.</p> </li> </ul> <p>Examples: <code>['dedupe_only', 'link_only', 'link_and_dedupe']</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#probability_two_random_records_match","title":"probability_two_random_records_match","text":"<p>The probability that two records chosen at random (with no blocking) are a match.  For example, if there are a million input records and each has on average one match, then this value should be 1/1,000,000.</p> <p>If you estimate parameters using expectation maximisation (EM), this provides an initial value (prior) from which the EM algorithm will start iterating.  EM will then estimate the true value of this parameter.</p> <p>Default value: <code>0.0001</code></p> <p>Examples: <code>[1e-05, 0.006]</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#em_convergence","title":"em_convergence","text":"<p>Convergence tolerance for the Expectation Maximisation algorithm</p> <p>The algorithm will stop converging when the maximum of the change in model parameters between iterations is below this value</p> <p>Default value: <code>0.0001</code></p> <p>Examples: <code>[0.0001, 1e-05, 1e-06]</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#max_iterations","title":"max_iterations","text":"<p>The maximum number of Expectation Maximisation iterations to run (even if convergence has not been reached)</p> <p>Default value: <code>25</code></p> <p>Examples: <code>[20, 150]</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#unique_id_column_name","title":"unique_id_column_name","text":"<p>Splink requires that the input dataset has a column that uniquely identifies each reecord.  <code>unique_id_column_name</code> is the name of the column in the input dataset representing this unique id</p> <p>For linking tasks, ids must be unique within each dataset being linked, and do not need to be globally unique across input datasets</p> <p>Default value: <code>unique_id</code></p> <p>Examples: <code>['unique_id', 'id', 'pk']</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#source_dataset_column_name","title":"source_dataset_column_name","text":"<p>The name of the column in the input dataset representing the source dataset</p> <p>Where we are linking datasets, we can't guarantee that the unique id column is globally unique across datasets, so we combine it with a source_dataset column.  Usually, this is created by Splink for the user</p> <p>Default value: <code>source_dataset</code></p> <p>Examples: <code>['source_dataset', 'dataset_name']</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#retain_matching_columns","title":"retain_matching_columns","text":"<p>If set to true, each column used by the <code>comparisons</code> sql expressions will be retained in output datasets</p> <p>This is helpful so that the user can inspect matches, but once the comparison vector (gamma) columns are computed, this information is not actually needed by the algorithm.  The algorithm will run faster and use less resources if this is set to false.</p> <p>Default value: <code>True</code></p> <p>Examples: <code>[False, True]</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#retain_intermediate_calculation_columns","title":"retain_intermediate_calculation_columns","text":"<p>Retain intermediate calculation columns, such as the bayes factors associated with each column in <code>comparisons</code></p> <p>The algorithm will run faster and use less resources if this is set to false.</p> <p>Default value: <code>False</code></p> <p>Examples: <code>[False, True]</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#comparisons","title":"comparisons","text":"<p>A list specifying how records should be compared for probabalistic matching.  Each element is a dictionary</p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#blocking_rules_to_generate_predictions","title":"blocking_rules_to_generate_predictions","text":"<p>A list of one or more blocking rules to apply. A cartesian join is applied if <code>blocking_rules_to_generate_predictions</code> is empty or not supplied.</p> <p>Each rule is a SQL expression representing the blocking rule, which will be used to create a join.  The left table is aliased with <code>l</code> and the right table is aliased with <code>r</code>. For example, if you want to block on a <code>first_name</code> column, the blocking rule would be</p> <p><code>l.first_name = r.first_name</code>.</p> <p>To block on first name and the first letter of surname, it would be</p> <p><code>l.first_name = r.first_name and substr(l.surname,1,1) = substr(r.surname,1,1)</code>.</p> <p>Note that splink deduplicates the comparisons generated by the blocking rules.</p> <p>If empty or not supplied, all comparisons between the input dataset(s) will be generated and blocking will not be used. For large input datasets, this will generally be computationally intractable because it will generate comparisons equal to the number of rows squared.</p> <p>Default value: <code>[]</code></p> <p>Examples: <code>[['l.first_name = r.first_name AND l.surname = r.surname', 'l.dob = r.dob']]</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#additional_columns_to_retain","title":"additional_columns_to_retain","text":"<p>A list of columns not being used in the probabalistic matching comparisons that you want to include in your results.</p> <p>By default, splink drops columns which are not used by any comparisons.  This gives you the option to retain columns which are not used by the model.  A common example is if the user has labelled data (training data) and wishes to retain the labels in the outputs</p> <p>Default value: <code>[]</code></p> <p>Examples: <code>[['cluster', 'col_2'], ['other_information']]</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#bayes_factor_column_prefix","title":"bayes_factor_column_prefix","text":"<p>The prefix to use for the columns that will be created to store the bayes factors</p> <p>Default value: <code>bf_</code></p> <p>Examples: <code>['bf_', '__bf__']</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#term_frequency_adjustment_column_prefix","title":"term_frequency_adjustment_column_prefix","text":"<p>The prefix to use for the columns that will be created to store the term frequency adjustments</p> <p>Default value: <code>tf_</code></p> <p>Examples: <code>['tf_', '__tf__']</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#comparison_vector_value_column_prefix","title":"comparison_vector_value_column_prefix","text":"<p>The prefix to use for the columns that will be created to store the comparison vector values</p> <p>Default value: <code>gamma_</code></p> <p>Examples: <code>['gamma_', '__gamma__']</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#sql_dialect","title":"sql_dialect","text":"<p>The SQL dialect in which sql_conditions are written.  Must be a valid sqlglot dialect</p> <p>Default value: <code>None</code></p> <p>Examples: <code>['spark', 'duckdb', 'presto', 'sqlite']</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#settings-keys-nested-within-each-member-of-comparisons","title":"Settings keys nested within each member of <code>comparisons</code>","text":"","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#output_column_name","title":"output_column_name","text":"<p>The name used to refer to this comparison in the output dataset.  By default, Splink will set this to the name(s) of any input columns used in the comparison.  This key is most useful to give a clearer description to comparisons that use multiple input columns.  e.g. a location column that uses postcode and town may be named location</p> <p>For a comparison column that uses a single input column, e.g. first_name, this will be set first_name. For comparison columns that use multiple columns, if left blank, this will be set to the concatenation of columns used.</p> <p>Examples: <code>['first_name', 'surname']</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#comparison_description","title":"comparison_description","text":"<p>An optional label to describe this comparison, to be used in charting outputs.</p> <p>Examples: <code>['First name exact match', 'Surname with middle levenshtein level']</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#comparison_levels","title":"comparison_levels","text":"<p>Comparison levels specify how input values should be compared.  Each level corresponds to an assessment of similarity, such as exact match, jaro winkler match, one side of the match being null, etc</p> <p>Each comparison level represents a branch of a SQL case expression. They are specified in order of evaluation, each with a sql_condition that represents the branch of a case expression</p> <p>Example:  <pre><code>[{\n\"sql_condition\": \"first_name_l IS NULL OR first_name_r IS NULL\", \"label_for_charts\": \"null\", \"null_level\": True\n}, {\n\"sql_condition\": \"first_name_l = first_name_r\", \"label_for_charts\": \"exact_match\", \"tf_adjustment_column\": \"first_name\"\n}, {\n\"sql_condition\": \"ELSE\", \"label_for_charts\": \"else\"\n}]\n</code></pre></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#settings-keys-nested-within-each-member-of-comparison_levels","title":"Settings keys nested within each member of <code>comparison_levels</code>","text":"","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#sql_condition","title":"sql_condition","text":"<p>A branch of a SQL case expression without WHEN and THEN e.g. 'jaro_winkler_sim(surname_l, surname_r) &gt; 0.88'</p> <p>Examples: <code>['forename_l = forename_r', 'jaro_winkler_sim(surname_l, surname_r) &gt; 0.88']</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#label_for_charts","title":"label_for_charts","text":"<p>A label for this comparson level, which will appear on charts as a reminder of what the level represents</p> <p>Examples: <code>['exact', 'postcode exact']</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#u_probability","title":"u_probability","text":"<p>the u probability for this comparison level - i.e. the proportion of records that match this level amongst truly non-matching records</p> <p>Examples: <code>[0.9]</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#m_probability","title":"m_probability","text":"<p>the m probability for this comparison level - i.e. the proportion of records that match this level amongst truly matching records</p> <p>Examples: <code>[0.1]</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#is_null_level","title":"is_null_level","text":"<p>If true, m and u values will not be estimated and instead the match weight will be zero for this column.  See treatment of nulls here on page 356, quote '. Under this MAR assumption, we can simply ignore missing data.': https://imai.fas.harvard.edu/research/files/linkage.pdf</p> <p>Default value: <code>False</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#tf_adjustment_column","title":"tf_adjustment_column","text":"<p>Make term frequency adjustments for this comparison level using this input column</p> <p>Default value: <code>None</code></p> <p>Examples: <code>['first_name', 'postcode']</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#tf_adjustment_weight","title":"tf_adjustment_weight","text":"<p>Make term frequency adjustments using this weight. A weight of 1.0 is a full adjustment.  A weight of 0.0 is no adjustment.  A weight of 0.5 is a half adjustment</p> <p>Default value: <code>1.0</code></p> <p>Examples: <code>['first_name', 'postcode']</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"settings_dict_guide.html#tf_minimum_u_value","title":"tf_minimum_u_value","text":"<p>Where the term frequency adjustment implies a u value below this value, use this minimum value instead</p> <p>This prevents excessive weight being assigned to very unusual terms, such as a collision on a typo</p> <p>Default value: <code>0.0</code></p> <p>Examples: <code>[0.001, 1e-09]</code></p>","tags":["settings","Dedupe","Link","Link and Dedupe","Expectation Maximisation","Comparisons","Blocking Rules"]},{"location":"demos/index.html","title":"splink_demos","text":"<p>This repo contains interactive notebooks containing demonstration and tutorial for version 3 of the Splink record linking library, the homepage for which is here.</p>"},{"location":"demos/index.html#running-these-notebooks-interactively","title":"Running these notebooks interactively","text":"<p>You can run these notebooks in an interactive Jupyter notebook by clicking the button below:</p> <p></p>"},{"location":"demos/index.html#running-these-notebooks-locally-in-vscode","title":"Running these notebooks locally in VSCode","text":"<p>If you don't already have it, you'll need to install java on your system in order to run <code>pyspark</code>, which splink currently depends on. Download java for your specific OS from here.</p> <p>You can check the installation went correctly by using:</p> <p><code>java -version</code> within a terminal instance. It should return details of your java installation.</p> <p>If you have multiple java installations, you may need to change the version of java you're currently using.</p> <p>To download the example notebooks, simply clone this repository:</p> <pre><code>git clone git@github.com:moj-analytical-services/splink_demos.git\n</code></pre> <p>Create a virtual environment using:</p> <pre><code>python3 -m venv venv\nsource venv/bin/activate\n</code></pre> <p>Install the package list (which includes <code>pyspark</code>) with:</p> <pre><code>pip3 install -r requirements.txt\n</code></pre> <p>and, if you want to use jupyter, add a kernel corresopnding to your venv:</p> <pre><code>python -m ipykernel install --user --name=splink_demos\njupyter lab\n</code></pre>"},{"location":"demos/00_Tutorial_Introduction.html","title":"0. Tutorial introduction","text":""},{"location":"demos/00_Tutorial_Introduction.html#introductory-tutorial","title":"Introductory tutorial","text":"<p>This is the introduction to a five part tutorial which demonstrates how to de-duplicate a small dataset using simple settings.</p> <p>The aim of the tutorial is to demonstrate core Splink functionality succinctly, rather that comprehensively document all configuration options.</p> <p>The seven parts are:</p> <ul> <li> <p>1. Data prep pre-requisites</p> </li> <li> <p>2. Exploratory analysis</p> </li> <li> <p>3. Choosing blocking rules to optimise runtimes</p> </li> <li> <p>4. Estimating model parameters</p> </li> <li> <p>5. Predicting results</p> </li> <li> <p>6. Visualising predictions</p> </li> <li> <p>7. Quality assurance</p> </li> </ul> <p>Throughout the tutorial, we use the duckdb backend, which is the recommended option for smaller datasets of up to around 1 million records on a normal laptop.</p> <p>You can find these tutorial notebooks in the <code>splink_demos</code> repo, and you can run them live in your web browser by clicking the following link:</p> <p></p>"},{"location":"demos/00_Tutorial_Introduction.html#end-to-end-demos","title":"End-to-end demos","text":"<p>After following the steps of the tutorial, it might prove useful to have a look at some of the example notebooks that show various use-case scenarios of Splink from start to finish.</p>"},{"location":"demos/01_Prerequisites.html","title":"1. Data prep prerequisites","text":"<p>Splink requires that you clean your data and assign unique IDs to rows before linking. </p> <p>This section outlines the additional data cleaning steps needed before loading data into Splink.</p>"},{"location":"demos/01_Prerequisites.html#data-prerequisites","title":"Data Prerequisites","text":""},{"location":"demos/01_Prerequisites.html#unique-ids","title":"Unique IDs","text":"<ul> <li>Each input dataset must have a unique ID column, which is unique within the dataset.  By default, Splink assumes this column will be called <code>unique_id</code>, but this can be changed with the <code>unique_id_column_name</code> key in your Splink settings.  The unique id is essential because it enables Splink to keep track each row correctly. </li> </ul>"},{"location":"demos/01_Prerequisites.html#conformant-input-datasets","title":"Conformant input datasets","text":"<ul> <li>Input datasets must be conformant, meaning they share the same column names and data formats. For instance, if one dataset has a \"date of birth\" column and another has a \"dob\" column, rename them to match. Ensure data type and number formatting are consistent across both columns. The order of columns in input dataframes is not important.</li> </ul>"},{"location":"demos/01_Prerequisites.html#cleaning","title":"Cleaning","text":"<ul> <li>Ensure data consistency by cleaning your data. This process includes standardizing date formats, matching text case, and handling invalid data. For example, if one dataset uses \"yyyy-mm-dd\" date format and another uses \"mm/dd/yyyy,\" convert them to the same format before using Splink.  Try also to identify and rectify any obvious data entry errors, such as removing values such as 'Mr' or 'Mrs' from a 'first name' column.</li> </ul>"},{"location":"demos/01_Prerequisites.html#ensure-nulls-are-consistently-and-correctly-represented","title":"Ensure nulls are consistently and correctly represented","text":"<ul> <li>Ensure null values (or other 'not known' indicators) are represented as true nulls, not empty strings. Splink treats null values differently from empty strings, so using true nulls guarantees proper matching across datasets.</li> </ul>"},{"location":"demos/01_Prerequisites.html#further-details-on-data-cleaning-and-standardisation","title":"Further details on data cleaning and standardisation","text":"<p>Splink performs optimally with cleaned and standardized data. Here is a non-exhaustive list of suggestions for data cleaning rules to enhance matching accuracy:</p> <ul> <li>Trim leading and trailing whitespace from string values (e.g., \" john smith \" becomes \"john smith\").</li> <li>Remove special characters from string values (e.g., \"O'Hara\" becomes \"Ohara\").</li> <li>Standardise date formats as strings in \"yyyy-mm-dd\" format.</li> <li>Replace abbreviations with full words (e.g., standardize \"St.\" and \"Street\" to \"Street\").</li> </ul>"},{"location":"demos/02_Exploratory_analysis.html","title":"2. Exploratory analysis","text":"<pre><code>import pandas as pd \nimport altair as alt\nalt.renderers.enable('mimetype')\n\ndf = pd.read_csv(\"./data/fake_1000.csv\")\ndf.head(5)\n</code></pre> unique_id first_name surname dob city email cluster 0 0 Robert Alan 1971-06-24 NaN robert255@smith.net 0 1 1 Robert Allen 1971-05-24 NaN roberta25@smith.net 0 2 2 Rob Allen 1971-06-24 London roberta25@smith.net 0 3 3 Robert Alen 1971-06-24 Lonon NaN 0 4 4 Grace NaN 1997-04-26 Hull grace.kelly52@jones.com 1 <pre><code># Initialise the linker, passing in the input dataset(s)\nfrom splink.duckdb.duckdb_linker import DuckDBLinker\nlinker = DuckDBLinker(df)\n</code></pre> <p>It's important to understand the level of missingness in your data, because columns with higher levels of missingness are less useful for data linking.</p> <pre><code>linker.missingness_chart()\n</code></pre> <p>The above summary chart shows that in this dataset, the <code>email</code>, <code>city</code>, <code>surname</code> and <code>forename</code> columns contain nulls, but the level of missingness is relatively low (less than 22%).</p> <p>The distribution of values in your data is important for two main reasons:</p> <ol> <li> <p>Columns with higher cardinality (number of distinct values) are usually more useful for data linking.  For instance, date of birth is a much stronger linkage variable than gender.</p> </li> <li> <p>The skew of values is important.  If you have a <code>city</code> column that has 1,000 distinct values, but 75% of them are <code>London</code>, this is much less useful for linkage than if the 1,000 values were equally distributed</p> </li> </ol> <p>The <code>linker.profile_columns()</code> method creates summary charts to help you understand these aspects of your data. </p> <p>You may input column names (e.g. <code>first_name</code>), or arbitrary sql expressions like <code>concat(first_name, surname)</code>.</p> <pre><code>linker.profile_columns([\"first_name\", \"city\", \"surname\", \"email\", \"substr(dob, 1,4)\"], top_n=10, bottom_n=5)\n</code></pre> <p>This chart is very information-dense, but here are some key takehomes relevant to our linkage:</p> <ul> <li> <p>There is strong skew in the <code>city</code> field with around 20% of the values being <code>London</code>.  We therefore will probably want to use <code>term_frequency_adjustments</code> in our linkage model, so that it can weight a match on London differently to a match on, say, <code>Norwich</code>.</p> </li> <li> <p>Looking at the \"Bottom 5 values by value count\", we can see typos in the data in most fields.  This tells us this information was possibly entered by hand, or using Optical Character Recognition, giving us an insight into the type of data entry errors we may see.</p> </li> <li> <p>Email is a much more uniquely-identifying field than any others, with a maximum value count of 6.  It's likely to be a strong linking variable.</p> </li> </ul>"},{"location":"demos/02_Exploratory_analysis.html#exploratory-analysis","title":"Exploratory analysis","text":"<p>The purpose of exploratory analysis is to understand your data and any idiosyncrasies which may be relevant to the task of data linking.</p> <p>Splink includes functionality to visualise and summarise your data, to identify characteristics most salient to data linking.</p> <p>In this notebook we perform some basic exploratory analysis, and interpret the results.</p>"},{"location":"demos/02_Exploratory_analysis.html#read-in-the-data","title":"Read in the data","text":"<p>For the purpose of this tutorial we will use a 1,000 row synthetic dataset that contains duplicates.</p> <p>The first five rows of this dataset are printed below.</p> <p>Note that the cluster column represents the 'ground truth' - a column which tells us with which rows refer to the same person. In most real linkage scenarios, we wouldn't have this column (this is what Splink is trying to estimate.)</p>"},{"location":"demos/02_Exploratory_analysis.html#instantiate-the-linker","title":"Instantiate the linker","text":"<p>Most of Splink's core functionality can be accessed as methods on a linker object.  For example, to make predictions, you would call <code>linker.predict()</code>.</p> <p>We therefore begin by instantiating the linker, passing in the data we wish to deduplicate.</p>"},{"location":"demos/02_Exploratory_analysis.html#analyse-missingness","title":"Analyse missingness","text":""},{"location":"demos/02_Exploratory_analysis.html#analyse-the-distribution-of-values-in-your-data","title":"Analyse the distribution of values in your data","text":""},{"location":"demos/02_Exploratory_analysis.html#next-steps","title":"Next steps","text":"<p>At this point, we have begin to develop a strong understanding of our data.  It's time to move on to estimating a linkage model</p>"},{"location":"demos/02_Exploratory_analysis.html#further-reading","title":"Further reading","text":"<p>You can find the documentation for the exploratory analysis tools in Splink here</p>"},{"location":"demos/03_Blocking.html","title":"3. Blocking","text":"<pre><code>import pandas as pd \nimport altair as alt\nalt.renderers.enable('mimetype')\n\ndf = pd.read_csv(\"./data/fake_1000.csv\")\n</code></pre> <pre><code>from splink.duckdb.duckdb_linker import DuckDBLinker\nsettings = {\"link_type\": \"dedupe_only\"}\nlinker = DuckDBLinker(df, settings)\n\nblocking_rule_1 = \"substr(l.first_name,1,1) = substr(r.first_name,1,1) and l.surname = r.surname\"\ncount = linker.count_num_comparisons_from_blocking_rule(blocking_rule_1)\nprint(f\"Number of comparisons generated by '{blocking_rule_1}': {count:,.0f}\")\n\nblocking_rule_2 = \"l.surname = r.surname\"\ncount = linker.count_num_comparisons_from_blocking_rule(blocking_rule_2)\nprint(f\"Number of comparisons generated by '{blocking_rule_2}': {count:,.0f}\")\n\nblocking_rule_3 = \"l.email = r.email\"\ncount = linker.count_num_comparisons_from_blocking_rule(blocking_rule_3)\nprint(f\"Number of comparisons generated by '{blocking_rule_3}': {count:,.0f}\")\n\nblocking_rule_3 = \"l.city = r.city and l.first_name = r.first_name\"\ncount = linker.count_num_comparisons_from_blocking_rule(blocking_rule_3)\nprint(f\"Number of comparisons generated by '{blocking_rule_3}': {count:,.0f}\")\n</code></pre> <pre>\n<code>Number of comparisons generated by 'substr(l.first_name,1,1) = substr(r.first_name,1,1) and l.surname = r.surname': 473\nNumber of comparisons generated by 'l.surname = r.surname': 1,638\nNumber of comparisons generated by 'l.email = r.email': 682\nNumber of comparisons generated by 'l.city = r.city and l.first_name = r.first_name': 315\n</code>\n</pre> <p>The maximum number of comparisons that you can compute will be affected by your choice of SQL backend, and how powerful your computer is.</p> <p>For linkages in DuckDB on a standard laptop, we suggest using blocking rules that create no more than about 20 million comparisons.  For Spark and Athena, try starting with fewer than a a billion comparisons, before scaling up.</p> <pre><code>blocking_rules = [blocking_rule_1, blocking_rule_2, blocking_rule_3]\nlinker.cumulative_num_comparisons_from_blocking_rules_chart(blocking_rules)\n</code></pre> <pre><code>linker.profile_columns(\"city || left(first_name,1) \")\n</code></pre>"},{"location":"demos/03_Blocking.html#choosing-blocking-rules-to-optimise-runtime","title":"Choosing blocking rules to optimise runtime","text":"<p>To link records, we need to compare pairs of records, and decide which pairs are matches and non matches.</p> <p>For most large datasets, it is computationally intractable to compare every row with every other row, since the number of comparisons rises quadratically with the number of records.  </p> <p>Instead we rely on blocking rules, which specify which pairwise comparisons to generate.  For example, we could generate the subset of pairwise comparisons where either first name or surname matches.</p> <p>This is part of a two step process to link data:</p> <ol> <li> <p>Use blocking rules to generate candidate pairwise record comparisons</p> </li> <li> <p>Use a probabilistic linkage model to score these candidate pairs, to determine which ones should be linked</p> </li> </ol> <p>Blocking rules are the most important determinant of the performance of your linkage job.  </p> <p>When deciding on your blocking rules, you're trading off accuracy for performance:</p> <ul> <li>If your rules are too loose, your linkage job may fail.  </li> <li>If they're too tight, you may miss some valid links. </li> </ul> <p>This tutorial clarifies what blocking rules are, and how to choose good rules.</p>"},{"location":"demos/03_Blocking.html#blocking-rules-in-splink","title":"Blocking rules in Splink","text":"<p>In Splink, blocking rules are specified as SQL expressions. </p> <p>For example, to generate the subset of record comparisons where the first name matches, we can specify the following blocking rule:</p> <p><code>l.first_name = r.first_name</code></p> <p>Since blocking rules are SQL expressions, they can be arbitrarily complex.  For example, you could create record comparisons where the initial of the first name and the surname match with the following rule:</p> <p><code>substr(l.first_name, 1,1) = substr(r.first_name, 1,1) and l.surname = r.surname</code></p>"},{"location":"demos/03_Blocking.html#devising-effective-blocking-rules","title":"Devising effective blocking rules","text":"<p>The aims of your blocking rules are twofold: 1. Eliminate enough non-matching comparison pairs so your record linkage job is small enough to compute 2. Eliminate as few truly matching pairs as possible (ideally none)</p> <p>It is usually impossible to find a single blocking rule which achieves both aims, so we recommend using multiple blocking rules.  </p> <p>When we specify multiple blocking rules, Splink will generate all comparison pairs that meet any one of the rules.</p> <p>For example, consider the following blocking rule:</p> <p><code>l.first_name = r.first_name and l.dob = r.dob</code></p> <p>This rule is likely to be effective in reducing the number of comparison pairs.  It will retain all truly matching pairs, except those with errors or nulls in either the <code>first_name</code> or <code>dob</code> fields.</p> <p>Now consider a second blocking rule:</p> <p><code>l.email and r.email</code>.</p> <p>This will retain all truly matching pairs, except those with errors or nulls in the <code>email</code> column.</p> <p>Individually, these blocking rules are problematic because they exclude true matches where the records contain typos of certain types.  But between them, they might do quite a good job.  </p> <p>For a true match to be eliminated by the use of these two blocking rules, it would have to have an error in both  email AND (first name or date of birth).  </p> <p>This is not completely implausible, but it is significantly less likely than if we'd just used a single rule.</p> <p>More generally, we can often specify multiple blocking rules such that it becomes highly implausible that a true match would not meet at least one of these blocking critera.  This is the recommended approach in Splink.  Generally we would recommend between about 3 and 10, though even more is possible.</p> <p>The question then becomes how to choose what to put in this list.</p>"},{"location":"demos/03_Blocking.html#splink-tools-to-help-choose-your-blocking-rules","title":"Splink tools to help choose your blocking rules","text":"<p>Splink contains a number of tools to help you choose effective blocking rules.  Let's try them out, using our small test dataset:</p>"},{"location":"demos/03_Blocking.html#counting-the-number-of-comparisons-created-by-a-single-blocking-rule","title":"Counting the number of comparisons created by a single blocking rule","text":"<p>On large datasets, some blocking rules imply the creation of trillions of record comparisons, which would cause a linkage job to fail.</p> <p>Before using a blocking rule in a linkage job, it's therefore a good idea to count the number of records it generates to ensure it is not too loose:</p>"},{"location":"demos/03_Blocking.html#counting-the-number-of-comparisons-created-by-a-list-of-blocking-rules","title":"Counting the number of comparisons created by a list of blocking rules","text":"<p>As noted above, it's usually a good idea to use multiple blocking rules.  It's therefore useful to know how many record comparisons will be generated when these rules are applied.</p> <p>Since the same record comparison may be created by several blocking rules, and Splink automatically deduplicates these comparisons, we cannot simply total the number of comparisons generated by each rule individually.  </p> <p>Splink provides a chart that shows the marginal (additional) comparisons generated by each blocking rule, after deduplication:</p>"},{"location":"demos/03_Blocking.html#understanding-why-certain-blocking-rules-create-large-numbers-of-comparisons","title":"Understanding why certain blocking rules create large numbers of comparisons","text":"<p>Finally, we can use the <code>profile_columns</code> function we saw in the previous tutorial to understand a specific blocking rule in more depth.</p> <p>Suppose we're interested in blocking on city and first initial.  </p> <p>Within each distinct value of <code>(city, first initial)</code>, all possible pairwise comparisons will be generated.</p> <p>So for instance, if there are 15 distinct records with <code>London,J</code> then these records will result in <code>n(n-1)/2 = 105</code> pairwise comparisons being generated.</p> <p>In a larger dataset, we might observe 10,000 <code>London,J</code> records, which would then be responsible for <code>49,995,000</code> comparisons.  </p> <p>These high-frequency values therefore have a disproportionate influence on the overall number of pairwise comparisons, and so it can be useful to analyse skew, as follows:</p>"},{"location":"demos/04_Estimating_model_parameters.html","title":"4. Estimating model parameters","text":"<pre><code># Begin by reading in the tutorial data again\nfrom splink.duckdb.duckdb_linker import DuckDBLinker\nimport pandas as pd \nimport altair as alt\nalt.renderers.enable(\"mimetype\")\ndf = pd.read_csv(\"./data/fake_1000.csv\")\n</code></pre> <pre><code>import splink.duckdb.duckdb_comparison_library as cl\n\nemail_comparison =  cl.levenshtein_at_thresholds(\"email\", 2)\nprint(email_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match vs. Email within levenshtein threshold 2 vs. anything else' of \"email\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: \"email_l\" IS NULL OR \"email_r\" IS NULL\n    - 'Exact match' with SQL rule: \"email_l\" = \"email_r\"\n    - 'Levenshtein &lt;= 2' with SQL rule: levenshtein(\"email_l\", \"email_r\") &lt;= 2\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <ol> <li><code>Comparison Template</code> functions which have been created for specific data types. For example, names.</li> </ol> <pre><code>import splink.duckdb.duckdb_comparison_template_library as ctl\n\nfirst_name_comparison = ctl.name_comparison(\"first_name\")\nprint(first_name_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match vs. First_Name within jaro_winkler thresholds 0.95, 0.88 vs. anything else' of \"first_name\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: \"first_name_l\" IS NULL OR \"first_name_r\" IS NULL\n    - 'Exact match first_name' with SQL rule: \"first_name_l\" = \"first_name_r\"\n    - 'Jaro_winkler_similarity &gt;= 0.95' with SQL rule: jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &gt;= 0.95\n    - 'Jaro_winkler_similarity &gt;= 0.88' with SQL rule: jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &gt;= 0.88\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <pre><code>settings = {\n    \"link_type\": \"dedupe_only\",\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\"),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        cl.levenshtein_at_thresholds(\"email\", 2),\n    ],\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"retain_matching_columns\": True,\n    \"retain_intermediate_calculation_columns\": True,\n}\n\nlinker = DuckDBLinker(df, settings)\n</code></pre> <p>In words, this setting dictionary says:</p> <ul> <li>We are performing a <code>dedupe_only</code> (the other options are <code>link_only</code>, or <code>link_and_dedupe</code>, which may be used if there are multiple input datasets).</li> <li>When comparing records, we will use information from the <code>first_name</code>, <code>surname</code>, <code>dob</code>, <code>city</code> and <code>email</code> columns to compute a match score.</li> <li>The <code>blocking_rules_to_generate_predictions</code> states that we will only check for duplicates amongst records where either the <code>first_name</code> or <code>surname</code> is identical.</li> <li>We have enabled term frequency adjustments for the 'city' column, because some values (e.g. <code>London</code>) appear much more frequently than others.</li> <li>We have set <code>retain_intermediate_calculation_columns</code> and <code>additional_columns_to_retain</code> to <code>True</code>  so that Splink outputs additional information that helps the user understand the calculations. If they were <code>False</code>, the computations would run faster.</li> </ul> <pre><code>deterministic_rules = [\n    \"l.first_name = r.first_name and levenshtein(r.dob, l.dob) &lt;= 1\",\n    \"l.surname = r.surname and levenshtein(r.dob, l.dob) &lt;= 1\",\n    \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) &lt;= 2\",\n    \"l.email = r.email\"\n]\n\nlinker.estimate_probability_two_random_records_match(deterministic_rules, recall=0.7)\n</code></pre> <pre>\n<code>Probability two random records match is estimated to be  0.00333.\nThis means that amongst all possible pairwise record comparisons, one in 300.13 are expected to match.  With 499,500 total possible comparisons, we expect a total of around 1,664.29 matching pairs\n</code>\n</pre> <pre><code>linker.estimate_u_using_random_sampling(max_pairs=1e6)\n</code></pre> <pre>\n<code>----- Estimating u probabilities using random sampling -----\n\nEstimated u probabilities using random sampling\n\nYour model is not yet fully trained. Missing estimates for:\n    - first_name (no m values are trained).\n    - surname (no m values are trained).\n    - dob (no m values are trained).\n    - city (no m values are trained).\n    - email (no m values are trained).\n</code>\n</pre> <pre><code>training_blocking_rule = \"l.first_name = r.first_name and l.surname = r.surname\"\ntraining_session_fname_sname = linker.estimate_parameters_using_expectation_maximisation(training_blocking_rule)\n</code></pre> <pre>\n<code>\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.first_name = r.first_name and l.surname = r.surname\n\nParameter estimates will be made for the following comparison(s):\n    - dob\n    - city\n    - email\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - first_name\n    - surname\n\nIteration 1: Largest change in params was -0.537 in the m_probability of dob, level `Exact match`\nIteration 2: Largest change in params was 0.0394 in probability_two_random_records_match\nIteration 3: Largest change in params was 0.00951 in probability_two_random_records_match\nIteration 4: Largest change in params was 0.00279 in probability_two_random_records_match\nIteration 5: Largest change in params was 0.000913 in probability_two_random_records_match\nIteration 6: Largest change in params was 0.000316 in probability_two_random_records_match\nIteration 7: Largest change in params was 0.000112 in probability_two_random_records_match\nIteration 8: Largest change in params was 4.03e-05 in probability_two_random_records_match\n\nEM converged after 8 iterations\n\nYour model is not yet fully trained. Missing estimates for:\n    - first_name (no m values are trained).\n    - surname (no m values are trained).\n</code>\n</pre> <p>In a second estimation pass, we block on dob. This allows us to estimate parameters for the <code>first_name</code> and <code>surname</code> comparisons.</p> <p>Between the two estimation passes, we now have parameter estimates for all comparisons.</p> <pre><code>from numpy import fix\n\n\ntraining_blocking_rule = \"l.dob = r.dob\"\ntraining_session_dob = linker.estimate_parameters_using_expectation_maximisation(training_blocking_rule)\n</code></pre> <pre>\n<code>\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.dob = r.dob\n\nParameter estimates will be made for the following comparison(s):\n    - first_name\n    - surname\n    - city\n    - email\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - dob\n\nIteration 1: Largest change in params was -0.393 in the m_probability of surname, level `Exact match surname`\nIteration 2: Largest change in params was 0.125 in probability_two_random_records_match\nIteration 3: Largest change in params was 0.0395 in probability_two_random_records_match\nIteration 4: Largest change in params was 0.0155 in probability_two_random_records_match\nIteration 5: Largest change in params was 0.00735 in probability_two_random_records_match\nIteration 6: Largest change in params was 0.00386 in probability_two_random_records_match\nIteration 7: Largest change in params was 0.00214 in probability_two_random_records_match\nIteration 8: Largest change in params was 0.00122 in probability_two_random_records_match\nIteration 9: Largest change in params was 0.000706 in probability_two_random_records_match\nIteration 10: Largest change in params was 0.000414 in probability_two_random_records_match\nIteration 11: Largest change in params was 0.000244 in probability_two_random_records_match\nIteration 12: Largest change in params was 0.000144 in probability_two_random_records_match\nIteration 13: Largest change in params was 8.52e-05 in probability_two_random_records_match\n\nEM converged after 13 iterations\n\nYour model is fully trained. All comparisons have at least one estimate for their m and u values\n</code>\n</pre> <p>Note that Splink includes other algorithms for estimating m and u values, which are documented here.</p> <pre><code>linker.match_weights_chart()\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code>linker.m_u_parameters_chart()\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code>settings = linker.save_settings_to_json(\"./demo_settings/saved_model_from_demo.json\", overwrite=True)\n</code></pre> <pre><code>linker.unlinkables_chart()\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <p>In the above chart, we can see that about 1.3% of records in the input dataset are unlinkable at a threshold match weight of 6.11 (correponding to a match probability of around 98.6%)</p>"},{"location":"demos/04_Estimating_model_parameters.html#specifying-and-estimating-a-linkage-model","title":"Specifying and estimating a linkage model","text":"<p>In the last tutorial we looked at how we can use blocking rules to generate pairwise record comparisons.</p> <p>Now it's time to estimate a probabilistic linkage model to score each of these comparisons. The resultant match score is a prediction of whether the two records represent the same entity (e.g. are the same person).  </p> <p>The purpose of estimating the model is to learn the relative importance of different parts of your data for the purpose of data linking.  </p> <p>For example, a match on date of birth is a much stronger indicator that two records refer to the same entity than a match on gender.  A mismatch on gender may be a stronger indicate against two records referring than a mismatch on name, since names are more likely to be entered differently.</p> <p>The relative importance of different information is captured in the (partial) 'match weights', which can be learned from your data.  These match weights are then added up to compute the overall match score.</p> <p>The match weights are are derived from the <code>m</code> and <code>u</code> parameters of the underlying Fellegi Sunter model.  Splink uses various statistical routines to estimate these parameters.  Further details of the underlying theory can be found here, which will help you understand this part of the tutorial.</p>"},{"location":"demos/04_Estimating_model_parameters.html#specifying-a-linkage-model","title":"Specifying a linkage model","text":"<p>To build a linkage model, the user defines the partial match weights that <code>splink</code> needs to estimate.    This is done by defining how the information in the input records should be compared.</p> <p>To be concrete, here is an example comparison:</p> first_name_l first_name_r surname_l surname_r dob_l dob_r city_l city_r email_l email_r Robert Rob Allen Allen 1971-05-24 1971-06-24 nan London roberta25@smith.net roberta25@smith.net <p>What functions should we use to assess the similarity of <code>Rob</code> vs. <code>Robert</code> in the the <code>first_name</code> field?  </p> <p>Should similarity in the <code>dob</code> field be computed in the same way, or a different way?</p> <p>Your job as the developer of a linkage model is to decide what comparisons are most appropriate for the types of data you have.  </p> <p>Splink can then estimate how much weight to place on a fuzzy match of <code>Rob</code> vs. <code>Robert</code>, relative to an exact match on <code>Robert</code>, or a non-match.</p> <p>Defining these scenarios is done using <code>Comparison</code>s.</p>"},{"location":"demos/04_Estimating_model_parameters.html#comparisons","title":"Comparisons","text":"<p>The concept of a <code>Comparison</code> has a specific definition within Splink: it defines how data from one or more input columns is compared, using SQL expressions to assess similarity.</p> <p>For example, one <code>Comparison</code> may represent how similarity is assessed for a person's date of birth.  </p> <p>Another <code>Comparison</code> may represent the comparison of a person's name or location.</p> <p>A model is composed of many <code>Comparison</code>s, which between them assess the similarity of all of the columns being used for data linking.  </p> <p>Each <code>Comparison</code> contains two or more <code>ComparisonLevels</code> which define n discrete gradations of similarity between the input columns within the Comparison.</p> <p>As such <code>ComparisonLevels</code>are nested within <code>Comparisons</code> as follows:</p> <pre><code>Data Linking Model\n\u251c\u2500-- Comparison: Date of birth\n\u2502    \u251c\u2500-- ComparisonLevel: Exact match\n\u2502    \u251c\u2500-- ComparisonLevel: One character difference\n\u2502    \u251c\u2500-- ComparisonLevel: All other\n\u251c\u2500-- Comparison: Surname\n\u2502    \u251c\u2500-- ComparisonLevel: Exact match on surname\n\u2502    \u251c\u2500-- ComparisonLevel: All other\n\u2502    etc.\n</code></pre> <p>Our example data would therefore result in the following comparisons, for <code>dob</code> and <code>surname</code>:</p> dob_l dob_r comparison_level interpretation 1971-05-24 1971-05-24 Exact match great match 1971-05-24 1971-06-24 One character difference ok match 1971-05-24 2000-01-02 All other bad match surname_l surname_r comparison_level interpretation Rob Rob Exact match great match Rob Jane All other bad match Rob Robert All other bad match, this comparison has no notion of nicknames <p>More information about comparisons can be found here.</p> <p>We will now use these concepts to build a data linking model.</p>"},{"location":"demos/04_Estimating_model_parameters.html#specifying-the-model-using-comparisons","title":"Specifying the model using comparisons","text":"<p>Splink includes libraries of comparison functions to make it simple to get started. These are split into two categories:</p> <ol> <li><code>Comparison</code> functions which apply a particular fuzzy matching function. For example, levenshtein distance.</li> </ol>"},{"location":"demos/04_Estimating_model_parameters.html#specifying-the-full-settings-dictionary","title":"Specifying the full settings dictionary","text":"<p><code>Comparisons</code> are specified as part of the Splink <code>settings</code>, a Python dictionary which controls all of the configuration of a Splink model:</p>"},{"location":"demos/04_Estimating_model_parameters.html#estimate-the-parameters-of-the-model","title":"Estimate the parameters of the model","text":"<p>Now that we have specified our linkage model, we need to estimate the <code>probability_two_random_records_match</code>, <code>u</code>, and <code>m</code> parameters.</p> <ul> <li> <p>The <code>probability_two_random_records_match</code> parameter is the probability that two records taken at random from your input data represent a match (typically a very small number).</p> </li> <li> <p>The <code>u</code> values are the proportion of records falling into each <code>ComparisonLevel</code> amongst truly non-matching records.</p> </li> <li> <p>The <code>m</code> values are the proportion of records falling into each <code>ComparisonLevel</code> amongst truly matching records</p> </li> </ul> <p>You can read more about the theory of what these mean.</p> <p>We can estimate these parameters using unlabeled data. If we have labels, then we can estimate them even more accurately.</p>"},{"location":"demos/04_Estimating_model_parameters.html#estimation-of-probability_two_random_records_match","title":"Estimation of <code>probability_two_random_records_match</code>","text":"<p>In some cases, the <code>probability_two_random_records_match</code> will be known. For example, if you are linking two tables of 10,000 records and expect a one-to-one match, then you should set this value to <code>1/10_000</code> in your settings instead of estimating it.</p> <p>More generally, this parameter is unknown and needs to be estimated.  </p> <p>It can be estimated accurately enough for most purposes by combining a series of deterministic matching rules and a guess of the recall corresponding to those rules.  For further details of the rationale behind this appraoch see here.</p> <p>In this example, I guess that the following deterministic matching rules have a recall of about 70%:</p>"},{"location":"demos/04_Estimating_model_parameters.html#estimation-of-u-probabilities","title":"Estimation of <code>u</code> probabilities","text":"<p>Once we have the <code>probability_two_random_records_match</code> parameter, we can estimate the <code>u</code> probabilities.</p> <p>We estimate <code>u</code> using the <code>estimate_u_using_random_sampling</code> method, which doesn't require any labels.</p> <p>It works by sampling random pairs of records, since most of these pairs are going to be non-matches. Over these non-matches we compute the distribution of <code>ComparisonLevel</code>s for each <code>Comparison</code>.</p> <p>For instance, for <code>gender</code>, we would find that the the gender matches 50% of the time, and mismatches 50% of the time. </p> <p>For <code>dob</code> on the other hand, we would find that the <code>dob</code> matches 1% of the time, has a \"one character difference\" 3% of the time, and everything else happens 96% of the time.</p> <p>The larger the random sample, the more accurate the predictions. You control this using the <code>max_pairs</code> parameter. For large datasets, we recommend using at least 10 million - but the higher the better and 1 billion is often appropriate for larger datasets.</p>"},{"location":"demos/04_Estimating_model_parameters.html#estimation-of-m-probabilities","title":"Estimation of <code>m</code> probabilities","text":"<p><code>m</code> is the trickiest of the parameters to estimate, because we have to have some idea of what the true matches are.</p> <p>If we have labels, we can directly estimate it.</p> <p>If we do not have labelled data, the <code>m</code> parameters can be estimated using an iterative maximum likelihood approach called Expectation Maximisation. </p>"},{"location":"demos/04_Estimating_model_parameters.html#estimating-directly","title":"Estimating directly","text":"<p>If we have labels, we can estimate <code>m</code> directly using the <code>estimate_m_from_label_column</code> method of the linker.</p> <p>For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model.</p> <p>Note that this column does not need to be fully populated. A common case is where a unique identifier such as social security number is only partially populated.</p> <p>For example (in this tutorial we don't have labels, so we're not actually going to use this):</p> <pre><code>linker.estimate_m_from_label_column(\"social_security_number\")\n</code></pre>"},{"location":"demos/04_Estimating_model_parameters.html#estimating-with-expectation-maximisation","title":"Estimating with Expectation Maximisation","text":"<p>This algorithm estimates the <code>m</code> values by generating pairwise record comparisons, and using them to maximise a likelihood function. </p> <p>Each estimation pass requires the user to configure an estimation blocking rule to reduce the number of record comparisons generated to a manageable level.</p> <p>In our first estimation pass, we block on <code>first_name</code> and <code>surname</code>, meaning we will generate all record comparisons that have <code>first_name</code> and <code>surname</code> exactly equal.   </p> <p>Recall we are trying to estimate the <code>m</code> values of the model, i.e. proportion of records falling into each <code>ComparisonLevel</code> amongst truly matching records.</p> <p>This means that, in this training session, we cannot estimate parameter estimates for the <code>first_name</code> or <code>surname</code> columns, since they will be equal for all the comparisons we do.</p> <p>We can, however, estimate parameter estimates for all of the other columns.  The output messages produced by Splink confirm this.</p>"},{"location":"demos/04_Estimating_model_parameters.html#visualising-model-parameters","title":"Visualising model parameters","text":"<p>Splink can generate a number of charts to help you understand your model.  For an introduction to these charts and how to interpret them, please see this video.</p> <p>The final estimated match weights can be viewed in the match weights chart:</p>"},{"location":"demos/04_Estimating_model_parameters.html#saving-the-model","title":"Saving the model","text":"<p>We can save the model, including our estimated parameters, to a <code>.json</code> file, so we can use it in the next tutorial.</p>"},{"location":"demos/04_Estimating_model_parameters.html#detecting-unlinkable-records","title":"Detecting unlinkable records","text":"<p>An interesting application of our trained model that is useful to explore before making any predictions is to detect 'unlinkable' records.</p> <p>Unlinkable records are those which do not contain enough information to be linked.  A simple example would be a record containing only 'John Smith', and null in all other fields.  This record may link to other records, but we'll never know because there's not enough information to disambiguate any potential links.   Unlinkable records can be found by linking records to themselves - if, even when matched to themselves, they don't meet the match threshold score, we can be sure they will never link to anything.</p>"},{"location":"demos/04_Estimating_model_parameters.html#next-steps","title":"Next steps","text":"<p>Now we have trained a model, we can move on to using it predict matching records.</p>"},{"location":"demos/04_Estimating_model_parameters.html#further-reading","title":"Further reading","text":"<p>Full documentation for all of the ways of estimating model parameters can be found here.</p>"},{"location":"demos/05_Predicting_results.html","title":"5. Predicting results","text":"<pre><code>from splink.duckdb.duckdb_linker import DuckDBLinker\nimport pandas as pd \npd.options.display.max_columns = 1000\ndf = pd.read_csv(\"./data/fake_1000.csv\")\n</code></pre> <pre><code>linker = DuckDBLinker(df)\nlinker.load_settings_from_json(\"./demo_settings/saved_model_from_demo.json\")\n</code></pre> <pre><code>df_predictions = linker.predict(threshold_match_probability=0.2)\ndf_predictions.as_pandas_dataframe(limit=5)\n</code></pre> match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name bf_first_name surname_l surname_r gamma_surname bf_surname dob_l dob_r gamma_dob bf_dob city_l city_r gamma_city tf_city_l tf_city_r bf_city bf_tf_adj_city email_l email_r gamma_email bf_email match_key 0 12.728972 0.999853 4 5 Grace Grace 1 85.803382 NaN Kelly -1 1.000000 1997-04-26 1991-04-26 1 92.704873 Hull NaN -1 0.001230 NaN 1.000000 1.000000 grace.kelly52@jones.com grace.kelly52@jones.com 3 255.30162 0 1 11.216421 0.999580 26 29 Thomas Thomas 1 85.803382 Gabriel Gabriel 3 89.480899 1976-09-15 1976-08-15 1 92.704873 Loodon NaN -1 0.001230 NaN 1.000000 1.000000 gabriel.t54@nnichls.info NaN -1 1.00000 0 2 11.216421 0.999580 28 29 Thomas Thomas 1 85.803382 Gabriel Gabriel 3 89.480899 1976-09-15 1976-08-15 1 92.704873 London NaN -1 0.212792 NaN 1.000000 1.000000 gabriel.t54@nichols.info NaN -1 1.00000 0 3 -1.284382 0.291055 37 860 Theodore Theodore 1 85.803382 Morris Marshall 0 0.267994 1978-08-19 1972-07-25 0 0.464198 Birmingham Birmingham 1 0.049200 0.0492 10.264354 1.120874 t.m39@brooks-sawyer.com NaN -1 1.00000 0 4 -1.284382 0.291055 39 860 Theodore Theodore 1 85.803382 Morris Marshall 0 0.267994 1978-08-19 1972-07-25 0 0.464198 Birmingham Birmingham 1 0.049200 0.0492 10.264354 1.120874 t.m39@brooks-sawyer.com NaN -1 1.00000 0 <pre><code>clusters = linker.cluster_pairwise_predictions_at_threshold(df_predictions, threshold_match_probability=0.5)\nclusters.as_pandas_dataframe(limit=10)\n</code></pre> <pre>\n<code>Completed iteration 1, root rows count 13\nCompleted iteration 2, root rows count 1\nCompleted iteration 3, root rows count 0\n</code>\n</pre> cluster_id unique_id first_name surname dob city email cluster tf_city 0 0 0 Robert Alan 1971-06-24 NaN robert255@smith.net 0 NaN 1 0 1 Robert Allen 1971-05-24 NaN roberta25@smith.net 0 NaN 2 0 2 Rob Allen 1971-06-24 London roberta25@smith.net 0 0.212792 3 0 3 Robert Alen 1971-06-24 Lonon NaN 0 0.007380 4 4 4 Grace NaN 1997-04-26 Hull grace.kelly52@jones.com 1 0.001230 5 4 5 Grace Kelly 1991-04-26 NaN grace.kelly52@jones.com 1 NaN 6 6 6 Logan pMurphy 1973-08-01 NaN NaN 2 NaN 7 7 7 NaN NaN 2015-03-03 Portsmouth evied56@harris-bailey.net 3 0.017220 8 8 8 NaN Dean 2015-03-03 NaN NaN 3 NaN 9 8 9 Evie Dean 2015-03-03 Pootsmruth evihd56@earris-bailey.net 3 0.001230 <pre><code>sql = f\"\"\"\nselect * \nfrom {df_predictions.physical_name}\nlimit 2\n\"\"\"\nlinker.query_sql(sql)\n</code></pre> match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name bf_first_name surname_l surname_r gamma_surname bf_surname dob_l dob_r gamma_dob bf_dob city_l city_r gamma_city tf_city_l tf_city_r bf_city bf_tf_adj_city email_l email_r gamma_email bf_email match_key 0 12.728972 0.999853 4 5 Grace Grace 1 85.803382 NaN Kelly -1 1.000000 1997-04-26 1991-04-26 1 92.704873 Hull NaN -1 0.00123 NaN 1.0 1.0 grace.kelly52@jones.com grace.kelly52@jones.com 3 255.30162 0 1 11.216421 0.999580 26 29 Thomas Thomas 1 85.803382 Gabriel Gabriel 3 89.480899 1976-09-15 1976-08-15 1 92.704873 Loodon NaN -1 0.00123 NaN 1.0 1.0 gabriel.t54@nnichls.info NaN -1 1.00000 0"},{"location":"demos/05_Predicting_results.html#predicting-which-records-match","title":"Predicting which records match","text":"<p>In the previous tutorial, we built and estimated a linkage model.</p> <p>In this tutorial, we will load the estimated model and use it to make predictions of which pairwise record comparisons match.</p>"},{"location":"demos/05_Predicting_results.html#load-estimated-model-from-previous-tutorial","title":"Load estimated model from previous tutorial","text":""},{"location":"demos/05_Predicting_results.html#predicting-match-weights-using-the-trained-model","title":"Predicting match weights using the trained model","text":"<p>We use <code>linker.predict()</code> to run the model.  </p> <p>Under the hood this will:</p> <ul> <li> <p>Generate all pairwise record comparisons that match at least one of the <code>blocking_rules_to_generate_predictions</code></p> </li> <li> <p>Use the rules specified in the <code>Comparisons</code> to evaluate the similarity of the input data</p> </li> <li> <p>Use the estimated match weights, applying term frequency adjustments where requested to produce the final <code>match_weight</code> and <code>match_probability</code> scores</p> </li> </ul> <p>Optionally, a <code>threshold_match_probability</code> or <code>threshold_match_weight</code> can be provided, which will drop any row where the predicted score is below the threshold.</p>"},{"location":"demos/05_Predicting_results.html#clustering","title":"Clustering","text":"<p>The result of <code>linker.predict()</code> is a list of pairwise record comparisons and their associated scores. For instance, if we have input records A, B, C and D, it could be represented conceptually as: <pre><code>A -&gt; B with score 0.9\nB -&gt; C with score 0.95\nC -&gt; D with score 0.1\nD -&gt; E with score 0.99\n</code></pre></p> <p>Often, an alternative representation of this result is more useful, where each row is an input record, and where records link, they are assigned to the same cluster.</p> <p>With a score threshold of 0.5, the above data could be represented conceptually as:</p> <pre><code>ID, Cluster ID\nA,  1\nB,  1\nC,  1\nD,  2\nE,  2\n</code></pre> <p>The algorithm that converts between the pairwise results and the clusters is called connected components, and it is included in Splink.  You can use it as follows:</p>"},{"location":"demos/06_Visualising_predictions.html","title":"6. Visualising predictions","text":"<pre><code># Rerun our predictions to we're ready to view the charts\nfrom splink.duckdb.duckdb_linker import DuckDBLinker\nimport pandas as pd \nimport altair as alt\nalt.renderers.enable('mimetype')\n\ndf = pd.read_csv(\"./data/fake_1000.csv\")\nlinker = DuckDBLinker(df)\nlinker.load_settings_from_json(\"./demo_settings/saved_model_from_demo.json\")\ndf_predictions = linker.predict(threshold_match_probability=0.2)\n</code></pre> <pre><code>records_to_view  = df_predictions.as_record_dict(limit=5)\nlinker.waterfall_chart(records_to_view, filter_nulls=False)\n</code></pre> <pre><code>linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", overwrite=True)\n\n# You can view the scv.html file in your browser, or inline in a notbook as follows\nfrom IPython.display import IFrame\nIFrame(\n    src=\"./scv.html\", width=\"100%\", height=1200\n)  \n</code></pre> <pre><code>df_clusters = linker.cluster_pairwise_predictions_at_threshold(df_predictions, threshold_match_probability=0.5)\n\nlinker.cluster_studio_dashboard(df_predictions, df_clusters, \"cluster_studio.html\", sampling_method=\"by_cluster_size\", overwrite=True)\n\n# You can view the scv.html file in your browser, or inline in a notbook as follows\nfrom IPython.display import IFrame\nIFrame(\n    src=\"./cluster_studio.html\", width=\"100%\", height=1200\n)\n</code></pre> <pre>\n<code>Completed iteration 1, root rows count 13\nCompleted iteration 2, root rows count 1\nCompleted iteration 3, root rows count 0\n</code>\n</pre>"},{"location":"demos/06_Visualising_predictions.html#visualising-predictions","title":"Visualising predictions","text":"<p>Splink contains a variety of tools to help you visualise your predictions.</p> <p>The idea is that, by developing an understanding of how your model works, you can gain confidence that the predictions it makes are sensible, or alternatively find examples of where your model isn't working, which may help you improve the model specification and fix these problems.</p>"},{"location":"demos/06_Visualising_predictions.html#waterfall-chart","title":"Waterfall chart","text":"<p>The waterfall chart provides a means of visualising individual predictions to understand how Splink computed the final matchweight for a particular pairwise record comparison.</p> <p>To plot a waterfall chart, the user chooses one or more records from the results of <code>linker.predict()</code>, and provides these records to the <code>linker.waterfall_chart()</code> function.</p> <p>For an introduction to waterfall charts and how to interpret them, please see this video.</p>"},{"location":"demos/06_Visualising_predictions.html#comparison-viewer-dashboard","title":"Comparison viewer dashboard","text":"<p>The comparison viewer dashboard takes this one step further by producing an interactive dashboard that contains example predictions from across the spectrum of match scores.</p> <p>An in-depth video describing how to interpret the dashboard can be found here.</p>"},{"location":"demos/06_Visualising_predictions.html#cluster-studio-dashboard","title":"Cluster studio dashboard","text":"<p>Cluster studio is an interactive dashboards that visualises the results of clustering your predictions.</p> <p>It provides examples of clusters of different sizes.  The shape and size of clusters can be indicative of problems with record linkage, so it provides a tool to help you find potential false positive and negative links.</p>"},{"location":"demos/07_Quality_assurance.html","title":"7. Quality assurance","text":"<pre><code># Rerun our predictions to we're ready to view the charts\nfrom splink.duckdb.duckdb_linker import DuckDBLinker\nimport pandas as pd \nimport altair as alt\nalt.renderers.enable('mimetype')\n\ndf = pd.read_csv(\"./data/fake_1000.csv\")\nlinker = DuckDBLinker(df)\nlinker.load_settings_from_json(\"./demo_settings/saved_model_from_demo.json\")\ndf_predictions = linker.predict(threshold_match_probability=0.2)\n</code></pre> <pre><code>df_labels = pd.read_csv(\"./data/fake_1000_labels.csv\")\ndf_labels.head(5)\nlabels_table = linker.register_labels_table(df_labels)\n</code></pre> <pre><code>linker.roc_chart_from_labels_table(labels_table)\n</code></pre> <pre><code>linker.precision_recall_chart_from_labels_table(labels_table)\n</code></pre> <pre><code>roc_table = linker.truth_space_table_from_labels_table(labels_table)\nroc_table.as_pandas_dataframe(limit=5)\n</code></pre> truth_threshold match_probability row_count P N TP TN FP FN P_rate N_rate TP_rate TN_rate FP_rate FN_rate precision recall F1 0 -16.180925 0.000013 1225.0 80.0 1145.0 80.0 0.0 1145.0 0.0 0.0 0.934694 1.0 0.000000 1.000000 0.0 0.065306 1.0 0.122699 1 -15.197628 0.000027 1225.0 80.0 1145.0 80.0 106.0 1039.0 0.0 0.0 0.934694 1.0 0.092576 0.907424 0.0 0.071492 1.0 0.133556 2 -15.058351 0.000029 1225.0 80.0 1145.0 80.0 194.0 951.0 0.0 0.0 0.934694 1.0 0.169432 0.830568 0.0 0.077595 1.0 0.144144 3 -14.281196 0.000050 1225.0 80.0 1145.0 80.0 373.0 772.0 0.0 0.0 0.934694 1.0 0.325764 0.674236 0.0 0.093897 1.0 0.171674 4 -14.075054 0.000058 1225.0 80.0 1145.0 80.0 416.0 729.0 0.0 0.0 0.934694 1.0 0.363319 0.636681 0.0 0.098888 1.0 0.180180"},{"location":"demos/07_Quality_assurance.html#quality-assurance-of-prediction-results","title":"Quality assurance of prediction results","text":"<p>In the previous tutorial, we looked at various ways to visualise the results of our model.  </p> <p>These are useful for quality assurance purposes because they allow us to understand how our model works and verify that it is doing something sensible.  They can also be useful to identify examples where the model is not performing as expected.  </p> <p>In addition to these spot checks, Splink also has functions to perform more formal accuracy analysis.  These functions allow you to understand the likely prevalence of false positives and false negatives in your linkage models.</p> <p>They rely on the existence of a sample of labelled (ground truth) matches, which may have been produced (for example) by human beings.  For the accuracy analysis to be unbiased, the sample should be representative of the overall dataset.</p>"},{"location":"demos/07_Quality_assurance.html#load-in-labels","title":"Load in labels","text":"<p>The labels file contains a list of pairwise comparisons which represent matches and non-matches.</p> <p>The required format of the labels file is described here.</p>"},{"location":"demos/07_Quality_assurance.html#receiver-operating-characteristic-curve","title":"Receiver operating characteristic curve","text":"<p>A ROC chart shows how the number of false positives and false negatives varies depending on the match threshold chosen.  The match threshold is the match weight chosen as a cutoff for which pairwise comparisons to accept as matches.</p>"},{"location":"demos/07_Quality_assurance.html#precision-recall-chart","title":"Precision-recall chart","text":"<p>An alternative representation of truth space is called a precision recall curve.</p> <p>This can be plotted as follows:</p>"},{"location":"demos/07_Quality_assurance.html#truth-table","title":"Truth table","text":"<p>Finally, Splink can also report the underlying table used to construct the ROC and precision recall curves.</p>"},{"location":"demos/athena_deduplicate_50k_synthetic.html","title":"Athena deduplicate 50k synthetic","text":"<pre><code>from splink.athena.athena_linker import AthenaLinker\nimport altair as alt\nalt.renderers.enable('mimetype')\n\nimport pandas as pd\npd.options.display.max_rows = 1000\ndf = pd.read_parquet(\"./data/historical_figures_with_errors_50k.parquet\")\n</code></pre> <p>Create a boto3 session to be used within the linker</p> <pre><code>import boto3\nmy_session = boto3.Session(region_name=\"eu-west-1\")\n</code></pre> <pre><code># Simple settings dictionary will be used for exploratory analysis\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name and l.surname = r.surname\",\n        \"l.surname = r.surname and l.dob = r.dob\",\n        \"l.first_name = r.first_name and l.dob = r.dob\",\n        \"l.postcode_fake = r.postcode_fake and l.first_name = r.first_name\",\n    ],\n}\n</code></pre> <pre><code># Set the output bucket and the additional filepath to write outputs to\n############################################\n# EDIT THESE BEFORE ATTEMPTING TO RUN THIS #\n############################################\n\nbucket = \"my_s3_bucket\"\ndatabase = \"my_athena_database\"\nfilepath = \"athena_testing\"  # file path inside of your bucket\naws_filepath = f\"s3://{bucket}/{filepath}\"\n\n# Sessions are generated with a unique ID...\nlinker = AthenaLinker(\n    input_table_or_tables=df,\n    boto3_session=my_session,\n    # the bucket to store splink's parquet files\n    output_bucket=bucket,\n    # the database to store splink's outputs\n    output_database=database,\n    # folder to output data to\n    output_filepath=filepath,  \n    # table name within your database\n    # if blank, it will default to __splink__input_table_randomid\n    input_table_aliases=\"__splink__testings\",\n    settings_dict=settings,\n)\n\nlinker.profile_columns(\n    [\"first_name\", \"postcode_fake\", \"substr(dob, 1,4)\"], top_n=10, bottom_n=5\n)\n</code></pre> <pre><code>linker.cumulative_num_comparisons_from_blocking_rules_chart()\n</code></pre> <pre><code>linker.drop_all_tables_created_by_splink(delete_s3_folders=True)\n</code></pre> <pre><code>import splink.athena.athena_comparison_library as cl\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name and l.surname = r.surname\",\n        \"l.surname = r.surname and l.dob = r.dob\",\n        \"l.first_name = r.first_name and l.dob = r.dob\",\n        \"l.postcode_fake = r.postcode_fake and l.first_name = r.first_name\",\n    ],\n    \"comparisons\": [\n        cl.levenshtein_at_thresholds(\"first_name\", [1,2], term_frequency_adjustments=True),\n        cl.levenshtein_at_thresholds(\"surname\", [1,2], term_frequency_adjustments=True),\n        cl.levenshtein_at_thresholds(\"dob\", [1,2], term_frequency_adjustments=True),\n        cl.levenshtein_at_thresholds(\"postcode_fake\", 2,term_frequency_adjustments=True),\n        cl.exact_match(\"birth_place\", term_frequency_adjustments=True),\n        cl.exact_match(\"occupation\",  term_frequency_adjustments=True),\n    ],\n    \"retain_matching_columns\": True,\n    \"retain_intermediate_calculation_columns\": True,\n    \"max_iterations\": 10,\n    \"em_convergence\": 0.01\n}\n</code></pre> <pre><code># Write our dataframe to s3/our backing database\nimport awswrangler as wr\nwr.s3.to_parquet(\n    df,  # pandas dataframe\n    path=f\"{aws_filepath}/historical_figures_with_errors_50k\",\n    dataset=True,\n    database=database,\n    table=\"historical_figures_with_errors_50k\",\n    mode=\"overwrite\",\n    compression=\"snappy\",\n)\n</code></pre> <pre><code># Initialise our linker with historical_figures_with_errors_50k from our database\nlinker = AthenaLinker(\n    input_table_or_tables=\"historical_figures_with_errors_50k\",  \n    settings_dict=settings,\n    boto3_session=my_session,\n    output_bucket=bucket,  # the bucket to store splink's parquet files \n    output_database=database,  # the database to store splink's outputs\n    output_filepath=filepath  # folder to output data to\n)\n</code></pre> <pre><code>linker.estimate_probability_two_random_records_match(\n    [\n        \"l.first_name = r.first_name and l.surname = r.surname and l.dob = r.dob\",\n        \"substr(l.first_name,1,2) = substr(r.first_name,1,2) and l.surname = r.surname and substr(l.postcode_fake,1,2) = substr(r.postcode_fake,1,2)\",\n        \"l.dob = r.dob and l.postcode_fake = r.postcode_fake\",\n    ],\n    recall=0.6,\n)\n</code></pre> <pre>\n<code>Probability two random records match is estimated to be  0.000136.\nThis means that amongst all possible pairwise record comparisons, one in 7,362.31 are expected to match.  With 1,279,041,753 total possible comparisons, we expect a total of around 173,728.33 matching pairs\n</code>\n</pre> <pre><code>linker.estimate_u_using_random_sampling(max_pairs=5e6)\n</code></pre> <pre>\n<code>----- Estimating u probabilities using random sampling -----\n\nEstimated u probabilities using random sampling\n\nYour model is not yet fully trained. Missing estimates for:\n    - first_name (no m values are trained).\n    - surname (no m values are trained).\n    - dob (no m values are trained).\n    - postcode_fake (no m values are trained).\n    - birth_place (no m values are trained).\n    - occupation (no m values are trained).\n</code>\n</pre> <pre><code>blocking_rule = \"l.first_name = r.first_name and l.surname = r.surname\"\ntraining_session_names = linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n</code></pre> <pre>\n<code>\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.first_name = r.first_name and l.surname = r.surname\n\nParameter estimates will be made for the following comparison(s):\n    - dob\n    - postcode_fake\n    - birth_place\n    - occupation\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - first_name\n    - surname\n\nIteration 1: Largest change in params was -0.533 in probability_two_random_records_match\nIteration 2: Largest change in params was -0.0419 in the m_probability of birth_place, level `All other comparisons`\nIteration 3: Largest change in params was -0.0154 in the m_probability of birth_place, level `All other comparisons`\nIteration 4: Largest change in params was 0.00489 in the m_probability of birth_place, level `Exact match`\n\nEM converged after 4 iterations\n\nYour model is not yet fully trained. Missing estimates for:\n    - first_name (no m values are trained).\n    - surname (no m values are trained).\n</code>\n</pre> <pre><code>blocking_rule = \"l.dob = r.dob\"\ntraining_session_dob = linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n</code></pre> <pre>\n<code>\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.dob = r.dob\n\nParameter estimates will be made for the following comparison(s):\n    - first_name\n    - surname\n    - postcode_fake\n    - birth_place\n    - occupation\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - dob\n\nIteration 1: Largest change in params was -0.356 in the m_probability of first_name, level `Exact match`\nIteration 2: Largest change in params was 0.0401 in the m_probability of first_name, level `All other comparisons`\nIteration 3: Largest change in params was 0.00536 in the m_probability of first_name, level `All other comparisons`\n\nEM converged after 3 iterations\n\nYour model is fully trained. All comparisons have at least one estimate for their m and u values\n</code>\n</pre> <pre><code>linker.match_weights_chart()\n</code></pre> <pre><code>linker.unlinkables_chart()\n</code></pre> <pre><code>df_predict = linker.predict()\ndf_e = df_predict.as_pandas_dataframe(limit=5)\ndf_e\n</code></pre> match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name tf_first_name_l tf_first_name_r bf_first_name ... bf_birth_place bf_tf_adj_birth_place occupation_l occupation_r gamma_occupation tf_occupation_l tf_occupation_r bf_occupation bf_tf_adj_occupation match_key 0 19.465751 0.999999 Q5536981-1 Q5536981-4 george george 3 0.028014 0.028014 48.723867 ... 162.73433 0.097709 politician politician 1 0.088932 0.088932 21.983413 0.459975 0 1 33.572592 1.000000 Q5536981-1 Q5536981-5 george george 3 0.028014 0.028014 48.723867 ... 162.73433 0.097709 politician politician 1 0.088932 0.088932 21.983413 0.459975 0 2 33.572592 1.000000 Q5536981-1 Q5536981-6 george george 3 0.028014 0.028014 48.723867 ... 162.73433 0.097709 politician politician 1 0.088932 0.088932 21.983413 0.459975 0 3 33.572592 1.000000 Q5536981-1 Q5536981-7 george george 3 0.028014 0.028014 48.723867 ... 162.73433 0.097709 politician politician 1 0.088932 0.088932 21.983413 0.459975 0 4 22.025628 1.000000 Q5536981-1 Q5536981-8 george george 3 0.028014 0.028014 48.723867 ... 162.73433 0.097709 politician politician 1 0.088932 0.088932 21.983413 0.459975 0 <p>5 rows \u00d7 47 columns</p> <p>You can also view rows in this dataset as a waterfall chart as follows:</p> <pre><code>from splink.charts import waterfall_chart\nrecords_to_plot = df_e.to_dict(orient=\"records\")\nlinker.waterfall_chart(records_to_plot, filter_nulls=False)\n</code></pre> <pre><code>clusters = linker.cluster_pairwise_predictions_at_threshold(df_predict, threshold_match_probability=0.95)\n</code></pre> <pre>\n<code>Completed iteration 1, root rows count 642\nCompleted iteration 2, root rows count 119\nCompleted iteration 3, root rows count 35\nCompleted iteration 4, root rows count 6\nCompleted iteration 5, root rows count 0\n</code>\n</pre> <pre><code>linker.cluster_studio_dashboard(df_predict, clusters, \"50k_cluster.html\", sampling_method='by_cluster_size', overwrite=True)\n\nfrom IPython.display import IFrame\n\nIFrame(\n    src=\"./50k_cluster.html\", width=\"100%\", height=1200\n)\n</code></pre> <pre><code>linker.roc_chart_from_labels_column(\"cluster\",match_weight_round_to_nearest=0.02)\n</code></pre> <pre><code>records = linker.prediction_errors_from_labels_column(\n    \"cluster\",\n    threshold=0.999,\n    include_false_negatives=False,\n    include_false_positives=True,\n).as_record_dict()\nlinker.waterfall_chart(records)\n</code></pre> <pre><code># Some of the false negatives will be because they weren't detected by the blocking rules\nrecords = linker.prediction_errors_from_labels_column(\n    \"cluster\",\n    threshold=0.5,\n    include_false_negatives=True,\n    include_false_positives=False,\n).as_record_dict(limit=50)\n\nlinker.waterfall_chart(records)\n</code></pre> <p>And finally, clean up all tables except <code>df_predict</code></p> <pre><code>linker.drop_tables_in_current_splink_run(tables_to_exclude=df_predict)\n</code></pre>"},{"location":"demos/athena_deduplicate_50k_synthetic.html#linking-a-dataset-of-real-historical-persons","title":"Linking a dataset of real historical persons","text":"<p>In this example, we deduplicate a more realistic dataset. The data is based on historical persons scraped from wikidata. Duplicate records are introduced with a variety of errors introduced.</p>"},{"location":"demos/athena_deduplicate_50k_synthetic.html#athenalinker-setup","title":"AthenaLinker Setup","text":"<p>To work nicely with Athena, you need to outline various filepaths, buckets and the database(s) you wish to interact with.</p> <p>The AthenaLinker has three required inputs: * input_table_or_tables - the input table to use for linking. This can either be a table in a database or a pandas dataframe * output_database - the database to output all of your splink tables to. * output_bucket - the s3 bucket you wish any parquet files produced by splink to be output to.</p> <p>and two optional inputs: * output_filepath - the s3 filepath to output files to. This is an extension of output_bucket and dictate the full filepath your files will be output to. * input_table_aliases - the name of your table within your database, should you choose to use a pandas df as an input.</p>"},{"location":"demos/athena_deduplicate_50k_synthetic.html#perform-garbage-collection","title":"Perform garbage collection","text":"<p>To clean up your selected database and its backing data on AWS, you can use <code>drop_all_tables_created_by_splink</code>. This allows splink to automatically search for any tables prefixed with <code>__splink__df...</code> in your given database and delete them.</p> <p>Alternatively, if you want to delete splink tables from another database that you didn't select in the initialisation step, you can run <code>drop_splink_tables_from_database(database_name)</code>.</p>"},{"location":"demos/athena_deduplicate_50k_synthetic.html#you-can-also-read-data-directly-from-a-database","title":"You can also read data directly from a database","text":"<p>Simply add your data to your database and enter the name of the resulting table into the linker object.</p> <p>This can be done with either:</p> <p>wr.catalog.create_parquet_table(...)</p> <p>or</p> <p>wr.s3.to_parquet(...)</p> <p>See the awswrangler API for more info.</p>"},{"location":"demos/example_accuracy_analysis_from_labels_column.html","title":"QA from ground truth column","text":"<pre><code>import pandas as pd \nimport altair as alt\nalt.renderers.enable(\"mimetype\")\n\ndf = pd.read_csv(\"./data/fake_1000.csv\")\ndf.head(2)\n</code></pre> unique_id first_name surname dob city email cluster 0 0 Robert Alan 1971-06-24 NaN robert255@smith.net 0 1 1 Robert Allen 1971-05-24 NaN roberta25@smith.net 0 <pre><code>from splink.duckdb.duckdb_linker import DuckDBLinker\nimport splink.duckdb.duckdb_comparison_template_library as ctl\nimport splink.duckdb.duckdb_comparison_library as cl\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\"),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        cl.levenshtein_at_thresholds(\"email\", 2),\n    ],\n    \"retain_matching_columns\": True,\n    \"retain_intermediate_calculation_columns\": True,\n}\n</code></pre> <pre><code>linker = DuckDBLinker(df, settings, set_up_basic_logging=False)\ndeterministic_rules = [\n    \"l.first_name = r.first_name and levenshtein(r.dob, l.dob) &lt;= 1\",\n    \"l.surname = r.surname and levenshtein(r.dob, l.dob) &lt;= 1\",\n    \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) &lt;= 2\",\n    \"l.email = r.email\"\n]\n\nlinker.estimate_probability_two_random_records_match(deterministic_rules, recall=0.7)\n</code></pre> <pre><code>linker.estimate_u_using_random_sampling(max_pairs=1e6, seed=5)\n</code></pre> <pre><code>session_dob = linker.estimate_parameters_using_expectation_maximisation(\"l.dob = r.dob\")\nsession_email = linker.estimate_parameters_using_expectation_maximisation(\"l.email = r.email\")\n</code></pre> <pre><code>linker.truth_space_table_from_labels_column(\n    \"cluster\", match_weight_round_to_nearest=0.1\n).as_pandas_dataframe(limit=5)\n</code></pre> truth_threshold match_probability row_count p n tp tn fp fn P_rate N_rate tp_rate tn_rate fp_rate fn_rate precision recall f1 0 -17.2 0.000007 4353.0 2031.0 2322.0 2031.0 0.0 2322.0 0.0 0.0 0.533425 1.000000 0.000000 1.000000 0.000000 0.466575 1.000000 0.636278 1 -16.6 0.000010 4353.0 2031.0 2322.0 2029.0 0.0 2322.0 2.0 0.0 0.533425 0.999015 0.000000 1.000000 0.000985 0.466330 0.999015 0.635851 2 -16.5 0.000011 4353.0 2031.0 2322.0 2029.0 234.0 2088.0 2.0 0.0 0.533425 0.999015 0.100775 0.899225 0.000985 0.492835 0.999015 0.660052 3 -16.0 0.000015 4353.0 2031.0 2322.0 2029.0 429.0 1893.0 2.0 0.0 0.533425 0.999015 0.184755 0.815245 0.000985 0.517338 0.999015 0.681788 4 -15.4 0.000023 4353.0 2031.0 2322.0 2027.0 429.0 1893.0 4.0 0.0 0.533425 0.998031 0.184755 0.815245 0.001969 0.517092 0.998031 0.681345 <pre><code>linker.roc_chart_from_labels_column(\"cluster\")\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code>linker.precision_recall_chart_from_labels_column(\"cluster\")\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code># Plot some false positives\nlinker.prediction_errors_from_labels_column(\n    \"cluster\", include_false_negatives=True, include_false_positives=True\n).as_pandas_dataframe(limit=5)\n</code></pre> clerical_match_score found_by_blocking_rules match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name bf_first_name ... tf_city_r bf_city bf_tf_adj_city email_l email_r gamma_email bf_email cluster_l cluster_r match_key 0 1.0 True -4.621133 0.039048 177 178 Ellie Ellie 3 85.824271 ... NaN 1.0 1.0 NaN elliee@oguzmanoc.m -1 1.000000 48 48 0 1 1.0 True -2.770475 0.127823 248 249 Joshua Joshua 3 85.824271 ... NaN 1.0 1.0 NaN j.williams@levine-johnson.com -1 1.000000 64 64 0 2 1.0 True -5.770653 0.017988 324 328 Kai Kai 3 85.824271 ... NaN 1.0 1.0 k.t50eherand@z.ncom k.t50@her.andezncodm 0 0.124985 87 87 0 3 1.0 True -0.142871 0.475262 361 362 Mohammed Mohammed 3 85.824271 ... NaN 1.0 1.0 NaN mohammedfox24@wilson.com -1 1.000000 95 95 0 4 1.0 True -2.770475 0.127823 376 380 Eliza Eliza 3 85.824271 ... NaN 1.0 1.0 NaN elizataylor@marshall.com -1 1.000000 98 98 0 <p>5 rows \u00d7 32 columns</p> <pre><code>records = linker.prediction_errors_from_labels_column(\n    \"cluster\", include_false_negatives=True, include_false_positives=True\n).as_record_dict(limit=5)\n\nlinker.waterfall_chart(records)\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre>"},{"location":"demos/example_accuracy_analysis_from_labels_column.html#quality-assurance-when-you-have-fully-labelled-data","title":"Quality assurance when you have fully labelled data","text":"<p>In this example, our data contains a fully-populated ground-truth column called <code>cluster</code> that enables us to perform accuracy analysis of the final model</p>"},{"location":"demos/example_deduplicate_50k_synthetic.html","title":"Deduplicate 50k rows historical persons","text":"<pre><code>from splink.duckdb.duckdb_linker import DuckDBLinker\nimport altair as alt\nalt.renderers.enable('mimetype')\n\nimport pandas as pd \npd.options.display.max_rows = 1000\ndf = pd.read_parquet(\"./data/historical_figures_with_errors_50k.parquet\")\n</code></pre> <pre><code># Simple settings dictionary will be used for exploratory analysis\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name and l.surname = r.surname\",\n        \"l.surname = r.surname and l.dob = r.dob\",\n        \"l.first_name = r.first_name and l.dob = r.dob\",\n        \"l.postcode_fake = r.postcode_fake and l.first_name = r.first_name\",\n    ],\n}\nlinker = DuckDBLinker(df, settings)\n\nlinker.profile_columns(\n    [\"first_name\", \"postcode_fake\", \"substr(dob, 1,4)\"], top_n=10, bottom_n=5\n)\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code>linker.cumulative_num_comparisons_from_blocking_rules_chart()\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code>import splink.duckdb.duckdb_comparison_template_library as ctl\nimport splink.duckdb.duckdb_comparison_library as cl\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name and l.surname = r.surname\",\n        \"l.surname = r.surname and l.dob = r.dob\",\n        \"l.first_name = r.first_name and l.dob = r.dob\",\n        \"l.postcode_fake = r.postcode_fake and l.first_name = r.first_name\",\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\", term_frequency_adjustments_name=True),\n        ctl.name_comparison(\"surname\", term_frequency_adjustments_name=True),\n        cl.levenshtein_at_thresholds(\"dob\", [1, 2], term_frequency_adjustments=True),\n        cl.levenshtein_at_thresholds(\"postcode_fake\", 2, term_frequency_adjustments=True),\n        cl.exact_match(\"birth_place\", term_frequency_adjustments=True),\n        cl.exact_match(\"occupation\",  term_frequency_adjustments=True),\n    ],\n    \"retain_matching_columns\": True,\n    \"retain_intermediate_calculation_columns\": True,\n    \"max_iterations\": 10,\n    \"em_convergence\": 0.01\n}\n\nlinker = DuckDBLinker(df, settings)\n</code></pre> <pre><code>linker.estimate_probability_two_random_records_match(\n    [\n        \"l.first_name = r.first_name and l.surname = r.surname and l.dob = r.dob\",\n        \"substr(l.first_name,1,2) = substr(r.first_name,1,2) and l.surname = r.surname and substr(l.postcode_fake,1,2) = substr(r.postcode_fake,1,2)\",\n        \"l.dob = r.dob and l.postcode_fake = r.postcode_fake\",\n    ],\n    recall=0.6,\n)\n</code></pre> <pre>\n<code>Probability two random records match is estimated to be  0.000136.\nThis means that amongst all possible pairwise record comparisons, one in 7,362.31 are expected to match.  With 1,279,041,753 total possible comparisons, we expect a total of around 173,728.33 matching pairs\n</code>\n</pre> <pre><code>linker.estimate_u_using_random_sampling(max_pairs=5e6)\n</code></pre> <pre>\n<code>----- Estimating u probabilities using random sampling -----\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n</code>\n</pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='100%'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre>\n<code>\nEstimated u probabilities using random sampling\n\nYour model is not yet fully trained. Missing estimates for:\n    - first_name (no m values are trained).\n    - surname (no m values are trained).\n    - dob (no m values are trained).\n    - postcode_fake (no m values are trained).\n    - birth_place (no m values are trained).\n    - occupation (no m values are trained).\n</code>\n</pre> <pre><code>training_blocking_rule = \"l.first_name = r.first_name and l.surname = r.surname\"\ntraining_session_names = linker.estimate_parameters_using_expectation_maximisation(training_blocking_rule)\n</code></pre> <pre>\n<code>\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.first_name = r.first_name and l.surname = r.surname\n\nParameter estimates will be made for the following comparison(s):\n    - dob\n    - postcode_fake\n    - birth_place\n    - occupation\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - first_name\n    - surname\n\nIteration 1: Largest change in params was -0.527 in probability_two_random_records_match\nIteration 2: Largest change in params was -0.0356 in probability_two_random_records_match\nIteration 3: Largest change in params was 0.0127 in the m_probability of birth_place, level `Exact match`\nIteration 4: Largest change in params was -0.00402 in the m_probability of birth_place, level `All other comparisons`\n\nEM converged after 4 iterations\n\nYour model is not yet fully trained. Missing estimates for:\n    - first_name (no m values are trained).\n    - surname (no m values are trained).\n</code>\n</pre> <pre><code>training_blocking_rule = \"l.dob = r.dob\"\ntraining_session_dob = linker.estimate_parameters_using_expectation_maximisation(training_blocking_rule)\n</code></pre> <pre>\n<code>\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.dob = r.dob\n\nParameter estimates will be made for the following comparison(s):\n    - first_name\n    - surname\n    - postcode_fake\n    - birth_place\n    - occupation\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - dob\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n</code>\n</pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='100%'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre>\n<code>\nIteration 1: Largest change in params was -0.355 in the m_probability of first_name, level `Exact match first_name`\nIteration 2: Largest change in params was 0.0409 in the m_probability of first_name, level `All other comparisons`\nIteration 3: Largest change in params was 0.00542 in the m_probability of first_name, level `All other comparisons`\n\nEM converged after 3 iterations\n\nYour model is fully trained. All comparisons have at least one estimate for their m and u values\n</code>\n</pre> <p>The final match weights can be viewed in the match weights chart:</p> <pre><code>linker.match_weights_chart()\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code>linker.unlinkables_chart()\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code>df_predict = linker.predict()\ndf_e = df_predict.as_pandas_dataframe(limit=5)\ndf_e\n</code></pre> match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name tf_first_name_l tf_first_name_r bf_first_name ... bf_birth_place bf_tf_adj_birth_place occupation_l occupation_r gamma_occupation tf_occupation_l tf_occupation_r bf_occupation bf_tf_adj_occupation match_key 0 18.246490 0.999997 Q2296770-1 Q2296770-14 thomas thomas 3 0.028667 0.028667 43.813525 ... 1.000000 1.000000 politician politician 1 0.088932 0.088932 22.41148 0.451024 0 1 41.233210 1.000000 Q90404618-1 Q90404618-3 emlie emlie 3 0.000099 0.000099 43.813525 ... 169.330475 4.957391 playwright playwright 1 0.002728 0.002728 22.41148 14.700783 0 2 41.233210 1.000000 Q90404618-2 Q90404618-3 emlie emlie 3 0.000099 0.000099 43.813525 ... 169.330475 4.957391 playwright playwright 1 0.002728 0.002728 22.41148 14.700783 0 3 62.523251 1.000000 Q631006-1 Q631006-2 moses moses 3 0.001168 0.001168 43.813525 ... 169.330475 109.062610 rabbi rabbi 1 0.000316 0.000316 22.41148 126.794252 0 4 16.829677 0.999991 Q7795446-1 Q7795446-6 thomas thomas 3 0.028667 0.028667 43.813525 ... 169.330475 2.117721 judge judge 1 0.010479 0.010479 22.41148 3.827751 0 <p>5 rows \u00d7 47 columns</p> <p>You can also view rows in this dataset as a waterfall chart as follows:</p> <pre><code>from splink.charts import waterfall_chart\nrecords_to_plot = df_e.to_dict(orient=\"records\")\nlinker.waterfall_chart(records_to_plot, filter_nulls=False)\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code>clusters = linker.cluster_pairwise_predictions_at_threshold(df_predict, threshold_match_probability=0.95)\n</code></pre> <pre>\n<code>Completed iteration 1, root rows count 621\nCompleted iteration 2, root rows count 107\nCompleted iteration 3, root rows count 37\nCompleted iteration 4, root rows count 9\nCompleted iteration 5, root rows count 0\n</code>\n</pre> <pre><code>linker.cluster_studio_dashboard(df_predict, clusters, \"50k_cluster.html\", sampling_method='by_cluster_size', overwrite=True)\n\nfrom IPython.display import IFrame\n\nIFrame(\n    src=\"./50k_cluster.html\", width=\"100%\", height=1200\n)  \n</code></pre> <pre><code>linker.roc_chart_from_labels_column(\"cluster\",match_weight_round_to_nearest=0.02)\n</code></pre> <pre>\n<code>/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n</code>\n</pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='100%'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code>records = linker.prediction_errors_from_labels_column(\n    \"cluster\",\n    threshold=0.999,\n    include_false_negatives=False,\n    include_false_positives=True,\n).as_record_dict()\nlinker.waterfall_chart(records)\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code># Some of the false negatives will be because they weren't detected by the blocking rules\nrecords = linker.prediction_errors_from_labels_column(\n    \"cluster\",\n    threshold=0.5,\n    include_false_negatives=True,\n    include_false_positives=False,\n).as_record_dict(limit=50)\n\nlinker.waterfall_chart(records)\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre>"},{"location":"demos/example_deduplicate_50k_synthetic.html#linking-a-dataset-of-real-historical-persons","title":"Linking a dataset of real historical persons","text":"<p>In this example, we deduplicate a more realistic dataset.  The data is based on historical persons scraped from wikidata.  Duplicate records are introduced with a variety of errors introduced.</p>"},{"location":"demos/example_febrl3.html","title":"Febrl3 Dedupe","text":"<pre><code>import pandas as pd \nimport altair as alt\nalt.renderers.enable('mimetype')\n\ndf = pd.read_csv(\"./data/febrl/dataset3.csv\", delimiter=\", \", dtype={\"date_of_birth\":str}, engine=\"python\")\ndf[\"cluster\"] = df[\"rec_id\"].apply(lambda x: \"-\".join(x.split('-')[:2]))\ndf.head(2)\n</code></pre> rec_id given_name surname street_number address_1 address_2 suburb postcode state date_of_birth soc_sec_id cluster 0 rec-1496-org mitchell green 7.0 wallaby place delmar cleveland 2119 sa 19560409 1804974 rec-1496 1 rec-552-dup-3 harley mccarthy 177.0 pridhamstreet milton marsden 3165 nsw 19080419 6089216 rec-552 <pre><code>from splink.duckdb.duckdb_linker import DuckDBLinker\n\nsettings = {\n    \"unique_id_column_name\": \"rec_id\",\n    \"link_type\": \"dedupe_only\",\n}\n\nlinker = DuckDBLinker(df, settings)\n</code></pre> <pre><code>linker.missingness_chart()\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code>linker.profile_columns(list(df.columns))\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code>blocking_rules = [\n        \"l.soc_sec_id = r.soc_sec_id\",\n        \"l.given_name = r.given_name\",\n        \"l.surname = r.surname\",\n        \"l.date_of_birth = r.date_of_birth\",\n        \"l.postcode = r.postcode\"\n]\nlinker.cumulative_num_comparisons_from_blocking_rules_chart(blocking_rules)\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code>from splink.duckdb.duckdb_linker import DuckDBLinker\nimport splink.duckdb.duckdb_comparison_library as cl\nimport splink.duckdb.duckdb_comparison_template_library as ctl\n\n\nsettings = {\n    \"unique_id_column_name\": \"rec_id\",\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": blocking_rules,\n    \"comparisons\": [\n        ctl.name_comparison(\"given_name\", term_frequency_adjustments_name=True),\n        ctl.name_comparison(\"surname\", term_frequency_adjustments_name=True),\n        cl.levenshtein_at_thresholds(\"date_of_birth\", [1, 2]),\n        cl.levenshtein_at_thresholds(\"soc_sec_id\", [2]),\n        cl.exact_match(\"street_number\", term_frequency_adjustments=True),\n        cl.exact_match(\"postcode\", term_frequency_adjustments=True),\n    ],\n    \"retain_intermediate_calculation_columns\": True,\n}\n\nlinker = DuckDBLinker(df, settings)\n</code></pre> <pre><code>deterministic_rules = [\n    \"l.soc_sec_id = r.soc_sec_id\",\n    \"l.given_name = r.given_name and l.surname = r.surname and l.date_of_birth = r.date_of_birth\",\n    \"l.given_name = r.surname and l.surname = r.given_name and l.date_of_birth = r.date_of_birth\"\n]\n\nlinker.estimate_probability_two_random_records_match(deterministic_rules, recall=0.9)\n</code></pre> <pre>\n<code>Probability two random records match is estimated to be  0.000527.\nThis means that amongst all possible pairwise record comparisons, one in 1,899.32 are expected to match.  With 12,497,500 total possible comparisons, we expect a total of around 6,580.00 matching pairs\n</code>\n</pre> <pre><code>linker.estimate_u_using_random_sampling(max_pairs=1e6)\n</code></pre> <pre>\n<code>----- Estimating u probabilities using random sampling -----\n\nEstimated u probabilities using random sampling\n\nYour model is not yet fully trained. Missing estimates for:\n    - given_name (no m values are trained).\n    - surname (no m values are trained).\n    - date_of_birth (no m values are trained).\n    - soc_sec_id (no m values are trained).\n    - street_number (no m values are trained).\n    - postcode (no m values are trained).\n</code>\n</pre> <pre><code>comparison = linker._settings_obj.comparisons[2].as_dict()\n</code></pre> <pre><code>session_dob = linker.estimate_parameters_using_expectation_maximisation(\"substr(l.date_of_birth,1,3) = substr(r.date_of_birth,1,3)\")\nsession_postcode = linker.estimate_parameters_using_expectation_maximisation(\"substr(l.postcode,1,2) = substr(r.postcode,1,2)\")\n</code></pre> <pre>\n<code>\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nsubstr(l.date_of_birth,1,3) = substr(r.date_of_birth,1,3)\n\nParameter estimates will be made for the following comparison(s):\n    - given_name\n    - surname\n    - soc_sec_id\n    - street_number\n    - postcode\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - date_of_birth\n\nIteration 1: Largest change in params was -0.507 in probability_two_random_records_match\nIteration 2: Largest change in params was -0.00766 in the m_probability of soc_sec_id, level `All other comparisons`\nIteration 3: Largest change in params was -0.000717 in the m_probability of soc_sec_id, level `All other comparisons`\nIteration 4: Largest change in params was -8.36e-05 in the m_probability of soc_sec_id, level `All other comparisons`\n\nEM converged after 4 iterations\n\nYour model is not yet fully trained. Missing estimates for:\n    - date_of_birth (no m values are trained).\n\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nsubstr(l.postcode,1,2) = substr(r.postcode,1,2)\n\nParameter estimates will be made for the following comparison(s):\n    - given_name\n    - surname\n    - date_of_birth\n    - soc_sec_id\n    - street_number\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - postcode\n\nIteration 1: Largest change in params was -0.228 in probability_two_random_records_match\nIteration 2: Largest change in params was 0.0178 in the m_probability of soc_sec_id, level `Exact match`\nIteration 3: Largest change in params was -0.0013 in the m_probability of soc_sec_id, level `All other comparisons`\nIteration 4: Largest change in params was -0.000103 in the m_probability of soc_sec_id, level `All other comparisons`\nIteration 5: Largest change in params was -8.43e-06 in the m_probability of soc_sec_id, level `All other comparisons`\n\nEM converged after 5 iterations\n\nYour model is fully trained. All comparisons have at least one estimate for their m and u values\n</code>\n</pre> <pre><code>linker.match_weights_chart()\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code>results = linker.predict(threshold_match_probability=0.2)\n</code></pre> <pre><code>linker.roc_chart_from_labels_column(\"cluster\")\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code>pred_errors_df = linker.prediction_errors_from_labels_column(\"cluster\").as_pandas_dataframe()\nlen(pred_errors_df)\npred_errors_df.head()\n</code></pre> clerical_match_score found_by_blocking_rules match_weight match_probability rec_id_l rec_id_r given_name_l given_name_r gamma_given_name tf_given_name_l ... postcode_l postcode_r gamma_postcode tf_postcode_l tf_postcode_r bf_postcode bf_tf_adj_postcode cluster_l cluster_r match_key 0 1.0 True -1.190222 0.304704 rec-941-dup-0 rec-941-dup-3 coby cobuy 2 0.001032 ... 3078 3088 0 0.0010 0.0008 0.217529 1.0 rec-941 rec-941 0 1 1.0 True -5.483332 0.021865 rec-1727-dup-1 rec-1727-org campblel joshua 0 0.000206 ... 3189 3198 0 0.0008 0.0008 0.217529 1.0 rec-1727 rec-1727 0 2 1.0 True -9.000515 0.001949 rec-1320-dup-1 rec-1320-dup-4 amber kexel 0 0.004542 ... 461 4061 0 0.0002 0.0006 0.217529 1.0 rec-1320 rec-1320 0 3 1.0 True -2.102919 0.188830 rec-1212-dup-0 rec-1212-dup-1 bkows lucy 0 0.000206 ... 3084 3085 0 0.0018 0.0002 0.217529 1.0 rec-1212 rec-1212 0 4 1.0 True -3.344909 0.089601 rec-1460-dup-0 rec-1460-dup-2 benjamin noah 0 0.011148 ... 7305 7307 0 0.0014 0.0002 0.217529 1.0 rec-1460 rec-1460 0 <p>5 rows \u00d7 45 columns</p> <pre><code>records = linker.prediction_errors_from_labels_column(\"cluster\").as_record_dict(limit=10)\nlinker.waterfall_chart(records)\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre>"},{"location":"demos/example_febrl3.html#deduplicating-the-febrl3-dataset","title":"Deduplicating the febrl3 dataset","text":"<p>See A.2  here and here for the source of this data</p>"},{"location":"demos/example_febrl4.html","title":"Febrl4 link-only","text":"<p>Firstly let's read in the data and have a little look at it</p> <pre><code>import pandas as pd \nimport altair as alt\nfrom IPython.display import IFrame\nalt.renderers.enable('mimetype')\n\ndef read_and_prepare(fn):\n    df = pd.read_csv(\n        fn,\n        delimiter=\", \",\n        dtype={\"date_of_birth\": str},\n        engine=\"python\"\n    )\n    df[\"cluster\"] = df[\"rec_id\"].apply(lambda x: \"-\".join(x.split('-')[:2]))\n    return df\ndfs = [\n    read_and_prepare(f\"./data/febrl/dataset4{dataset}.csv\")\n    for dataset in [\"a\", \"b\"]\n]\n\ndisplay(dfs[0].head(2))\ndisplay(dfs[1].head(2))\n</code></pre> rec_id given_name surname street_number address_1 address_2 suburb postcode state date_of_birth soc_sec_id cluster 0 rec-1070-org michaela neumann 8.0 stanley street miami winston hills 4223 nsw 19151111 5304218 rec-1070 1 rec-1016-org courtney painter 12.0 pinkerton circuit bega flats richlands 4560 vic 19161214 4066625 rec-1016 rec_id given_name surname street_number address_1 address_2 suburb postcode state date_of_birth soc_sec_id cluster 0 rec-561-dup-0 elton NaN 3.0 light setreet pinehill windermere 3212 vic 19651013 1551941 rec-561 1 rec-2642-dup-0 mitchell maxon 47.0 edkins street lochaoair north ryde 3355 nsw 19390212 8859999 rec-2642 <p>Next, to better understand which variables will prove useful in linking, we have a look at how populated each column is, as well as the distribution of unique values within each</p> <pre><code>from splink.duckdb.duckdb_linker import DuckDBLinker\n\nbasic_settings = {\n    \"unique_id_column_name\": \"rec_id\",\n    \"link_type\": \"link_only\",\n    # NB as we are linking one-one, we know the probability that a random pair will be a match\n    # hence we could set:\n    # \"probability_two_random_records_match\": 1/5000,\n    # however we will not specify this here, as we will use this as a check that\n    # our estimation procedure returns something sensible\n}\n\nlinker = DuckDBLinker(dfs, basic_settings)\n</code></pre> <pre><code>linker.missingness_chart()\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code>cols_to_profile = list(dfs[0].columns)\ncols_to_profile = [col for col in cols_to_profile if col not in (\"rec_id\", \"cluster\")]\nlinker.profile_columns(cols_to_profile)\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <p>Next let's come up with some candidate blocking rules, which define which record comparisons are generated, and have a look at how many comparisons each will generate.</p> <p>For blocking rules that we use in prediction, our aim is to have the union of all rules cover all true matches, whilst avoiding generating so many comparisons that it becomes computationally intractable - i.e. each true match should have at least one of the following conditions holding.</p> <pre><code>blocking_rules = [\n    \"l.given_name = r.given_name AND l.surname = r.surname\",\n    \"l.date_of_birth = r.date_of_birth\",\n    \"l.soc_sec_id = r.soc_sec_id\",\n    \"l.state = r.state AND l.address_1 = r.address_1\",\n    \"l.street_number = r.street_number AND l.address_1 = r.address_1\",\n    \"l.postcode = r.postcode\",\n]\nlinker.cumulative_num_comparisons_from_blocking_rules_chart(blocking_rules)\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <p>The broadest rule, having a matching postcode, unsurpisingly gives the largest number of comparisons. For this small dataset we still have a very manageable number, but if it was larger we might have needed to include a further <code>AND</code> condition with it to break the number of comparisons further.</p> <p>Now we get the full settings by including the blocking rules, as well as deciding the actual comparisons we will be including in our model.</p> <p>We will define two models, each with a separate linker with different settings, so that we can compare performance. One will be a very basic model, whilst the other will include a lot more detail.</p> <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\nimport splink.duckdb.duckdb_comparison_library as cl\nimport splink.duckdb.duckdb_comparison_template_library as ctl\n\n# the simple model only considers a few columns, and only two comparison levels for each\nsimple_model_settings = {\n    **basic_settings,\n    \"blocking_rules_to_generate_predictions\": blocking_rules,\n    \"comparisons\": [\n        cl.exact_match(\"given_name\", term_frequency_adjustments=True),\n        cl.exact_match(\"surname\", term_frequency_adjustments=True),\n        cl.exact_match(\"street_number\", term_frequency_adjustments=True),\n    ],\n    \"retain_intermediate_calculation_columns\": True,\n}\n# the detailed model considers more columns, using the information we saw in the exploratory phase\n# we also include further comparison levels to account for typos and other differences\ndetailed_model_settings = {\n    **basic_settings,\n    \"blocking_rules_to_generate_predictions\": blocking_rules,\n    \"comparisons\": [\n        ctl.name_comparison(\"given_name\", term_frequency_adjustments_name=True),\n        ctl.name_comparison(\"surname\", term_frequency_adjustments_name=True),\n        cl.levenshtein_at_thresholds(\"date_of_birth\", [1, 2]),\n        cl.levenshtein_at_thresholds(\"soc_sec_id\", [1, 2]),\n        cl.exact_match(\"street_number\", term_frequency_adjustments=True),\n        cl.levenshtein_at_thresholds(\"postcode\", [1, 2], term_frequency_adjustments=True),\n        # we don't consider further location columns as they will be strongly correlated with postcode\n    ],\n    \"retain_intermediate_calculation_columns\": True,\n}\n\n\nlinker_simple = DuckDBLinker(dfs, simple_model_settings)\nlinker_detailed = DuckDBLinker(dfs, detailed_model_settings)\n</code></pre> <p>We need to furnish our models with parameter estimates so that we can generate results. We will focus on the detailed model, generating the values for the simple model at the end</p> <p>We can instead estimate the probability two random records match, and compare with the known value of 1/5000 = 0.0002, to see how well our estimation procedure works.</p> <p>To do this we come up with some deterministic rules - the aim here is that we generate very few false positives (i.e. we expect that the majority of records with at least one of these conditions holding are true matches), whilst also capturing the majority of matches - our guess here is that these two rules should capture 80% of all matches.</p> <pre><code>deterministic_rules = [\n    \"l.soc_sec_id = r.soc_sec_id\",\n    \"l.given_name = r.given_name and l.surname = r.surname and l.date_of_birth = r.date_of_birth\",\n]\n\nlinker_detailed.estimate_probability_two_random_records_match(deterministic_rules, recall=0.8)\n</code></pre> <pre>\n<code>Probability two random records match is estimated to be  0.000238.\nThis means that amongst all possible pairwise record comparisons, one in 4,195.51 are expected to match.  With 25,000,000 total possible comparisons, we expect a total of around 5,958.75 matching pairs\n</code>\n</pre> <p>Even playing around with changing these deterministic rules, or the nominal recall leaves us with an answer which is pretty close to our known value</p> <p>Next we estimate <code>u</code> and <code>m</code> values for each comparison, so that we can move to generating predictions</p> <pre><code>linker_detailed.estimate_u_using_random_sampling(max_pairs=1e7)\n</code></pre> <pre>\n<code>----- Estimating u probabilities using random sampling -----\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n</code>\n</pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='100%'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre>\n<code>\nEstimated u probabilities using random sampling\n\nYour model is not yet fully trained. Missing estimates for:\n    - given_name (no m values are trained).\n    - surname (no m values are trained).\n    - date_of_birth (no m values are trained).\n    - soc_sec_id (no m values are trained).\n    - street_number (no m values are trained).\n    - postcode (no m values are trained).\n</code>\n</pre> <p>When training the <code>m</code> values using expectation maximisation, we need somre more blocking rules to reduce the total number of comparisons. For each rule, we want to ensure that we have neither proportionally too many matches, or too few.</p> <p>We must run this multiple times using different rules so that we can obtain estimates for all comparisons - if we block on e.g. <code>date_of_birth</code>, then we cannot compute the <code>m</code> values for the <code>date_of_birth</code> comparison, as we have only looked at records where these match.</p> <pre><code>session_dob = linker_detailed.estimate_parameters_using_expectation_maximisation(\n    \"l.date_of_birth = r.date_of_birth\"\n)\nsession_pc = linker_detailed.estimate_parameters_using_expectation_maximisation(\n    \"l.postcode = r.postcode\"\n)\n</code></pre> <pre>\n<code>\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.date_of_birth = r.date_of_birth\n\nParameter estimates will be made for the following comparison(s):\n    - given_name\n    - surname\n    - soc_sec_id\n    - street_number\n    - postcode\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - date_of_birth\n\nIteration 1: Largest change in params was 0.362 in probability_two_random_records_match\nIteration 2: Largest change in params was 0.00425 in probability_two_random_records_match\nIteration 3: Largest change in params was 5.61e-05 in the m_probability of soc_sec_id, level `All other comparisons`\n\nEM converged after 3 iterations\n\nYour model is not yet fully trained. Missing estimates for:\n    - date_of_birth (no m values are trained).\n\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.postcode = r.postcode\n\nParameter estimates will be made for the following comparison(s):\n    - given_name\n    - surname\n    - date_of_birth\n    - soc_sec_id\n    - street_number\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - postcode\n\nIteration 1: Largest change in params was 0.0293 in the m_probability of date_of_birth, level `All other comparisons`\nIteration 2: Largest change in params was 0.000301 in the m_probability of date_of_birth, level `All other comparisons`\nIteration 3: Largest change in params was 4.38e-06 in the m_probability of soc_sec_id, level `All other comparisons`\n\nEM converged after 3 iterations\n\nYour model is fully trained. All comparisons have at least one estimate for their m and u values\n</code>\n</pre> <p>If we wish we can have a look at how our parameter estimates changes over these training sessions</p> <pre><code># session_dob.m_u_values_interactive_history_chart()\n</code></pre> <p>For variables that aren't used in the <code>m</code>-training blocking rules, we have two estimates --- one from each of the training sessions (see for example <code>street_number</code>). We can have a look at how the values compare between them, to ensure that we don't have drastically different values, which may be indicative of an issue.</p> <pre><code>linker_detailed.parameter_estimate_comparisons_chart()\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <p>We repeat our parameter estimations for the simple model in much the same fashion</p> <pre><code>linker_simple.estimate_probability_two_random_records_match(deterministic_rules, recall=0.8)\nlinker_simple.estimate_u_using_random_sampling(max_pairs=1e7)\nsession_ssid = linker_simple.estimate_parameters_using_expectation_maximisation(\n    \"l.given_name = r.given_name\"\n)\nsession_pc = linker_simple.estimate_parameters_using_expectation_maximisation(\n    \"l.street_number = r.street_number\"\n)\n# linker_simple.parameter_estimate_comparisons_chart()\n</code></pre> <pre>\n<code>Probability two random records match is estimated to be  0.000238.\nThis means that amongst all possible pairwise record comparisons, one in 4,195.51 are expected to match.  With 25,000,000 total possible comparisons, we expect a total of around 5,958.75 matching pairs\n----- Estimating u probabilities using random sampling -----\n\nEstimated u probabilities using random sampling\n\nYour model is not yet fully trained. Missing estimates for:\n    - given_name (no m values are trained).\n    - surname (no m values are trained).\n    - street_number (no m values are trained).\n\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.given_name = r.given_name\n\nParameter estimates will be made for the following comparison(s):\n    - surname\n    - street_number\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - given_name\n\nIteration 1: Largest change in params was 0.101 in the m_probability of surname, level `All other comparisons`\nIteration 2: Largest change in params was -0.0467 in the m_probability of surname, level `Exact match`\nIteration 3: Largest change in params was -0.0326 in the m_probability of surname, level `Exact match`\nIteration 4: Largest change in params was -0.0215 in the m_probability of surname, level `Exact match`\nIteration 5: Largest change in params was 0.0136 in the m_probability of surname, level `All other comparisons`\nIteration 6: Largest change in params was -0.00839 in the m_probability of surname, level `Exact match`\nIteration 7: Largest change in params was -0.00516 in the m_probability of surname, level `Exact match`\nIteration 8: Largest change in params was -0.00319 in the m_probability of surname, level `Exact match`\nIteration 9: Largest change in params was 0.00198 in the m_probability of surname, level `All other comparisons`\nIteration 10: Largest change in params was 0.00124 in the m_probability of surname, level `All other comparisons`\nIteration 11: Largest change in params was -0.000777 in the m_probability of surname, level `Exact match`\nIteration 12: Largest change in params was -0.00049 in the m_probability of surname, level `Exact match`\nIteration 13: Largest change in params was 0.00031 in the m_probability of surname, level `All other comparisons`\nIteration 14: Largest change in params was -0.000197 in the m_probability of surname, level `Exact match`\nIteration 15: Largest change in params was 0.000125 in the m_probability of surname, level `All other comparisons`\nIteration 16: Largest change in params was 7.96e-05 in the m_probability of surname, level `All other comparisons`\n\nEM converged after 16 iterations\n\nYour model is not yet fully trained. Missing estimates for:\n    - given_name (no m values are trained).\n\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.street_number = r.street_number\n\nParameter estimates will be made for the following comparison(s):\n    - given_name\n    - surname\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - street_number\n\nIteration 1: Largest change in params was -0.0665 in the m_probability of given_name, level `Exact match`\nIteration 2: Largest change in params was -0.0432 in the m_probability of given_name, level `Exact match`\nIteration 3: Largest change in params was -0.0275 in the m_probability of given_name, level `Exact match`\nIteration 4: Largest change in params was 0.0159 in the m_probability of given_name, level `All other comparisons`\nIteration 5: Largest change in params was 0.00869 in the m_probability of given_name, level `All other comparisons`\nIteration 6: Largest change in params was -0.00471 in the m_probability of given_name, level `Exact match`\nIteration 7: Largest change in params was -0.00258 in the m_probability of given_name, level `Exact match`\nIteration 8: Largest change in params was 0.00145 in the m_probability of given_name, level `All other comparisons`\nIteration 9: Largest change in params was 0.000842 in the m_probability of given_name, level `All other comparisons`\nIteration 10: Largest change in params was -0.000505 in the m_probability of given_name, level `Exact match`\nIteration 11: Largest change in params was -0.000313 in the m_probability of given_name, level `Exact match`\nIteration 12: Largest change in params was -0.000214 in the m_probability of surname, level `Exact match`\nIteration 13: Largest change in params was 0.000154 in the m_probability of surname, level `All other comparisons`\nIteration 14: Largest change in params was 0.00011 in the m_probability of surname, level `All other comparisons`\nIteration 15: Largest change in params was -7.78e-05 in the m_probability of surname, level `Exact match`\n\nEM converged after 15 iterations\n\nYour model is fully trained. All comparisons have at least one estimate for their m and u values\n</code>\n</pre> <pre><code># import json\n# we can have a look at the full settings if we wish, including the values of our estimated parameters:\n# print(json.dumps(linker_detailed._settings_obj.as_dict(), indent=2))\n# we can also get a handy summary of of the model in an easily readable format if we wish:\n# print(linker_detailed._settings_obj.human_readable_description)\n# (we suppress output here for brevity)\n</code></pre> <p>We can now visualise some of the details of our models. We can look at the match weights, which tell us the relative importance for/against a match for each of our comparsion levels.</p> <p>Comparing the two models will show the added benefit we get in the more detailed model --- what in the simple model is classed as 'all other comparisons' is instead broken down further, and we can see that the detail of how this is broken down in fact gives us quite a bit of useful information about the likelihood of a match.</p> <pre><code>linker_simple.match_weights_chart()\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code>linker_detailed.match_weights_chart()\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <p>As well as the match weights, which give us an idea of the overall effect of each comparison level, we can also look at the individual <code>u</code> and <code>m</code> parameter estimates, which tells us about the prevalence of coincidences and mistakes (for further details/explanation about this see this article). We might want to revise aspects of our model based on the information we ascertain here.</p> <p>Note however that some of these values are very small, which is why the match weight chart is often more useful for getting a decent picture of things.</p> <pre><code># linker_simple.m_u_parameters_chart()\nlinker_detailed.m_u_parameters_chart()\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <p>It is also useful to have a look at unlinkable records - these are records which do not contain enough information to be linked at some match probability threshold. We can figure this out be seeing whether records are able to be matched with themselves.</p> <p>This is of course relative to the information we have put into the model - we see that in our simple model, at a 99% match threshold nearly 10% of records are unlinkable, as we have not included enough information in the model for distinct records to be adequately distinguished; this is not an issue in our more detailed model.</p> <pre><code>linker_simple.unlinkables_chart()\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code>linker_detailed.unlinkables_chart()\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <p>Our simple model doesn't do terribly, but suffers if we want to have a high match probability --- to be 99% certain of matches we have ~10% of records that we will be unable to link.</p> <p>Our detailed model, however, has enough nuance that we can at least self-link records.</p> <pre><code>predictions = linker_detailed.predict()\ndf_predictions = predictions.as_pandas_dataframe()\ndf_predictions.head(5)\n</code></pre> match_weight match_probability source_dataset_l source_dataset_r rec_id_l rec_id_r given_name_l given_name_r gamma_given_name tf_given_name_l ... gamma_postcode tf_postcode_l tf_postcode_r bf_postcode bf_tf_adj_postcode state_l state_r address_1_l address_1_r match_key 0 33.218065 1.0 __splink__input_table_0 __splink__input_table_1 rec-3440-org rec-3440-dup-0 matthew matthew 3 0.007251 ... 1 0.0002 0.0003 0.917896 1.000000 vic vic cowcher place cowcher dlace 0 1 42.790666 1.0 __splink__input_table_0 __splink__input_table_1 rec-4028-org rec-4028-dup-0 lachlan lachlan 3 0.011705 ... 3 0.0014 0.0014 729.077700 0.823831 sa sa lienhop street lienhop street 0 2 37.893804 1.0 __splink__input_table_0 __splink__input_table_1 rec-555-org rec-555-dup-0 harry harry 3 0.004765 ... 3 0.0013 0.0013 729.077700 0.887202 nsw nsw hodgson crescent charteris crescent 0 3 43.205900 1.0 __splink__input_table_0 __splink__input_table_1 rec-1155-org rec-1155-dup-0 jessica jessica 3 0.008805 ... 3 0.0024 0.0024 729.077700 0.480568 sa sa dugan street dugan street 0 4 33.661827 1.0 __splink__input_table_0 __splink__input_table_1 rec-18-org rec-18-dup-0 riley riley 3 0.005904 ... 3 0.0008 0.0008 729.077700 1.441704 vic nsw clem hill street clem hill street 0 <p>5 rows \u00d7 47 columns</p> <p>We can see how our model performs at different probability thresholds, with a couple of options depending on the space we wish to view things</p> <pre><code># linker_detailed.roc_chart_from_labels_column(\"cluster\")\nlinker_detailed.precision_recall_chart_from_labels_column(\"cluster\")\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <p>and we can easily see how many individuals we identify and link by looking at clusters generated at some threshold match probability of interest - in this example 99%</p> <pre><code>clusters = linker_detailed.cluster_pairwise_predictions_at_threshold(predictions, threshold_match_probability=0.99)\ndf_clusters = clusters.as_pandas_dataframe().sort_values(\"cluster_id\")\ndf_clusters.groupby(\"cluster_id\").size().value_counts()\n</code></pre> <pre>\n<code>Completed iteration 1, root rows count 0\n</code>\n</pre> <pre>\n<code>2    4950\n1     100\nName: count, dtype: int64</code>\n</pre> <p>In this case, we happen to know what the true links are, so we can manually inspect the ones that are doing worst to see what our model is not capturing - i.e. where we have false negatives.</p> <p>Similarly, we can look at the non-links which are performing the best, to see whether we have an issue with false positives.</p> <p>Ordinarily we would not have this luxury, and so would need to dig a bit deeper for clues as to how to improve our model, such as manually inspecting records across threshold probabilities, </p> <pre><code>df_predictions[\"cluster_l\"] = df_predictions[\"rec_id_l\"].apply(lambda x: \"-\".join(x.split('-')[:2]))\ndf_predictions[\"cluster_r\"] = df_predictions[\"rec_id_r\"].apply(lambda x: \"-\".join(x.split('-')[:2]))\ndf_true_links = df_predictions[df_predictions[\"cluster_l\"] == df_predictions[\"cluster_r\"]].sort_values(\"match_probability\")\n</code></pre> <pre><code>records_to_view = 3\nlinker_detailed.waterfall_chart(df_true_links.head(records_to_view).to_dict(orient=\"records\"))\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code>df_non_links = df_predictions[df_predictions[\"cluster_l\"] != df_predictions[\"cluster_r\"]].sort_values(\"match_probability\", ascending=False)\nlinker_detailed.waterfall_chart(df_non_links.head(records_to_view).to_dict(orient=\"records\"))\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code># we need to append a full name column to our source data frames\n# so that we can use it for term frequency adjustments\ndfs[0][\"full_name\"] = dfs[0][\"given_name\"] + \"_\" + dfs[0][\"surname\"]\ndfs[1][\"full_name\"] = dfs[1][\"given_name\"] + \"_\" + dfs[1][\"surname\"]\n\n\nextended_model_settings = {\n    **basic_settings,\n    \"blocking_rules_to_generate_predictions\": blocking_rules,\n    \"comparisons\": [\n        {\n            \"output_column_name\": \"Full name\",\n            \"comparison_levels\": [\n                {\n                    \"sql_condition\": \"(given_name_l IS NULL OR given_name_r IS NULL) and (surname_l IS NULL OR surname_r IS NULL)\",\n                    \"label_for_charts\": \"Null\",\n                    \"is_null_level\": True,\n                },\n                # full name match\n                cll.exact_match_level(\"full_name\", term_frequency_adjustments=True),\n                # typos - keep levels across full name rather than scoring separately\n                cll.jaro_winkler_level(\"full_name\", 0.9),\n                cll.jaro_winkler_level(\"full_name\", 0.7),\n                # name switched\n                cll.columns_reversed_level(\"given_name\", \"surname\"),\n                # name switched + typo\n                {\n                    \"sql_condition\": \"jaro_winkler_similarity(given_name_l, surname_r) + jaro_winkler_similarity(surname_l, given_name_r) &gt;= 1.8\",\n                    \"label_for_charts\": \"switched + jaro_winkler_similarity &gt;= 1.8\"\n                },\n                {\n                    \"sql_condition\": \"jaro_winkler_similarity(given_name_l, surname_r) + jaro_winkler_similarity(surname_l, given_name_r) &gt;= 1.4\",\n                    \"label_for_charts\": \"switched + jaro_winkler_similarity &gt;= 1.4\"\n                },\n                # single name match\n                cll.exact_match_level(\"given_name\", term_frequency_adjustments=True),\n                cll.exact_match_level(\"surname\", term_frequency_adjustments=True),\n                # single name cross-match\n                {\n                    \"sql_condition\": \"given_name_l = surname_r OR surname_l = given_name_r\",\n                    \"label_for_charts\": \"single name cross-matches\"\n                },                # single name typos\n                cll.jaro_winkler_level(\"given_name\", 0.9),\n                cll.jaro_winkler_level(\"surname\", 0.9),\n                # the rest\n                cll.else_level()\n            ]\n        },\n        {\n            \"output_column_name\": \"Date of birth\",\n            \"comparison_levels\": [\n                cll.null_level(\"date_of_birth\"),\n                cll.exact_match_level(\"date_of_birth\", term_frequency_adjustments=True),\n                cll.levenshtein_level(\"date_of_birth\", 1),\n                # this is the extra level\n                cll.and_( # we have the same set of letters, and two levenshtein edits between them - i.e. a single transposition\n                    cll.jaccard_level(\"date_of_birth\", 1),\n                    cll.levenshtein_level(\"date_of_birth\", 2)),\n                cll.levenshtein_level(\"date_of_birth\", 2),\n                cll.else_level()\n            ]\n        },\n        {\n            \"output_column_name\": \"Social security ID\",\n            \"comparison_levels\": [\n                cll.null_level(\"soc_sec_id\"),\n                cll.exact_match_level(\"soc_sec_id\", term_frequency_adjustments=True),\n                cll.levenshtein_level(\"soc_sec_id\", 1),\n                # this is the extra level\n                cll.and_( # we have the same set of letters, and two levenshtein edits between them - i.e. a single transposition\n                    cll.jaccard_level(\"soc_sec_id\", 1),\n                    cll.levenshtein_level(\"soc_sec_id\", 2)),\n                cll.levenshtein_level(\"soc_sec_id\", 2),\n                cll.else_level()\n            ]\n        },\n        {\n            \"output_column_name\": \"Street number\",\n            \"comparison_levels\": [\n                cll.null_level(\"street_number\"),\n                cll.exact_match_level(\"street_number\", term_frequency_adjustments=True),\n                cll.levenshtein_level(\"street_number\", 1),\n                # this is the extra level\n                cll.and_( # we have the same set of letters, and two levenshtein edits between them - i.e. a single transposition\n                    cll.jaccard_level(\"street_number\", 1),\n                    cll.levenshtein_level(\"street_number\", 2)),\n                cll.else_level()\n            ]\n        },\n        {\n            \"output_column_name\": \"Postcode\",\n            \"comparison_levels\": [\n                cll.null_level(\"postcode\"),\n                cll.exact_match_level(\"postcode\", term_frequency_adjustments=True),\n                cll.levenshtein_level(\"postcode\", 1),\n                cll.and_( # we have the same set of letters, and two levenshtein edits between them - i.e. a single transposition\n                    cll.jaccard_level(\"postcode\", 1),\n                    cll.levenshtein_level(\"postcode\", 2)),\n                cll.levenshtein_level(\"postcode\", 2),\n                cll.else_level()\n            ]\n        },\n        # we don't consider further location columns as they will be strongly correlated with postcode\n    ],\n    \"retain_intermediate_calculation_columns\": True,\n}\n</code></pre> <pre><code># train\nlinker_advanced = DuckDBLinker(dfs, extended_model_settings)\nlinker_advanced.estimate_probability_two_random_records_match(deterministic_rules, recall=0.8)\n# we increase target rows to improve accuracy for u values in full name comparison, as we have subdivided the data more finely\nlinker_advanced.estimate_u_using_random_sampling(max_pairs=1e8)\nsession_dob = linker_advanced.estimate_parameters_using_expectation_maximisation(\n    \"l.date_of_birth = r.date_of_birth\"\n)\nsession_pc = linker_advanced.estimate_parameters_using_expectation_maximisation(\n    \"l.postcode = r.postcode\"\n)\n</code></pre> <pre>\n<code>Probability two random records match is estimated to be  0.000238.\nThis means that amongst all possible pairwise record comparisons, one in 4,195.51 are expected to match.  With 25,000,000 total possible comparisons, we expect a total of around 5,958.75 matching pairs\n----- Estimating u probabilities using random sampling -----\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n</code>\n</pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='100%'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre>\n<code>\nEstimated u probabilities using random sampling\n\nYour model is not yet fully trained. Missing estimates for:\n    - Full name (no m values are trained).\n    - Date of birth (no m values are trained).\n    - Social security ID (no m values are trained).\n    - Street number (no m values are trained).\n    - Postcode (no m values are trained).\n\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.date_of_birth = r.date_of_birth\n\nParameter estimates will be made for the following comparison(s):\n    - Full name\n    - Social security ID\n    - Street number\n    - Postcode\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - Date of birth\n\nIteration 1: Largest change in params was -0.484 in the m_probability of Full name, level `Exact match`\nIteration 2: Largest change in params was 0.00197 in probability_two_random_records_match\nIteration 3: Largest change in params was 8.25e-06 in probability_two_random_records_match\n\nEM converged after 3 iterations\n\nYour model is not yet fully trained. Missing estimates for:\n    - Date of birth (no m values are trained).\n\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.postcode = r.postcode\n\nParameter estimates will be made for the following comparison(s):\n    - Full name\n    - Date of birth\n    - Social security ID\n    - Street number\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - Postcode\n\nIteration 1: Largest change in params was 0.0333 in the m_probability of Date of birth, level `All other comparisons`\nIteration 2: Largest change in params was 0.000447 in the m_probability of Date of birth, level `All other comparisons`\nIteration 3: Largest change in params was 1.09e-05 in the m_probability of Social security ID, level `All other comparisons`\n\nEM converged after 3 iterations\n\nYour model is fully trained. All comparisons have at least one estimate for their m and u values\n</code>\n</pre> <pre><code># linker_advanced.parameter_estimate_comparisons_chart()\n</code></pre> <pre><code>linker_advanced.match_weights_chart()\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code>predictions_adv = linker_advanced.predict()\ndf_predictions_adv = predictions_adv.as_pandas_dataframe()\nclusters_adv = linker_advanced.cluster_pairwise_predictions_at_threshold(predictions_adv, threshold_match_probability=0.99)\ndf_clusters_adv = clusters_adv.as_pandas_dataframe().sort_values(\"cluster_id\")\ndf_clusters_adv.groupby(\"cluster_id\").size().value_counts()\n</code></pre> <pre>\n<code>Completed iteration 1, root rows count 0\n</code>\n</pre> <pre>\n<code>2    4970\n1      60\nName: count, dtype: int64</code>\n</pre> <p>This is a pretty modest improvement on our previous model - however it is worth re-iterating that we should not necessarily expect to recover all matches, as in several cases it may be unreasonable for a model to have reasonable confidence that two records refer to the same entity.</p> <p>If we wished to improve matters we could iterate on this process - investigating where our model is not performing as we would hope, and seeing how we can adjust these areas to address these shortcomings.</p>"},{"location":"demos/example_febrl4.html#linking-the-febrl4-datasets","title":"Linking the febrl4 datasets","text":"<p>See A.2 here and here for the source of this data.</p> <p>It consists of two datasets, A and B, of 5000 records each, with each record in dataset A having a corresponding record in dataset B. The aim will be to capture as many of those 5000 true links as possible, with minimal false linkages.</p> <p>It is worth noting that we should not necessarily expect to capture all links. There are some links that although we know they do correspond to the same person, the data is so mismatched between them that we would not reasonably expect a model to link them, and indeed should a model do so may indicate that we have overengineered things using our knowledge of true links, which will not be a helpful reference in situations where we attempt to link unlabelled data, as will usually be the case.</p>"},{"location":"demos/example_febrl4.html#exploring-data-and-defining-model","title":"Exploring data and defining model","text":""},{"location":"demos/example_febrl4.html#estimating-model-parameters","title":"Estimating model parameters","text":""},{"location":"demos/example_febrl4.html#predictions","title":"Predictions","text":"<p>Now that we have had a look into the details of the models, we will focus on only our more detailed model, which should be able to capture more of the genuine links in our data</p>"},{"location":"demos/example_febrl4.html#further-refinements","title":"Further refinements","text":"<p>Looking at the non-links we have done well in having no false positives at any substantial match probability --- however looking at some of the true links we can see that there are a few that we are not capturing with sufficient match probability.</p> <p>We can see that there are a few features that we are not capturing/weighting appropriately * single-character transpostions, particularly in postcode (which is being lumped in with more 'severe typos'/probable non-matches) * given/sur-names being swapped with typos * given/sur-names being cross-matches on one only, with no match on the other cross</p> <p>We will quickly see if we can incorporate these features into a new model. As we are now going into more detail with the inter-relationship between given name and surname, it is probably no longer sensible to model them as independent comparisons, and so we will need to switch to a combined comparison on full name.</p>"},{"location":"demos/example_link_only.html","title":"Linking two tables of persons","text":"<pre><code>import pandas as pd \ndf_l = pd.read_parquet(\"./data/fake_df_l.parquet\")\ndf_r = pd.read_parquet(\"./data/fake_df_r.parquet\")\ndf_l.head(2)\n</code></pre> unique_id first_name surname dob city email group 0 0 Julia None 2015-10-29 London hannah88@powers.com 0 1 4 oNah Watson 2008-03-23 Bolton matthew78@ballard-mcdonald.net 1 <pre><code>from splink.duckdb.duckdb_linker import DuckDBLinker\nimport splink.duckdb.duckdb_comparison_library as cl\nimport splink.duckdb.duckdb_comparison_template_library as ctl\n\n\nsettings = {\n    \"link_type\": \"link_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\",),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        cl.levenshtein_at_thresholds(\"email\"),\n    ],       \n}\n</code></pre> <pre><code>linker = DuckDBLinker([df_l, df_r], settings, input_table_aliases=[\"df_left\", \"df_right\"])\ndeterministic_rules = [\n    \"l.first_name = r.first_name and levenshtein(r.dob, l.dob) &lt;= 1\",\n    \"l.surname = r.surname and levenshtein(r.dob, l.dob) &lt;= 1\",\n    \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) &lt;= 2\",\n    \"l.email = r.email\"\n]\n\nlinker.estimate_probability_two_random_records_match(deterministic_rules, recall=0.7)\n</code></pre> <pre>\n<code>Probability two random records match is estimated to be  0.00582.\nThis means that amongst all possible pairwise record comparisons, one in 171.80 are expected to match.  With 148,239 total possible comparisons, we expect a total of around 862.86 matching pairs\n</code>\n</pre> <pre><code>linker.estimate_u_using_random_sampling(max_pairs=1e6, seed=1)\n</code></pre> <pre>\n<code>----- Estimating u probabilities using random sampling -----\n\nEstimated u probabilities using random sampling\n\nYour model is not yet fully trained. Missing estimates for:\n    - first_name (no m values are trained).\n    - surname (no m values are trained).\n    - dob (no m values are trained).\n    - city (no m values are trained).\n    - email (no m values are trained).\n</code>\n</pre> <pre><code>session_dob = linker.estimate_parameters_using_expectation_maximisation(\"l.dob = r.dob\")\nsession_email = linker.estimate_parameters_using_expectation_maximisation(\"l.email = r.email\")\nsession_first_name = linker.estimate_parameters_using_expectation_maximisation(\"l.first_name = r.first_name\")\n</code></pre> <pre>\n<code>\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.dob = r.dob\n\nParameter estimates will be made for the following comparison(s):\n    - first_name\n    - surname\n    - city\n    - email\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - dob\n\nIteration 1: Largest change in params was -0.396 in the m_probability of first_name, level `Exact match first_name`\nIteration 2: Largest change in params was 0.204 in probability_two_random_records_match\nIteration 3: Largest change in params was 0.0611 in probability_two_random_records_match\nIteration 4: Largest change in params was 0.0226 in probability_two_random_records_match\nIteration 5: Largest change in params was 0.0109 in probability_two_random_records_match\nIteration 6: Largest change in params was 0.00619 in probability_two_random_records_match\nIteration 7: Largest change in params was 0.00385 in probability_two_random_records_match\nIteration 8: Largest change in params was 0.00255 in probability_two_random_records_match\nIteration 9: Largest change in params was 0.00176 in probability_two_random_records_match\nIteration 10: Largest change in params was 0.00126 in probability_two_random_records_match\nIteration 11: Largest change in params was 0.000915 in probability_two_random_records_match\nIteration 12: Largest change in params was 0.000678 in probability_two_random_records_match\nIteration 13: Largest change in params was 0.000508 in probability_two_random_records_match\nIteration 14: Largest change in params was 0.000384 in probability_two_random_records_match\nIteration 15: Largest change in params was 0.000292 in probability_two_random_records_match\nIteration 16: Largest change in params was 0.000224 in probability_two_random_records_match\nIteration 17: Largest change in params was 0.000172 in probability_two_random_records_match\nIteration 18: Largest change in params was 0.000133 in probability_two_random_records_match\nIteration 19: Largest change in params was 0.000102 in probability_two_random_records_match\nIteration 20: Largest change in params was 7.93e-05 in probability_two_random_records_match\n\nEM converged after 20 iterations\n\nYour model is not yet fully trained. Missing estimates for:\n    - dob (no m values are trained).\n\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.email = r.email\n\nParameter estimates will be made for the following comparison(s):\n    - first_name\n    - surname\n    - dob\n    - city\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - email\n\n\nWARNING:\nLevel Within 10 years on comparison dob not observed in dataset, unable to train m value\n\nWARNING:\nLevel All other comparisons on comparison dob not observed in dataset, unable to train m value\nIteration 1: Largest change in params was 0.395 in probability_two_random_records_match\n\nWARNING:\nLevel Within 10 years on comparison dob not observed in dataset, unable to train m value\n\nWARNING:\nLevel All other comparisons on comparison dob not observed in dataset, unable to train m value\nIteration 2: Largest change in params was 0.0866 in probability_two_random_records_match\n\nWARNING:\nLevel Within 10 years on comparison dob not observed in dataset, unable to train m value\n\nWARNING:\nLevel All other comparisons on comparison dob not observed in dataset, unable to train m value\nIteration 3: Largest change in params was 0.00957 in probability_two_random_records_match\n\nWARNING:\nLevel Within 10 years on comparison dob not observed in dataset, unable to train m value\n\nWARNING:\nLevel All other comparisons on comparison dob not observed in dataset, unable to train m value\nIteration 4: Largest change in params was 0.000796 in probability_two_random_records_match\n\nWARNING:\nLevel Within 10 years on comparison dob not observed in dataset, unable to train m value\n\nWARNING:\nLevel All other comparisons on comparison dob not observed in dataset, unable to train m value\nIteration 5: Largest change in params was 6.36e-05 in probability_two_random_records_match\n\nEM converged after 5 iterations\nm probability not trained for dob - Within 10 years (comparison vector value: 1). This usually means the comparison level was never observed in the training data.\nm probability not trained for dob - All other comparisons (comparison vector value: 0). This usually means the comparison level was never observed in the training data.\n\nYour model is not yet fully trained. Missing estimates for:\n    - dob (some m values are not trained).\n\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.first_name = r.first_name\n\nParameter estimates will be made for the following comparison(s):\n    - surname\n    - dob\n    - city\n    - email\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - first_name\n\nIteration 1: Largest change in params was -0.219 in the m_probability of surname, level `All other comparisons`\nIteration 2: Largest change in params was 0.00761 in the m_probability of email, level `All other comparisons`\nIteration 3: Largest change in params was 0.00277 in the m_probability of surname, level `All other comparisons`\nIteration 4: Largest change in params was 0.000917 in the m_probability of surname, level `All other comparisons`\nIteration 5: Largest change in params was 0.000303 in the m_probability of surname, level `All other comparisons`\nIteration 6: Largest change in params was 0.000105 in the m_probability of surname, level `All other comparisons`\nIteration 7: Largest change in params was -6.66e-05 in the m_probability of dob, level `All other comparisons`\n\nEM converged after 7 iterations\n\nYour model is fully trained. All comparisons have at least one estimate for their m and u values\n</code>\n</pre> <pre><code>results = linker.predict(threshold_match_probability=0.9)\n</code></pre> <pre><code>results.as_pandas_dataframe(limit=5)\n</code></pre> match_weight match_probability source_dataset_l source_dataset_r unique_id_l unique_id_r first_name_l first_name_r gamma_first_name surname_l ... dob_l dob_r gamma_dob city_l city_r gamma_city email_l email_r gamma_email match_key 0 4.539741 0.958779 df_left df_right 0 3 Julia Julia 3 NaN ... 2015-10-29 2015-10-29 5 London NaN -1 hannah88@powers.com hannah88opowersc@m 0 0 1 19.160694 0.999998 df_left df_right 44 48 Millie Millie 3 Harrs ... 1996-07-27 1996-07-27 5 London NaN -1 lynn04@jones.com lynn04@jones.com 3 0 2 10.599681 0.999356 df_left df_right 58 61 Elijah Elijah 3 NaN ... 1970-06-01 1970-06-01 5 Sunderland Sunderland 1 barbaraolson@smith.com NaN -1 0 3 13.351647 0.999904 df_left df_right 63 65 Maya Maya 3 Jones ... 2010-08-05 2010-10-20 2 Sunderland Sudderlnn 0 erin01@york.com erin01@york.com 3 0 4 15.852176 0.999983 df_left df_right 79 84 Carr Carr 3 Ethan ... 2013-01-21 2012-10-21 2 London London 1 stacyball@medina.biz stacyball@medina.biz 3 0 <p>5 rows \u00d7 22 columns</p>"},{"location":"demos/example_link_only.html#linking-without-deduplication","title":"Linking without deduplication","text":"<p>A simple record linkage model using the <code>link_only</code> link type.</p>"},{"location":"demos/example_pairwise_labels.html","title":"Estimating m probabilities from labels","text":"<pre><code>import pandas as pd \nimport altair as alt\nalt.renderers.enable(\"mimetype\")\n</code></pre> <pre>\n<code>RendererRegistry.enable('mimetype')</code>\n</pre> <pre><code>pairwise_labels = pd.read_csv(\"./data/pairwise_labels_to_estimate_m.csv\")\npairwise_labels\n</code></pre> unique_id_l source_dataset_l unique_id_r source_dataset_r 0 0 fake_1000 3 fake_1000 1 1 fake_1000 3 fake_1000 2 2 fake_1000 3 fake_1000 3 4 fake_1000 5 fake_1000 4 7 fake_1000 10 fake_1000 ... ... ... ... ... 2026 978 fake_1000 979 fake_1000 2027 985 fake_1000 986 fake_1000 2028 624 fake_1000 626 fake_1000 2029 625 fake_1000 626 fake_1000 2030 624 fake_1000 625 fake_1000 <p>2031 rows \u00d7 4 columns</p> <p>We now proceed to estimate the Fellegi Sunter model:</p> <pre><code>df = pd.read_csv(\"./data/fake_1000.csv\")\ndf.head(2)\n</code></pre> unique_id first_name surname dob city email cluster 0 0 Robert Alan 1971-06-24 NaN robert255@smith.net 0 1 1 Robert Allen 1971-05-24 NaN roberta25@smith.net 0 <pre><code>from splink.duckdb.duckdb_linker import DuckDBLinker\nimport splink.duckdb.duckdb_comparison_library as cl\nimport splink.duckdb.duckdb_comparison_template_library as ctl\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\"),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        cl.jaro_at_thresholds(\"email\", [0.9, 0.7]),\n    ],\n    \"retain_matching_columns\": True,\n    \"retain_intermediate_calculation_columns\": True,\n}\n</code></pre> <pre><code>linker = DuckDBLinker(df, settings, set_up_basic_logging=False)\ndeterministic_rules = [\n    \"l.first_name = r.first_name and levenshtein(r.dob, l.dob) &lt;= 1\",\n    \"l.surname = r.surname and levenshtein(r.dob, l.dob) &lt;= 1\",\n    \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) &lt;= 2\",\n    \"l.email = r.email\"\n]\n\nlinker.estimate_probability_two_random_records_match(deterministic_rules, recall=0.7)\n</code></pre> <pre><code>linker.estimate_u_using_random_sampling(max_pairs=1e6)\n</code></pre> <pre><code># Register the pairwise labels table with the database, and then use it to estimate the m values\nlabels_df = linker.register_labels_table(pairwise_labels, overwrite=True)\nlinker.estimate_m_from_pairwise_labels(labels_df)\n\n\n# Not if the labels table already existing in the dataset you could run\n# linker.estimate_m_from_pairwise_labels(\"labels_tablename_here\")\n</code></pre> <pre><code>training_blocking_rule = \"l.first_name = r.first_name\"\nlinker.estimate_parameters_using_expectation_maximisation(training_blocking_rule)\n</code></pre> <pre>\n<code>&lt;EMTrainingSession, blocking on l.first_name = r.first_name, deactivating comparisons first_name&gt;</code>\n</pre> <pre><code>linker.parameter_estimate_comparisons_chart()\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code>linker.match_weights_chart()\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre>"},{"location":"demos/example_pairwise_labels.html#estimating-m-from-a-sample-of-pairwise-labels","title":"Estimating m from a sample of pairwise labels","text":"<p>In this example, we estimate the m probabilities of the model from a table containing pairwise record comparisons which we know are 'true' matches.  For example, these may be the result of work by a clerical team who have manually labelled a sample of matches.</p> <p>The table must be in the following format:</p> source_dataset_l unique_id_l source_dataset_r unique_id_r df_1 1 df_2 2 df_1 1 df_2 3 <p>It is assumed that every record in the table represents a certain match.</p> <p>Note that the column names above are the defaults.  They should correspond to the values you've set for <code>unique_id_column_name</code> and  <code>source_dataset_column_name</code>, if you've chosen custom values.</p>"},{"location":"demos/example_quick_and_dirty_persons.html","title":"Quick and dirty persons model","text":"<pre><code>import pandas as pd \ndf = pd.read_parquet(\"./data/historical_figures_with_errors_50k.parquet\")\ndf.head(5)\n</code></pre> unique_id cluster full_name first_and_surname first_name surname dob birth_place postcode_fake gender occupation 0 Q2296770-1 Q2296770 thomas clifford, 1st baron clifford of chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon tq13 8df male politician 1 Q2296770-2 Q2296770 thomas of chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon tq13 8df male politician 2 Q2296770-3 Q2296770 tom 1st baron clifford of chudleigh tom chudleigh tom chudleigh 1630-08-01 devon tq13 8df male politician 3 Q2296770-4 Q2296770 thomas 1st chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon tq13 8hu None politician 4 Q2296770-5 Q2296770 thomas clifford, 1st baron chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon tq13 8df None politician <pre><code>from splink.duckdb.duckdb_linker import DuckDBLinker\nfrom splink.duckdb import duckdb_comparison_library as cl\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n    \"l.full_name = r.full_name\",\n    \"substr(l.full_name,1,6) = substr(r.full_name,1,6) and l.dob = r.dob and l.birth_place = r.birth_place\",\n    \"l.dob = r.dob and l.birth_place = r.birth_place\",\n    \"l.postcode_fake = r.postcode_fake\",\n    ],\n    \"comparisons\": [\n        cl.jaro_at_thresholds(\"full_name\", [0.9, 0.7], term_frequency_adjustments=True),\n        cl.levenshtein_at_thresholds(\"dob\", [1, 2]),\n        cl.levenshtein_at_thresholds(\"postcode_fake\", 2),\n        cl.jaro_winkler_at_thresholds(\"birth_place\", 0.9, term_frequency_adjustments=True),\n        cl.exact_match(\"occupation\",  term_frequency_adjustments=True),\n    ],       \n\n}\n</code></pre> <pre><code>linker = DuckDBLinker(df, settings, set_up_basic_logging=False)\ndeterministic_rules = [\n    \"l.full_name = r.full_name\",\n    \"l.postcode_fake = r.postcode_fake and l.dob = r.dob\",\n]\n\nlinker.estimate_probability_two_random_records_match(deterministic_rules, recall=0.6)\n</code></pre> <pre><code>linker.estimate_u_using_random_sampling(max_pairs=2e6)\n</code></pre> <pre><code>results = linker.predict(threshold_match_probability=0.9)\n</code></pre> <pre>\n<code>/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n</code>\n</pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='100%'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre>\n<code>\n -- WARNING --\nYou have called predict(), but there are some parameter estimates which have neither been estimated or specified in your settings dictionary.  To produce predictions the following untrained trained parameters will use default values.\nComparison: 'full_name':\n    m values not fully trained\nComparison: 'dob':\n    m values not fully trained\nComparison: 'postcode_fake':\n    m values not fully trained\nComparison: 'birth_place':\n    m values not fully trained\nComparison: 'occupation':\n    m values not fully trained\n</code>\n</pre> <pre><code>results.as_pandas_dataframe(limit=5)\n</code></pre> match_weight match_probability unique_id_l unique_id_r full_name_l full_name_r gamma_full_name dob_l dob_r gamma_dob postcode_fake_l postcode_fake_r gamma_postcode_fake birth_place_l birth_place_r gamma_birth_place occupation_l occupation_r gamma_occupation match_key 0 33.891401 1.000000 Q90404618-1 Q90404618-3 emlie clifford emlie clifford 3 1861-01-01 1861-01-01 3 wr11 7qp wr11 7qw 1 wychavon wychavon 2 playwright playwright 1 0 1 33.891401 1.000000 Q90404618-2 Q90404618-3 emlie clifford emlie clifford 3 1861-01-01 1861-01-01 3 wr11 7qp wr11 7qw 1 wychavon wychavon 2 playwright playwright 1 0 2 49.246966 1.000000 Q631006-1 Q631006-2 moses gaster moses gaster 3 1856-09-17 1856-09-17 3 ex20 3pz ex20 3pz 2 bucharest bucharest 2 rabbi rabbi 1 0 3 20.529423 0.999999 Q7795446-2 Q7795446-3 thomas barry thomas barry 3 1560-01-01 1560-01-01 3 cf14 5gh cf14 6tq 0 cardiff cardiff 2 judge judge 1 0 4 16.884481 0.999992 Q28924099-11 Q28924099-7 art ceely art ceely 3 1834-10-14 NaN -1 s75 1ju s75 1ju 2 barnsley NaN -1 cricketer cricketer 1 0"},{"location":"demos/example_quick_and_dirty_persons.html#historical-people-quick-and-dirty","title":"Historical people: Quick and dirty","text":"<p>This example shows how to get some initial record linkage results as quickly as possible.  </p> <p>There are many ways to improve the accuracy of this model.  But this may be a good place to start if you just want to give Splink a try and see what it's capable of.</p>"},{"location":"demos/example_real_time_record_linkage.html","title":"Real time record linkage","text":"<p>In this notebook, we demonstrate splink's incremental and real time linkage capabilities - specifically: - the <code>linker.compare_two_records</code> function, that allows you to interactively explore the results of a linkage model; and - the <code>linker.find_matches_to_new_records</code> that allows you to incrementally find matches to a small number of new records</p> <pre><code>import pandas as pd\nimport json\nfrom splink.duckdb.duckdb_linker import DuckDBLinker\nimport altair as alt\nalt.renderers.enable('mimetype')\n\nwith open(\"demo_settings/real_time_settings.json\") as f:\n    trained_settings = json.load(f)\n\ndf = pd.read_csv(\"./data/fake_1000.csv\")\n\nlinker = DuckDBLinker(df, trained_settings)\n</code></pre> <pre><code>linker.waterfall_chart(linker.predict().as_record_dict(limit=2))\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code>from splink.term_frequencies import compute_term_frequencies_from_concat_with_tf\nrecord_1  = {\n     'unique_id':1,\n     'first_name': \"Lucas\",\n     'surname': \"Smith\",\n     'dob': \"1984-01-02\",\n     'city': \"London\",\n     'email': \"lucas.smith@hotmail.com\"\n}\n\nrecord_2  = {\n     'unique_id':2,\n     'first_name': \"Lucas\",\n     'surname': \"Smith\",\n     'dob': \"1983-02-12\",\n     'city': \"Machester\",\n     'email': \"lucas.smith@hotmail.com\"\n}\n\nlinker._settings_obj_._retain_intermediate_calculation_columns = True\nlinker._settings_obj_._retain_matching_columns = True\n\nlinker.compute_tf_table(\"first_name\")\nlinker.compute_tf_table(\"surname\")\nlinker.compute_tf_table(\"dob\")\nlinker.compute_tf_table(\"city\")\nlinker.compute_tf_table(\"email\")\n\n\ndf_two = linker.compare_two_records(record_1, record_2)\ndf_two.as_pandas_dataframe()\n</code></pre> match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name tf_first_name_l tf_first_name_r bf_first_name ... tf_city_r bf_city bf_tf_adj_city email_l email_r gamma_email tf_email_l tf_email_r bf_email bf_tf_adj_email 0 13.161672 0.999891 1 2 Lucas Lucas 2 0.001203 0.001203 87.571229 ... NaN 0.446404 1.0 lucas.smith@hotmail.com lucas.smith@hotmail.com 1 NaN NaN 263.229168 1.0 <p>1 rows \u00d7 39 columns</p> <pre><code>import ipywidgets as widgets\nfields = [\"unique_id\", \"first_name\",\"surname\",\"dob\",\"email\",\"city\"]\n\nleft_text_boxes = []\nright_text_boxes = []\n\ninputs_to_interactive_output = {}\n\nfor f in fields:\n    wl = widgets.Text(description=f, value =str(record_1[f]))\n    left_text_boxes.append(wl)\n    inputs_to_interactive_output[f\"{f}_l\"] = wl\n    wr = widgets.Text( description=f, value =str(record_2[f]))\n    right_text_boxes.append(wr)\n    inputs_to_interactive_output[f\"{f}_r\"] = wr\n\n\nb1 = widgets.VBox(left_text_boxes)\nb2 = widgets.VBox(right_text_boxes)\nui = widgets.HBox([b1,b2])\n\ndef myfn(**kwargs):\n    my_args = dict(kwargs)\n\n    record_left = {}\n    record_right = {}\n\n    for key, value in my_args.items():\n        if value == '':\n            value = None\n        if key.endswith(\"_l\"):\n            record_left[key[:-2]] = value\n        if key.endswith(\"_r\"):\n            record_right[key[:-2]] = value\n\n\n    linker._settings_obj_._retain_intermediate_calculation_columns = True\n    linker._settings_obj_._retain_matching_columns = True\n\n    df_two = linker.compare_two_records(record_left, record_right)\n\n    recs = df_two.as_pandas_dataframe().to_dict(orient=\"records\")\n    from splink.charts import waterfall_chart\n    display(linker.waterfall_chart(recs, filter_nulls=False))\n\n\nout = widgets.interactive_output(myfn, inputs_to_interactive_output)\n\ndisplay(ui,out)\n</code></pre> <pre>\n<code>/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n</code>\n</pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre>\n<code>HBox(children=(VBox(children=(Text(value='1', description='unique_id'), Text(value='Lucas', description='first\u2026</code>\n</pre> <pre>\n<code>Output()</code>\n</pre> <pre><code>record = {'unique_id': 123987,\n 'first_name': \"Robert\",\n 'surname': \"Alan\",\n 'dob': \"1971-05-24\",\n 'city': \"London\",\n 'email': \"robert255@smith.net\"\n}\n\n\n\ndf_inc = linker.find_matches_to_new_records([record], blocking_rules=[]).as_pandas_dataframe()\ndf_inc.sort_values(\"match_weight\", ascending=False)\n</code></pre> match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name tf_first_name_l tf_first_name_r bf_first_name ... tf_city_r bf_city bf_tf_adj_city email_l email_r gamma_email tf_email_l tf_email_r bf_email bf_tf_adj_email 0 23.531793 1.000000 0 123987 Robert Robert 2 0.003610 0.00361 87.571229 ... 0.212792 1.000000 1.000000 robert255@smith.net robert255@smith.net 1 0.001267 0.001267 263.229168 1.730964 3 14.550320 0.999958 1 123987 Robert Robert 2 0.003610 0.00361 87.571229 ... 0.212792 1.000000 1.000000 roberta25@smith.net robert255@smith.net 0 0.002535 0.001267 0.423438 1.000000 5 10.388623 0.999255 3 123987 Robert Robert 2 0.003610 0.00361 87.571229 ... 0.212792 0.446404 1.000000 NaN robert255@smith.net -1 NaN 0.001267 1.000000 1.000000 4 2.427256 0.843228 2 123987 Rob Robert 0 0.001203 0.00361 0.218767 ... 0.212792 10.484859 0.259162 roberta25@smith.net robert255@smith.net 0 0.002535 0.001267 0.423438 1.000000 6 -2.123090 0.186697 8 123987 NaN Robert -1 NaN 0.00361 1.000000 ... 0.212792 1.000000 1.000000 NaN robert255@smith.net -1 NaN 0.001267 1.000000 1.000000 1 -2.205894 0.178139 754 123987 NaN Robert -1 NaN 0.00361 1.000000 ... 0.212792 1.000000 1.000000 j.c@whige.wort robert255@smith.net 0 0.001267 0.001267 0.423438 1.000000 2 -2.802309 0.125383 750 123987 NaN Robert -1 NaN 0.00361 1.000000 ... 0.212792 10.484859 0.259162 j.c@white.org robert255@smith.net 0 0.002535 0.001267 0.423438 1.000000 <p>7 rows \u00d7 39 columns</p> <pre><code>from splink.charts import waterfall_chart\n\n@widgets.interact(first_name='Robert', surname=\"Alan\", dob=\"1971-05-24\", city=\"London\", email=\"robert255@smith.net\")\ndef interactive_link(first_name, surname, dob, city, email):    \n\n    record = {'unique_id': 123987,\n     'first_name': first_name,\n     'surname': surname,\n     'dob': dob,\n     'city': city,\n     'email': email,\n     'group': 0}\n\n    for key in record.keys():\n        if type(record[key]) == str:\n            if record[key].strip() == \"\":\n                record[key] = None\n\n\n    df_inc = linker.find_matches_to_new_records([record], blocking_rules=[f\"(true)\"]).as_pandas_dataframe()\n    df_inc = df_inc.sort_values(\"match_weight\", ascending=False)\n    recs = df_inc.to_dict(orient=\"records\")\n\n\n\n    display(linker.waterfall_chart(recs, filter_nulls=False))\n</code></pre> <pre>\n<code>/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n</code>\n</pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre>\n<code>interactive(children=(Text(value='Robert', description='first_name'), Text(value='Alan', description='surname'\u2026</code>\n</pre> <pre><code>linker.match_weights_chart()\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre>"},{"location":"demos/example_real_time_record_linkage.html#real-time-linkage","title":"Real time linkage","text":""},{"location":"demos/example_real_time_record_linkage.html#step-1-load-a-pre-trained-linkage-model","title":"Step 1: Load a pre-trained linkage model","text":""},{"location":"demos/example_real_time_record_linkage.html#step-comparing-two-records","title":"Step  Comparing two records","text":"<p>It's now possible to compute a match weight for any two records using <code>linker.compare_two_records()</code></p>"},{"location":"demos/example_real_time_record_linkage.html#step-3-interactive-comparisons","title":"Step 3: Interactive comparisons","text":"<p>One interesting applicatin of <code>compare_two_records</code> is to create a simple interface that allows the user to input two records interactively, and get real time feedback.</p> <p>In the following cell we use <code>ipywidets</code> for this purpose.  \u2728\u2728 Change the values in the text boxes to see the waterfall chart update in real time. \u2728\u2728</p>"},{"location":"demos/example_real_time_record_linkage.html#finding-matching-records-interactively","title":"Finding matching records interactively","text":"<p>It is also possible to search the records in the input dataset rapidly using the <code>linker.find_matches_to_new_records()</code> function</p>"},{"location":"demos/example_real_time_record_linkage.html#interactive-interface-for-finding-records","title":"Interactive interface for finding records","text":"<p>Again, we can use <code>ipywidgets</code> to build an interactive interface for the <code>linker.find_matches_to_new_records</code> function</p>"},{"location":"demos/example_simple_pyspark.html","title":"Deduplication using Pyspark","text":"<pre><code>from splink.spark.jar_location import similarity_jar_location\n\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import types\n\nconf = SparkConf()\n# This parallelism setting is only suitable for a small toy example\nconf.set(\"spark.driver.memory\", \"12g\")\nconf.set(\"spark.default.parallelism\", \"16\")\n\n\n# Add custom similarity functions, which are bundled with Splink\n# documented here: https://github.com/moj-analytical-services/splink_scalaudfs\npath = similarity_jar_location()\nconf.set(\"spark.jars\", path)\n\nsc = SparkContext.getOrCreate(conf=conf)\n\nspark = SparkSession(sc)\nspark.sparkContext.setCheckpointDir(\"./tmp_checkpoints\")\n</code></pre> <pre>\n<code>23/05/04 09:25:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n</code>\n</pre> <pre><code>import pandas as pd \ndf = spark.read.csv(\"./data/fake_1000.csv\", header=True)\n</code></pre> <pre><code>import splink.spark.spark_comparison_library as cl\nimport splink.spark.spark_comparison_template_library as ctl\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\"),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        cl.levenshtein_at_thresholds(\"email\"),\n    ],\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"retain_matching_columns\": True,\n    \"retain_intermediate_calculation_columns\": True,\n    \"em_convergence\": 0.01\n}\n</code></pre> <pre><code>from splink.spark.spark_linker import SparkLinker\nlinker = SparkLinker(df, settings)\ndeterministic_rules = [\n    \"l.first_name = r.first_name and levenshtein(r.dob, l.dob) &lt;= 1\",\n    \"l.surname = r.surname and levenshtein(r.dob, l.dob) &lt;= 1\",\n    \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) &lt;= 2\",\n    \"l.email = r.email\"\n]\n\nlinker.estimate_probability_two_random_records_match(deterministic_rules, recall=0.6)\n</code></pre> <pre>\n<code>--WARN-- \n You are using datediff comparison\n                        with str-casting and ANSI is not enabled. Bad dates\n                        e.g. 1999-13-54 will not trigger an exception but will\n                        classed as comparison level = \"ELSE\". Ensure date strings\n                        are cleaned to remove bad dates \n\nProbability two random records match is estimated to be  0.00389.\nThis means that amongst all possible pairwise record comparisons, one in 257.25 are expected to match.  With 499,500 total possible comparisons, we expect a total of around 1,941.67 matching pairs\n</code>\n</pre> <pre><code>linker.estimate_u_using_random_sampling(max_pairs=5e5)\n</code></pre> <pre>\n<code>----- Estimating u probabilities using random sampling -----\n23/05/04 09:25:17 WARN DataSource: All paths were ignored:                      \n  file:/Users/rosskennedy/splink_demos/tmp_checkpoints/4a3d6387-4dc2-4a2b-8726-37954ed09b3a/__splink__df_concat_with_tf_d4ce2bbdc\n\nEstimated u probabilities using random sampling\n\nYour model is not yet fully trained. Missing estimates for:\n    - first_name (no m values are trained).\n    - surname (no m values are trained).\n    - dob (no m values are trained).\n    - city (no m values are trained).\n    - email (no m values are trained).\n</code>\n</pre> <pre><code>training_blocking_rule = \"l.first_name = r.first_name and l.surname = r.surname\"\ntraining_session_fname_sname = linker.estimate_parameters_using_expectation_maximisation(training_blocking_rule)\n\ntraining_blocking_rule = \"l.dob = r.dob\"\ntraining_session_dob = linker.estimate_parameters_using_expectation_maximisation(training_blocking_rule)\n</code></pre> <pre>\n<code>\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.first_name = r.first_name and l.surname = r.surname\n\nParameter estimates will be made for the following comparison(s):\n    - dob\n    - city\n    - email\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - first_name\n    - surname\n23/05/04 09:25:26 WARN DataSource: All paths were ignored:                      \n  file:/Users/rosskennedy/splink_demos/tmp_checkpoints/4a3d6387-4dc2-4a2b-8726-37954ed09b3a/__splink__df_comparison_vectors_90769078f\n\nIteration 1: Largest change in params was -0.539 in the m_probability of dob, level `Exact match`\nIteration 2: Largest change in params was 0.0385 in probability_two_random_records_match\nIteration 3: Largest change in params was 0.00854 in probability_two_random_records_match\n\nEM converged after 3 iterations\n\nYour model is not yet fully trained. Missing estimates for:\n    - first_name (no m values are trained).\n    - surname (no m values are trained).\n\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.dob = r.dob\n\nParameter estimates will be made for the following comparison(s):\n    - first_name\n    - surname\n    - city\n    - email\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - dob\n23/05/04 09:25:32 WARN DataSource: All paths were ignored:                      \n  file:/Users/rosskennedy/splink_demos/tmp_checkpoints/4a3d6387-4dc2-4a2b-8726-37954ed09b3a/__splink__df_comparison_vectors_035f39cb5\n\nIteration 1: Largest change in params was -0.395 in the m_probability of surname, level `Exact match surname`\nIteration 2: Largest change in params was 0.121 in probability_two_random_records_match\nIteration 3: Largest change in params was 0.0383 in probability_two_random_records_match\nIteration 4: Largest change in params was 0.0151 in probability_two_random_records_match\nIteration 5: Largest change in params was 0.00715 in probability_two_random_records_match\n\nEM converged after 5 iterations\n\nYour model is fully trained. All comparisons have at least one estimate for their m and u values\n</code>\n</pre> <pre><code>results = linker.predict(threshold_match_probability=0.9)\n</code></pre> <pre>\n<code>23/05/04 09:25:42 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n23/05/04 09:25:43 ERROR CodeGenerator: failed to compile: org.codehaus.janino.InternalCompilerException: Compiling \"GeneratedClass\" in \"generated.java\": Code of method \"bhj_doConsume_0$(Lorg/apache/spark/sql/catalyst/expressions/GeneratedClass$GeneratedIteratorForCodegenStage3;Lorg/apache/spark/unsafe/types/UTF8String;Lorg/apache/spark/unsafe/types/UTF8String;Lorg/apache/spark/unsafe/types/UTF8String;ZLorg/apache/spark/unsafe/types/UTF8String;ZLorg/apache/spark/unsafe/types/UTF8String;ZLorg/apache/spark/unsafe/types/UTF8String;ZDZ)V\" of class \"org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3\" grows beyond 64 KB\norg.codehaus.janino.InternalCompilerException: Compiling \"GeneratedClass\" in \"generated.java\": Code of method \"bhj_doConsume_0$(Lorg/apache/spark/sql/catalyst/expressions/GeneratedClass$GeneratedIteratorForCodegenStage3;Lorg/apache/spark/unsafe/types/UTF8String;Lorg/apache/spark/unsafe/types/UTF8String;Lorg/apache/spark/unsafe/types/UTF8String;ZLorg/apache/spark/unsafe/types/UTF8String;ZLorg/apache/spark/unsafe/types/UTF8String;ZLorg/apache/spark/unsafe/types/UTF8String;ZDZ)V\" of class \"org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3\" grows beyond 64 KB\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:366)\n    at org.codehaus.janino.UnitCompiler.access$000(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$1.visitCompilationUnit(UnitCompiler.java:336)\n    at org.codehaus.janino.UnitCompiler$1.visitCompilationUnit(UnitCompiler.java:333)\n    at org.codehaus.janino.Java$CompilationUnit.accept(Java.java:363)\n    at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:333)\n    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:235)\n    at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:464)\n    at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:314)\n    at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:237)\n    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:205)\n    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)\n    at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1489)\n    at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1586)\n    at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1583)\n    at org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n    at org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n    at org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n    at org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)\n    at org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)\n    at org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)\n    at org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)\n    at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1436)\n    at org.apache.spark.sql.execution.WholeStageCodegenExec.liftedTree1$1(WholeStageCodegenExec.scala:725)\n    at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:724)\n    at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:184)\n    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)\n    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)\n    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:180)\n    at org.apache.spark.sql.execution.UnionExec.$anonfun$doExecute$5(basicPhysicalOperators.scala:698)\n    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n    at scala.collection.Iterator.foreach(Iterator.scala:943)\n    at scala.collection.Iterator.foreach$(Iterator.scala:943)\n    at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n    at scala.collection.IterableLike.foreach(IterableLike.scala:74)\n    at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n    at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n    at scala.collection.TraversableLike.map(TraversableLike.scala:286)\n    at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n    at scala.collection.AbstractTraversable.map(Traversable.scala:108)\n    at org.apache.spark.sql.execution.UnionExec.doExecute(basicPhysicalOperators.scala:698)\n    at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:184)\n    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)\n    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)\n    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:180)\n    at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:135)\n    at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:135)\n    at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:140)\n    at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:139)\n    at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:68)\n    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)\n    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)\n    at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:68)\n    at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:67)\n    at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:115)\n    at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:170)\n    at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:170)\n    at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:172)\n    at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:82)\n    at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:256)\n    at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:254)\n    at scala.collection.immutable.List.foreach(List.scala:431)\n    at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:254)\n    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n    at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:226)\n    at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:365)\n    at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:350)\n    at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:184)\n    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)\n    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)\n    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:180)\n    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:186)\n    at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n    at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n    at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n    at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n    at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n    at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n    at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n    at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n    at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n    at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n    at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n    at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n    at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n    at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n    at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n    at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:781)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n    at py4j.Gateway.invoke(Gateway.java:282)\n    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n    at py4j.commands.CallCommand.execute(CallCommand.java:79)\n    at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n    at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n    at java.lang.Thread.run(Thread.java:750)\nCaused by: org.codehaus.janino.InternalCompilerException: Code of method \"bhj_doConsume_0$(Lorg/apache/spark/sql/catalyst/expressions/GeneratedClass$GeneratedIteratorForCodegenStage3;Lorg/apache/spark/unsafe/types/UTF8String;Lorg/apache/spark/unsafe/types/UTF8String;Lorg/apache/spark/unsafe/types/UTF8String;ZLorg/apache/spark/unsafe/types/UTF8String;ZLorg/apache/spark/unsafe/types/UTF8String;ZLorg/apache/spark/unsafe/types/UTF8String;ZDZ)V\" of class \"org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3\" grows beyond 64 KB\n    at org.codehaus.janino.CodeContext.makeSpace(CodeContext.java:1051)\n    at org.codehaus.janino.CodeContext.write(CodeContext.java:932)\n    at org.codehaus.janino.UnitCompiler.writeOpcode(UnitCompiler.java:12101)\n    at org.codehaus.janino.UnitCompiler.pushConstant(UnitCompiler.java:10688)\n    at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5661)\n    at org.codehaus.janino.UnitCompiler.access$9300(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$16.visitIntegerLiteral(UnitCompiler.java:4484)\n    at org.codehaus.janino.UnitCompiler$16.visitIntegerLiteral(UnitCompiler.java:4455)\n    at org.codehaus.janino.Java$IntegerLiteral.accept(Java.java:5763)\n    at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4455)\n    at org.codehaus.janino.UnitCompiler.fakeCompile(UnitCompiler.java:3766)\n    at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5677)\n    at org.codehaus.janino.UnitCompiler.compileArithmeticOperation(UnitCompiler.java:7804)\n    at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5001)\n    at org.codehaus.janino.UnitCompiler.access$8500(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$16.visitBinaryOperation(UnitCompiler.java:4476)\n    at org.codehaus.janino.UnitCompiler$16.visitBinaryOperation(UnitCompiler.java:4455)\n    at org.codehaus.janino.Java$BinaryOperation.accept(Java.java:5077)\n    at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4455)\n    at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5683)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3839)\n    at org.codehaus.janino.UnitCompiler.access$6100(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3799)\n    at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3779)\n    at org.codehaus.janino.Java$Assignment.accept(Java.java:4690)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3779)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2366)\n    at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1497)\n    at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:3064)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1573)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1559)\n    at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1496)\n    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$Block.accept(Java.java:2969)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.access$3700(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$7.compile(UnitCompiler.java:2751)\n    at org.codehaus.janino.UnitCompiler.compileTryCatch(UnitCompiler.java:3092)\n    at org.codehaus.janino.UnitCompiler.compileTryCatchFinally(UnitCompiler.java:2969)\n    at org.codehaus.janino.UnitCompiler.compileTryCatchFinallyWithResources(UnitCompiler.java:2773)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2745)\n    at org.codehaus.janino.UnitCompiler.access$2300(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitTryStatement(UnitCompiler.java:1502)\n    at org.codehaus.janino.UnitCompiler$6.visitTryStatement(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$TryStatement.accept(Java.java:3433)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1573)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1559)\n    at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1496)\n    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$Block.accept(Java.java:2969)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2486)\n    at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1498)\n    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$IfStatement.accept(Java.java:3140)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1573)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1559)\n    at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1496)\n    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$Block.accept(Java.java:2969)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2443)\n    at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1498)\n    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$IfStatement.accept(Java.java:3140)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1573)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1559)\n    at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1496)\n    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$Block.accept(Java.java:2969)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1598)\n    at org.codehaus.janino.UnitCompiler.access$2600(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitDoStatement(UnitCompiler.java:1505)\n    at org.codehaus.janino.UnitCompiler$6.visitDoStatement(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$DoStatement.accept(Java.java:3664)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1573)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1559)\n    at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1496)\n    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$Block.accept(Java.java:2969)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1598)\n    at org.codehaus.janino.UnitCompiler.access$2600(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitDoStatement(UnitCompiler.java:1505)\n    at org.codehaus.janino.UnitCompiler$6.visitDoStatement(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$DoStatement.accept(Java.java:3664)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1573)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1559)\n    at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1496)\n    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$Block.accept(Java.java:2969)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2486)\n    at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1498)\n    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$IfStatement.accept(Java.java:3140)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1573)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1559)\n    at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1496)\n    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$Block.accept(Java.java:2969)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1848)\n    at org.codehaus.janino.UnitCompiler.access$2200(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1501)\n    at org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$WhileStatement.accept(Java.java:3245)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1573)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1559)\n    at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1496)\n    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$Block.accept(Java.java:2969)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2486)\n    at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1498)\n    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$IfStatement.accept(Java.java:3140)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1573)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3420)\n    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1362)\n    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1335)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:807)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:975)\n    at org.codehaus.janino.UnitCompiler.access$700(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:392)\n    at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:384)\n    at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1445)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:384)\n    at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:1312)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:833)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:410)\n    at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:389)\n    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:384)\n    at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1594)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:384)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:362)\n    ... 117 more\n23/05/04 09:25:43 WARN WholeStageCodegenExec: Whole-stage codegen disabled for plan (id=3):\n *(3) Project [CASE WHEN CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN true WHEN (city#338 = city#1649) THEN false ELSE false END THEN false WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN true ELSE false END THEN CASE WHEN isnotnull(coalesce(tf_city#341, tf_city#1652)) THEN (POWER((0.0551475711801453 / CASE WHEN (coalesce(tf_city#341, tf_city#1652) &gt;= coalesce(tf_city#1652, tf_city#341)) THEN coalesce(tf_city#341, tf_city#1652) ELSE coalesce(tf_city#1652, tf_city#341) END), 1.0) = Infinity) ELSE false END WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN false ELSE true END THEN false ELSE false END THEN Infinity ELSE LOG2(cast(cast((((((CASE WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1646)) THEN true WHEN (first_name#335 = first_name#1646) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.88) THEN false ELSE false END THEN 0.003902390004522083 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1646)) THEN false WHEN (first_name#335 = first_name#1646) THEN true WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.88) THEN false ELSE false END THEN 0.3365268364360054 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1646)) THEN false WHEN (first_name#335 = first_name#1646) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.95) THEN true WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.88) THEN false ELSE false END THEN 0.27677318122666966 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1646)) THEN false WHEN (first_name#335 = first_name#1646) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.88) THEN true ELSE false END THEN 0.1990847971311897 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1646)) THEN false WHEN (first_name#335 = first_name#1646) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.88) THEN false ELSE true END THEN 0.0010900609336073567 END * CASE WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1647)) THEN true WHEN (surname#336 = surname#1647) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.88) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1647)) THEN false WHEN (surname#336 = surname#1647) THEN true WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.88) THEN false ELSE false END THEN 91.08622270105276 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1647)) THEN false WHEN (surname#336 = surname#1647) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.95) THEN true WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.88) THEN false ELSE false END THEN 84.76958263140128 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1647)) THEN false WHEN (surname#336 = surname#1647) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.88) THEN true ELSE false END THEN 57.648261658181795 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1647)) THEN false WHEN (surname#336 = surname#1647) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.88) THEN false ELSE true END THEN 0.2827410382590715 END) * CASE WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN true WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN true WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 224.04335331944097 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN true WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 93.3426724371058 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN true WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 13.284421514226679 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN true WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 2.014287367664165 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN true ELSE false END THEN 0.41016658710113535 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE true END THEN 8.919181060305974E-7 END) * CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN true WHEN (city#338 = city#1649) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN true ELSE false END THEN 10.269631237504209 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN false ELSE true END THEN 0.45896561950688874 END) * CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN true WHEN (city#338 = city#1649) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN true ELSE false END THEN CASE WHEN isnotnull(coalesce(tf_city#341, tf_city#1652)) THEN POWER((0.0551475711801453 / CASE WHEN (coalesce(tf_city#341, tf_city#1652) &gt;= coalesce(tf_city#1652, tf_city#341)) THEN coalesce(tf_city#341, tf_city#1652) ELSE coalesce(tf_city#1652, tf_city#341) END), 1.0) ELSE 1.0 END WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN false ELSE true END THEN 1.0 END) * CASE WHEN CASE WHEN (isnull(email#339) OR isnull(email#1650)) THEN true WHEN (email#339 = email#1650) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 2) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1650)) THEN false WHEN (email#339 = email#1650) THEN true WHEN (levenshtein(email#339, email#1650) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 2) THEN false ELSE false END THEN 256.78780963847476 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1650)) THEN false WHEN (email#339 = email#1650) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 1) THEN true WHEN (levenshtein(email#339, email#1650) &lt;= 2) THEN false ELSE false END THEN 236.76780078519445 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1650)) THEN false WHEN (email#339 = email#1650) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 2) THEN true ELSE false END THEN 207.73789452299874 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1650)) THEN false WHEN (email#339 = email#1650) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 2) THEN false ELSE true END THEN 0.13449069645050282 END) as string) as double)) END AS match_weight#1530, CASE WHEN CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN true WHEN (city#338 = city#1649) THEN false ELSE false END THEN false WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN true ELSE false END THEN CASE WHEN isnotnull(coalesce(tf_city#341, tf_city#1652)) THEN (POWER((0.0551475711801453 / CASE WHEN (coalesce(tf_city#341, tf_city#1652) &gt;= coalesce(tf_city#1652, tf_city#341)) THEN coalesce(tf_city#341, tf_city#1652) ELSE coalesce(tf_city#1652, tf_city#341) END), 1.0) = Infinity) ELSE false END WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN false ELSE true END THEN false ELSE false END THEN 1.0 ELSE (CASE WHEN CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN true WHEN (city#338 = city#1649) THEN false ELSE false END THEN false WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN true ELSE false END THEN CASE WHEN isnotnull(coalesce(tf_city#341, tf_city#1652)) THEN (POWER((0.0551475711801453 / CASE WHEN (coalesce(tf_city#341, tf_city#1652) &gt;= coalesce(tf_city#1652, tf_city#341)) THEN coalesce(tf_city#341, tf_city#1652) ELSE coalesce(tf_city#1652, tf_city#341) END), 1.0) = Infinity) ELSE false END WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN false ELSE true END THEN false ELSE false END THEN Infinity ELSE cast(cast((((((CASE WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1646)) THEN true WHEN (first_name#335 = first_name#1646) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.88) THEN false ELSE false END THEN 0.003902390004522083 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1646)) THEN false WHEN (first_name#335 = first_name#1646) THEN true WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.88) THEN false ELSE false END THEN 0.3365268364360054 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1646)) THEN false WHEN (first_name#335 = first_name#1646) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.95) THEN true WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.88) THEN false ELSE false END THEN 0.27677318122666966 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1646)) THEN false WHEN (first_name#335 = first_name#1646) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.88) THEN true ELSE false END THEN 0.1990847971311897 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1646)) THEN false WHEN (first_name#335 = first_name#1646) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.88) THEN false ELSE true END THEN 0.0010900609336073567 END * CASE WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1647)) THEN true WHEN (surname#336 = surname#1647) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.88) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1647)) THEN false WHEN (surname#336 = surname#1647) THEN true WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.88) THEN false ELSE false END THEN 91.08622270105276 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1647)) THEN false WHEN (surname#336 = surname#1647) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.95) THEN true WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.88) THEN false ELSE false END THEN 84.76958263140128 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1647)) THEN false WHEN (surname#336 = surname#1647) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.88) THEN true ELSE false END THEN 57.648261658181795 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1647)) THEN false WHEN (surname#336 = surname#1647) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.88) THEN false ELSE true END THEN 0.2827410382590715 END) * CASE WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN true WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN true WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 224.04335331944097 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN true WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 93.3426724371058 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN true WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 13.284421514226679 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN true WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 2.014287367664165 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN true ELSE false END THEN 0.41016658710113535 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE true END THEN 8.919181060305974E-7 END) * CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN true WHEN (city#338 = city#1649) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN true ELSE false END THEN 10.269631237504209 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN false ELSE true END THEN 0.45896561950688874 END) * CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN true WHEN (city#338 = city#1649) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN true ELSE false END THEN CASE WHEN isnotnull(coalesce(tf_city#341, tf_city#1652)) THEN POWER((0.0551475711801453 / CASE WHEN (coalesce(tf_city#341, tf_city#1652) &gt;= coalesce(tf_city#1652, tf_city#341)) THEN coalesce(tf_city#341, tf_city#1652) ELSE coalesce(tf_city#1652, tf_city#341) END), 1.0) ELSE 1.0 END WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN false ELSE true END THEN 1.0 END) * CASE WHEN CASE WHEN (isnull(email#339) OR isnull(email#1650)) THEN true WHEN (email#339 = email#1650) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 2) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1650)) THEN false WHEN (email#339 = email#1650) THEN true WHEN (levenshtein(email#339, email#1650) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 2) THEN false ELSE false END THEN 256.78780963847476 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1650)) THEN false WHEN (email#339 = email#1650) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 1) THEN true WHEN (levenshtein(email#339, email#1650) &lt;= 2) THEN false ELSE false END THEN 236.76780078519445 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1650)) THEN false WHEN (email#339 = email#1650) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 2) THEN true ELSE false END THEN 207.73789452299874 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1650)) THEN false WHEN (email#339 = email#1650) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 2) THEN false ELSE true END THEN 0.13449069645050282 END) as string) as double) END / CASE WHEN CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN true WHEN (city#338 = city#1649) THEN false ELSE false END THEN false WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN true ELSE false END THEN CASE WHEN isnotnull(coalesce(tf_city#341, tf_city#1652)) THEN (POWER((0.0551475711801453 / CASE WHEN (coalesce(tf_city#341, tf_city#1652) &gt;= coalesce(tf_city#1652, tf_city#341)) THEN coalesce(tf_city#341, tf_city#1652) ELSE coalesce(tf_city#1652, tf_city#341) END), 1.0) = Infinity) ELSE false END WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN false ELSE true END THEN false ELSE false END THEN Infinity ELSE (1.0 + cast(cast((((((CASE WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1646)) THEN true WHEN (first_name#335 = first_name#1646) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.88) THEN false ELSE false END THEN 0.003902390004522083 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1646)) THEN false WHEN (first_name#335 = first_name#1646) THEN true WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.88) THEN false ELSE false END THEN 0.3365268364360054 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1646)) THEN false WHEN (first_name#335 = first_name#1646) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.95) THEN true WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.88) THEN false ELSE false END THEN 0.27677318122666966 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1646)) THEN false WHEN (first_name#335 = first_name#1646) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.88) THEN true ELSE false END THEN 0.1990847971311897 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1646)) THEN false WHEN (first_name#335 = first_name#1646) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.88) THEN false ELSE true END THEN 0.0010900609336073567 END * CASE WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1647)) THEN true WHEN (surname#336 = surname#1647) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.88) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1647)) THEN false WHEN (surname#336 = surname#1647) THEN true WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.88) THEN false ELSE false END THEN 91.08622270105276 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1647)) THEN false WHEN (surname#336 = surname#1647) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.95) THEN true WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.88) THEN false ELSE false END THEN 84.76958263140128 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1647)) THEN false WHEN (surname#336 = surname#1647) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.88) THEN true ELSE false END THEN 57.648261658181795 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1647)) THEN false WHEN (surname#336 = surname#1647) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.88) THEN false ELSE true END THEN 0.2827410382590715 END) * CASE WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN true WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN true WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 224.04335331944097 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN true WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 93.3426724371058 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN true WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 13.284421514226679 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN true WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 2.014287367664165 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN true ELSE false END THEN 0.41016658710113535 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE true END THEN 8.919181060305974E-7 END) * CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN true WHEN (city#338 = city#1649) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN true ELSE false END THEN 10.269631237504209 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN false ELSE true END THEN 0.45896561950688874 END) * CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN true WHEN (city#338 = city#1649) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN true ELSE false END THEN CASE WHEN isnotnull(coalesce(tf_city#341, tf_city#1652)) THEN POWER((0.0551475711801453 / CASE WHEN (coalesce(tf_city#341, tf_city#1652) &gt;= coalesce(tf_city#1652, tf_city#341)) THEN coalesce(tf_city#341, tf_city#1652) ELSE coalesce(tf_city#1652, tf_city#341) END), 1.0) ELSE 1.0 END WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN false ELSE true END THEN 1.0 END) * CASE WHEN CASE WHEN (isnull(email#339) OR isnull(email#1650)) THEN true WHEN (email#339 = email#1650) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 2) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1650)) THEN false WHEN (email#339 = email#1650) THEN true WHEN (levenshtein(email#339, email#1650) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 2) THEN false ELSE false END THEN 256.78780963847476 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1650)) THEN false WHEN (email#339 = email#1650) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 1) THEN true WHEN (levenshtein(email#339, email#1650) &lt;= 2) THEN false ELSE false END THEN 236.76780078519445 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1650)) THEN false WHEN (email#339 = email#1650) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 2) THEN true ELSE false END THEN 207.73789452299874 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1650)) THEN false WHEN (email#339 = email#1650) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 2) THEN false ELSE true END THEN 0.13449069645050282 END) as string) as double)) END) END AS match_probability#1531, unique_id#334 AS unique_id_l#1532, unique_id#1645 AS unique_id_r#1533, first_name#335 AS first_name_l#1534, first_name#1646 AS first_name_r#1535, CASE WHEN (isnull(first_name#335) OR isnull(first_name#1646)) THEN -1 WHEN (first_name#335 = first_name#1646) THEN 3 WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.95) THEN 2 WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.88) THEN 1 ELSE 0 END AS gamma_first_name#1562, CASE WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1646)) THEN true WHEN (first_name#335 = first_name#1646) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.88) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1646)) THEN false WHEN (first_name#335 = first_name#1646) THEN true WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.88) THEN false ELSE false END THEN 86.2360850776162 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1646)) THEN false WHEN (first_name#335 = first_name#1646) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.95) THEN true WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.88) THEN false ELSE false END THEN 70.92401859013204 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1646)) THEN false WHEN (first_name#335 = first_name#1646) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.88) THEN true ELSE false END THEN 51.01612009575941 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1646)) THEN false WHEN (first_name#335 = first_name#1646) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.88) THEN false ELSE true END THEN 0.2793316230167143 END AS bf_first_name#1567, surname#336 AS surname_l#1536, surname#1647 AS surname_r#1537, CASE WHEN (isnull(surname#336) OR isnull(surname#1647)) THEN -1 WHEN (surname#336 = surname#1647) THEN 3 WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.95) THEN 2 WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.88) THEN 1 ELSE 0 END AS gamma_surname#1563, CASE WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1647)) THEN true WHEN (surname#336 = surname#1647) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.88) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1647)) THEN false WHEN (surname#336 = surname#1647) THEN true WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.88) THEN false ELSE false END THEN 91.08622270105276 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1647)) THEN false WHEN (surname#336 = surname#1647) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.95) THEN true WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.88) THEN false ELSE false END THEN 84.76958263140128 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1647)) THEN false WHEN (surname#336 = surname#1647) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.88) THEN true ELSE false END THEN 57.648261658181795 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1647)) THEN false WHEN (surname#336 = surname#1647) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.88) THEN false ELSE true END THEN 0.2827410382590715 END AS bf_surname#1568, dob#337 AS dob_l#1538, dob#1648 AS dob_r#1539, CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN -1 WHEN (dob#337 = dob#1648) THEN 5 WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN 4 WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN 3 WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN 2 WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN 1 ELSE 0 END AS gamma_dob#1564, CASE WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN true WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN true WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 224.04335331944097 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN true WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 93.3426724371058 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN true WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 13.284421514226679 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN true WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 2.014287367664165 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN true ELSE false END THEN 0.41016658710113535 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE true END THEN 8.919181060305974E-7 END AS bf_dob#1569, city#338 AS city_l#1540, city#1649 AS city_r#1541, CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN -1 WHEN (city#338 = city#1649) THEN 1 ELSE 0 END AS gamma_city#1565, tf_city#341 AS tf_city_l#1542, tf_city#1652 AS tf_city_r#1543, CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN true WHEN (city#338 = city#1649) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN true ELSE false END THEN 10.269631237504209 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN false ELSE true END THEN 0.45896561950688874 END AS bf_city#1570, CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN true WHEN (city#338 = city#1649) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN true ELSE false END THEN CASE WHEN isnotnull(coalesce(tf_city#341, tf_city#1652)) THEN POWER((0.0551475711801453 / CASE WHEN (coalesce(tf_city#341, tf_city#1652) &gt;= coalesce(tf_city#1652, tf_city#341)) THEN coalesce(tf_city#341, tf_city#1652) ELSE coalesce(tf_city#1652, tf_city#341) END), 1.0) ELSE 1.0 END WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN false ELSE true END THEN 1.0 END AS bf_tf_adj_city#1571, email#339 AS email_l#1544, ... 4 more fields]\n+- *(3) BroadcastHashJoin [first_name#335], [first_name#1646], Inner, BuildRight, ((unique_id#334 &lt; unique_id#1645) AND (CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN true WHEN (city#338 = city#1649) THEN false ELSE false END THEN false WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN true ELSE false END THEN CASE WHEN isnotnull(coalesce(tf_city#341, tf_city#1652)) THEN (POWER((0.0551475711801453 / CASE WHEN (coalesce(tf_city#341, tf_city#1652) &gt;= coalesce(tf_city#1652, tf_city#341)) THEN coalesce(tf_city#341, tf_city#1652) ELSE coalesce(tf_city#1652, tf_city#341) END), 1.0) = Infinity) ELSE false END WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN false ELSE true END THEN false ELSE false END OR (LOG2(cast(cast((((((CASE WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1646)) THEN true WHEN (first_name#335 = first_name#1646) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.88) THEN false ELSE false END THEN 0.003902390004522083 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1646)) THEN false WHEN (first_name#335 = first_name#1646) THEN true WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.88) THEN false ELSE false END THEN 0.3365268364360054 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1646)) THEN false WHEN (first_name#335 = first_name#1646) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.95) THEN true WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.88) THEN false ELSE false END THEN 0.27677318122666966 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1646)) THEN false WHEN (first_name#335 = first_name#1646) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.88) THEN true ELSE false END THEN 0.1990847971311897 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1646)) THEN false WHEN (first_name#335 = first_name#1646) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1646) &gt;= 0.88) THEN false ELSE true END THEN 0.0010900609336073567 END * CASE WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1647)) THEN true WHEN (surname#336 = surname#1647) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.88) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1647)) THEN false WHEN (surname#336 = surname#1647) THEN true WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.88) THEN false ELSE false END THEN 91.08622270105276 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1647)) THEN false WHEN (surname#336 = surname#1647) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.95) THEN true WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.88) THEN false ELSE false END THEN 84.76958263140128 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1647)) THEN false WHEN (surname#336 = surname#1647) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.88) THEN true ELSE false END THEN 57.648261658181795 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1647)) THEN false WHEN (surname#336 = surname#1647) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1647) &gt;= 0.88) THEN false ELSE true END THEN 0.2827410382590715 END) * CASE WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN true WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN true WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 224.04335331944097 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN true WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 93.3426724371058 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN true WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 13.284421514226679 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN true WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 2.014287367664165 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN true ELSE false END THEN 0.41016658710113535 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1648)) THEN false WHEN (dob#337 = dob#1648) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1648) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1648, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE true END THEN 8.919181060305974E-7 END) * CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN true WHEN (city#338 = city#1649) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN true ELSE false END THEN 10.269631237504209 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN false ELSE true END THEN 0.45896561950688874 END) * CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN true WHEN (city#338 = city#1649) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN true ELSE false END THEN CASE WHEN isnotnull(coalesce(tf_city#341, tf_city#1652)) THEN POWER((0.0551475711801453 / CASE WHEN (coalesce(tf_city#341, tf_city#1652) &gt;= coalesce(tf_city#1652, tf_city#341)) THEN coalesce(tf_city#341, tf_city#1652) ELSE coalesce(tf_city#1652, tf_city#341) END), 1.0) ELSE 1.0 END WHEN CASE WHEN (isnull(city#338) OR isnull(city#1649)) THEN false WHEN (city#338 = city#1649) THEN false ELSE true END THEN 1.0 END) * CASE WHEN CASE WHEN (isnull(email#339) OR isnull(email#1650)) THEN true WHEN (email#339 = email#1650) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 2) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1650)) THEN false WHEN (email#339 = email#1650) THEN true WHEN (levenshtein(email#339, email#1650) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 2) THEN false ELSE false END THEN 256.78780963847476 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1650)) THEN false WHEN (email#339 = email#1650) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 1) THEN true WHEN (levenshtein(email#339, email#1650) &lt;= 2) THEN false ELSE false END THEN 236.76780078519445 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1650)) THEN false WHEN (email#339 = email#1650) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 2) THEN true ELSE false END THEN 207.73789452299874 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1650)) THEN false WHEN (email#339 = email#1650) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1650) &lt;= 2) THEN false ELSE true END THEN 0.13449069645050282 END) as string) as double)) &gt;= 3.1699250014423126))), false\n   :- *(3) Filter (isnotnull(unique_id#334) AND isnotnull(first_name#335))\n   :  +- *(3) ColumnarToRow\n   :     +- FileScan parquet [unique_id#334,first_name#335,surname#336,dob#337,city#338,email#339,tf_city#341] Batched: true, DataFilters: [isnotnull(unique_id#334), isnotnull(first_name#335)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/rosskennedy/splink_demos/tmp_checkpoints/4a3d6387-4dc2-4a2..., PartitionFilters: [], PushedFilters: [IsNotNull(unique_id), IsNotNull(first_name)], ReadSchema: struct&lt;unique_id:string,first_name:string,surname:string,dob:string,city:string,email:string,tf_c...\n   +- BroadcastQueryStage 0\n      +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, false]),false), [id=#4847]\n         +- *(1) Filter (isnotnull(unique_id#1645) AND isnotnull(first_name#1646))\n            +- *(1) ColumnarToRow\n               +- FileScan parquet [unique_id#1645,first_name#1646,surname#1647,dob#1648,city#1649,email#1650,tf_city#1652] Batched: true, DataFilters: [isnotnull(unique_id#1645), isnotnull(first_name#1646)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/rosskennedy/splink_demos/tmp_checkpoints/4a3d6387-4dc2-4a2..., PartitionFilters: [], PushedFilters: [IsNotNull(unique_id), IsNotNull(first_name)], ReadSchema: struct&lt;unique_id:string,first_name:string,surname:string,dob:string,city:string,email:string,tf_c...\n\n23/05/04 09:25:44 ERROR CodeGenerator: failed to compile: org.codehaus.janino.InternalCompilerException: Compiling \"GeneratedClass\" in \"generated.java\": Code of method \"bhj_doConsume_0$(Lorg/apache/spark/sql/catalyst/expressions/GeneratedClass$GeneratedIteratorForCodegenStage4;Lorg/apache/spark/unsafe/types/UTF8String;Lorg/apache/spark/unsafe/types/UTF8String;ZLorg/apache/spark/unsafe/types/UTF8String;Lorg/apache/spark/unsafe/types/UTF8String;ZLorg/apache/spark/unsafe/types/UTF8String;ZLorg/apache/spark/unsafe/types/UTF8String;ZDZ)V\" of class \"org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4\" grows beyond 64 KB\norg.codehaus.janino.InternalCompilerException: Compiling \"GeneratedClass\" in \"generated.java\": Code of method \"bhj_doConsume_0$(Lorg/apache/spark/sql/catalyst/expressions/GeneratedClass$GeneratedIteratorForCodegenStage4;Lorg/apache/spark/unsafe/types/UTF8String;Lorg/apache/spark/unsafe/types/UTF8String;ZLorg/apache/spark/unsafe/types/UTF8String;Lorg/apache/spark/unsafe/types/UTF8String;ZLorg/apache/spark/unsafe/types/UTF8String;ZLorg/apache/spark/unsafe/types/UTF8String;ZDZ)V\" of class \"org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4\" grows beyond 64 KB\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:366)\n    at org.codehaus.janino.UnitCompiler.access$000(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$1.visitCompilationUnit(UnitCompiler.java:336)\n    at org.codehaus.janino.UnitCompiler$1.visitCompilationUnit(UnitCompiler.java:333)\n    at org.codehaus.janino.Java$CompilationUnit.accept(Java.java:363)\n    at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:333)\n    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:235)\n    at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:464)\n    at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:314)\n    at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:237)\n    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:205)\n    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)\n    at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1489)\n    at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1586)\n    at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1583)\n    at org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n    at org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n    at org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n    at org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)\n    at org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)\n    at org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)\n    at org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)\n    at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1436)\n    at org.apache.spark.sql.execution.WholeStageCodegenExec.liftedTree1$1(WholeStageCodegenExec.scala:725)\n    at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:724)\n    at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:184)\n    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)\n    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)\n    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:180)\n    at org.apache.spark.sql.execution.UnionExec.$anonfun$doExecute$5(basicPhysicalOperators.scala:698)\n    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n    at scala.collection.Iterator.foreach(Iterator.scala:943)\n    at scala.collection.Iterator.foreach$(Iterator.scala:943)\n    at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n    at scala.collection.IterableLike.foreach(IterableLike.scala:74)\n    at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n    at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n    at scala.collection.TraversableLike.map(TraversableLike.scala:286)\n    at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n    at scala.collection.AbstractTraversable.map(Traversable.scala:108)\n    at org.apache.spark.sql.execution.UnionExec.doExecute(basicPhysicalOperators.scala:698)\n    at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:184)\n    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)\n    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)\n    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:180)\n    at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:135)\n    at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:135)\n    at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:140)\n    at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:139)\n    at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:68)\n    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)\n    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)\n    at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:68)\n    at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:67)\n    at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:115)\n    at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:170)\n    at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:170)\n    at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:172)\n    at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:82)\n    at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:256)\n    at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:254)\n    at scala.collection.immutable.List.foreach(List.scala:431)\n    at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:254)\n    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n    at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:226)\n    at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:365)\n    at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:350)\n    at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:184)\n    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)\n    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)\n    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:180)\n    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:186)\n    at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n    at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n    at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n    at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n    at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n    at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n    at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n    at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n    at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n    at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n    at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n    at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n    at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n    at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n    at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n    at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:781)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n    at py4j.Gateway.invoke(Gateway.java:282)\n    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n    at py4j.commands.CallCommand.execute(CallCommand.java:79)\n    at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n    at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n    at java.lang.Thread.run(Thread.java:750)\nCaused by: org.codehaus.janino.InternalCompilerException: Code of method \"bhj_doConsume_0$(Lorg/apache/spark/sql/catalyst/expressions/GeneratedClass$GeneratedIteratorForCodegenStage4;Lorg/apache/spark/unsafe/types/UTF8String;Lorg/apache/spark/unsafe/types/UTF8String;ZLorg/apache/spark/unsafe/types/UTF8String;Lorg/apache/spark/unsafe/types/UTF8String;ZLorg/apache/spark/unsafe/types/UTF8String;ZLorg/apache/spark/unsafe/types/UTF8String;ZDZ)V\" of class \"org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4\" grows beyond 64 KB\n    at org.codehaus.janino.CodeContext.makeSpace(CodeContext.java:1051)\n    at org.codehaus.janino.CodeContext.write(CodeContext.java:932)\n    at org.codehaus.janino.UnitCompiler.writeByte(UnitCompiler.java:12085)\n    at org.codehaus.janino.UnitCompiler.store(UnitCompiler.java:11769)\n    at org.codehaus.janino.UnitCompiler.store(UnitCompiler.java:11756)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2597)\n    at org.codehaus.janino.UnitCompiler.access$2700(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitLocalVariableDeclarationStatement(UnitCompiler.java:1506)\n    at org.codehaus.janino.UnitCompiler$6.visitLocalVariableDeclarationStatement(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:3712)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1573)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1559)\n    at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1496)\n    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$Block.accept(Java.java:2969)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1598)\n    at org.codehaus.janino.UnitCompiler.access$2600(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitDoStatement(UnitCompiler.java:1505)\n    at org.codehaus.janino.UnitCompiler$6.visitDoStatement(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$DoStatement.accept(Java.java:3664)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1573)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1559)\n    at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1496)\n    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$Block.accept(Java.java:2969)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1598)\n    at org.codehaus.janino.UnitCompiler.access$2600(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitDoStatement(UnitCompiler.java:1505)\n    at org.codehaus.janino.UnitCompiler$6.visitDoStatement(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$DoStatement.accept(Java.java:3664)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1573)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1559)\n    at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1496)\n    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$Block.accept(Java.java:2969)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2486)\n    at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1498)\n    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$IfStatement.accept(Java.java:3140)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1573)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1559)\n    at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1496)\n    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$Block.accept(Java.java:2969)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1848)\n    at org.codehaus.janino.UnitCompiler.access$2200(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1501)\n    at org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$WhileStatement.accept(Java.java:3245)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1573)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1559)\n    at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1496)\n    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$Block.accept(Java.java:2969)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2486)\n    at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1498)\n    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1490)\n    at org.codehaus.janino.Java$IfStatement.accept(Java.java:3140)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1573)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3420)\n    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1362)\n    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1335)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:807)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:975)\n    at org.codehaus.janino.UnitCompiler.access$700(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:392)\n    at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:384)\n    at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1445)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:384)\n    at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:1312)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:833)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:410)\n    at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:226)\n    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:389)\n    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:384)\n    at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1594)\n    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:384)\n    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:362)\n    ... 117 more\n23/05/04 09:25:44 WARN WholeStageCodegenExec: Whole-stage codegen disabled for plan (id=4):\n *(4) Project [CASE WHEN CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN true WHEN (city#338 = city#1657) THEN false ELSE false END THEN false WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN true ELSE false END THEN CASE WHEN isnotnull(coalesce(tf_city#341, tf_city#1660)) THEN (POWER((0.0551475711801453 / CASE WHEN (coalesce(tf_city#341, tf_city#1660) &gt;= coalesce(tf_city#1660, tf_city#341)) THEN coalesce(tf_city#341, tf_city#1660) ELSE coalesce(tf_city#1660, tf_city#341) END), 1.0) = Infinity) ELSE false END WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN false ELSE true END THEN false ELSE false END THEN Infinity ELSE LOG2(cast(cast((((((CASE WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1654)) THEN true WHEN (first_name#335 = first_name#1654) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.88) THEN false ELSE false END THEN 0.003902390004522083 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1654)) THEN false WHEN (first_name#335 = first_name#1654) THEN true WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.88) THEN false ELSE false END THEN 0.3365268364360054 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1654)) THEN false WHEN (first_name#335 = first_name#1654) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.95) THEN true WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.88) THEN false ELSE false END THEN 0.27677318122666966 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1654)) THEN false WHEN (first_name#335 = first_name#1654) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.88) THEN true ELSE false END THEN 0.1990847971311897 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1654)) THEN false WHEN (first_name#335 = first_name#1654) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.88) THEN false ELSE true END THEN 0.0010900609336073567 END * CASE WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1655)) THEN true WHEN (surname#336 = surname#1655) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.88) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1655)) THEN false WHEN (surname#336 = surname#1655) THEN true WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.88) THEN false ELSE false END THEN 91.08622270105276 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1655)) THEN false WHEN (surname#336 = surname#1655) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.95) THEN true WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.88) THEN false ELSE false END THEN 84.76958263140128 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1655)) THEN false WHEN (surname#336 = surname#1655) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.88) THEN true ELSE false END THEN 57.648261658181795 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1655)) THEN false WHEN (surname#336 = surname#1655) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.88) THEN false ELSE true END THEN 0.2827410382590715 END) * CASE WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN true WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN true WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 224.04335331944097 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN true WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 93.3426724371058 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN true WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 13.284421514226679 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN true WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 2.014287367664165 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN true ELSE false END THEN 0.41016658710113535 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE true END THEN 8.919181060305974E-7 END) * CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN true WHEN (city#338 = city#1657) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN true ELSE false END THEN 10.269631237504209 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN false ELSE true END THEN 0.45896561950688874 END) * CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN true WHEN (city#338 = city#1657) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN true ELSE false END THEN CASE WHEN isnotnull(coalesce(tf_city#341, tf_city#1660)) THEN POWER((0.0551475711801453 / CASE WHEN (coalesce(tf_city#341, tf_city#1660) &gt;= coalesce(tf_city#1660, tf_city#341)) THEN coalesce(tf_city#341, tf_city#1660) ELSE coalesce(tf_city#1660, tf_city#341) END), 1.0) ELSE 1.0 END WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN false ELSE true END THEN 1.0 END) * CASE WHEN CASE WHEN (isnull(email#339) OR isnull(email#1658)) THEN true WHEN (email#339 = email#1658) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 2) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1658)) THEN false WHEN (email#339 = email#1658) THEN true WHEN (levenshtein(email#339, email#1658) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 2) THEN false ELSE false END THEN 256.78780963847476 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1658)) THEN false WHEN (email#339 = email#1658) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 1) THEN true WHEN (levenshtein(email#339, email#1658) &lt;= 2) THEN false ELSE false END THEN 236.76780078519445 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1658)) THEN false WHEN (email#339 = email#1658) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 2) THEN true ELSE false END THEN 207.73789452299874 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1658)) THEN false WHEN (email#339 = email#1658) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 2) THEN false ELSE true END THEN 0.13449069645050282 END) as string) as double)) END AS match_weight#1666, CASE WHEN CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN true WHEN (city#338 = city#1657) THEN false ELSE false END THEN false WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN true ELSE false END THEN CASE WHEN isnotnull(coalesce(tf_city#341, tf_city#1660)) THEN (POWER((0.0551475711801453 / CASE WHEN (coalesce(tf_city#341, tf_city#1660) &gt;= coalesce(tf_city#1660, tf_city#341)) THEN coalesce(tf_city#341, tf_city#1660) ELSE coalesce(tf_city#1660, tf_city#341) END), 1.0) = Infinity) ELSE false END WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN false ELSE true END THEN false ELSE false END THEN 1.0 ELSE (CASE WHEN CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN true WHEN (city#338 = city#1657) THEN false ELSE false END THEN false WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN true ELSE false END THEN CASE WHEN isnotnull(coalesce(tf_city#341, tf_city#1660)) THEN (POWER((0.0551475711801453 / CASE WHEN (coalesce(tf_city#341, tf_city#1660) &gt;= coalesce(tf_city#1660, tf_city#341)) THEN coalesce(tf_city#341, tf_city#1660) ELSE coalesce(tf_city#1660, tf_city#341) END), 1.0) = Infinity) ELSE false END WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN false ELSE true END THEN false ELSE false END THEN Infinity ELSE cast(cast((((((CASE WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1654)) THEN true WHEN (first_name#335 = first_name#1654) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.88) THEN false ELSE false END THEN 0.003902390004522083 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1654)) THEN false WHEN (first_name#335 = first_name#1654) THEN true WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.88) THEN false ELSE false END THEN 0.3365268364360054 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1654)) THEN false WHEN (first_name#335 = first_name#1654) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.95) THEN true WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.88) THEN false ELSE false END THEN 0.27677318122666966 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1654)) THEN false WHEN (first_name#335 = first_name#1654) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.88) THEN true ELSE false END THEN 0.1990847971311897 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1654)) THEN false WHEN (first_name#335 = first_name#1654) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.88) THEN false ELSE true END THEN 0.0010900609336073567 END * CASE WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1655)) THEN true WHEN (surname#336 = surname#1655) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.88) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1655)) THEN false WHEN (surname#336 = surname#1655) THEN true WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.88) THEN false ELSE false END THEN 91.08622270105276 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1655)) THEN false WHEN (surname#336 = surname#1655) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.95) THEN true WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.88) THEN false ELSE false END THEN 84.76958263140128 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1655)) THEN false WHEN (surname#336 = surname#1655) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.88) THEN true ELSE false END THEN 57.648261658181795 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1655)) THEN false WHEN (surname#336 = surname#1655) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.88) THEN false ELSE true END THEN 0.2827410382590715 END) * CASE WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN true WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN true WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 224.04335331944097 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN true WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 93.3426724371058 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN true WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 13.284421514226679 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN true WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 2.014287367664165 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN true ELSE false END THEN 0.41016658710113535 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE true END THEN 8.919181060305974E-7 END) * CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN true WHEN (city#338 = city#1657) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN true ELSE false END THEN 10.269631237504209 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN false ELSE true END THEN 0.45896561950688874 END) * CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN true WHEN (city#338 = city#1657) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN true ELSE false END THEN CASE WHEN isnotnull(coalesce(tf_city#341, tf_city#1660)) THEN POWER((0.0551475711801453 / CASE WHEN (coalesce(tf_city#341, tf_city#1660) &gt;= coalesce(tf_city#1660, tf_city#341)) THEN coalesce(tf_city#341, tf_city#1660) ELSE coalesce(tf_city#1660, tf_city#341) END), 1.0) ELSE 1.0 END WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN false ELSE true END THEN 1.0 END) * CASE WHEN CASE WHEN (isnull(email#339) OR isnull(email#1658)) THEN true WHEN (email#339 = email#1658) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 2) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1658)) THEN false WHEN (email#339 = email#1658) THEN true WHEN (levenshtein(email#339, email#1658) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 2) THEN false ELSE false END THEN 256.78780963847476 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1658)) THEN false WHEN (email#339 = email#1658) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 1) THEN true WHEN (levenshtein(email#339, email#1658) &lt;= 2) THEN false ELSE false END THEN 236.76780078519445 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1658)) THEN false WHEN (email#339 = email#1658) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 2) THEN true ELSE false END THEN 207.73789452299874 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1658)) THEN false WHEN (email#339 = email#1658) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 2) THEN false ELSE true END THEN 0.13449069645050282 END) as string) as double) END / CASE WHEN CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN true WHEN (city#338 = city#1657) THEN false ELSE false END THEN false WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN true ELSE false END THEN CASE WHEN isnotnull(coalesce(tf_city#341, tf_city#1660)) THEN (POWER((0.0551475711801453 / CASE WHEN (coalesce(tf_city#341, tf_city#1660) &gt;= coalesce(tf_city#1660, tf_city#341)) THEN coalesce(tf_city#341, tf_city#1660) ELSE coalesce(tf_city#1660, tf_city#341) END), 1.0) = Infinity) ELSE false END WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN false ELSE true END THEN false ELSE false END THEN Infinity ELSE (1.0 + cast(cast((((((CASE WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1654)) THEN true WHEN (first_name#335 = first_name#1654) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.88) THEN false ELSE false END THEN 0.003902390004522083 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1654)) THEN false WHEN (first_name#335 = first_name#1654) THEN true WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.88) THEN false ELSE false END THEN 0.3365268364360054 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1654)) THEN false WHEN (first_name#335 = first_name#1654) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.95) THEN true WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.88) THEN false ELSE false END THEN 0.27677318122666966 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1654)) THEN false WHEN (first_name#335 = first_name#1654) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.88) THEN true ELSE false END THEN 0.1990847971311897 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1654)) THEN false WHEN (first_name#335 = first_name#1654) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.88) THEN false ELSE true END THEN 0.0010900609336073567 END * CASE WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1655)) THEN true WHEN (surname#336 = surname#1655) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.88) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1655)) THEN false WHEN (surname#336 = surname#1655) THEN true WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.88) THEN false ELSE false END THEN 91.08622270105276 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1655)) THEN false WHEN (surname#336 = surname#1655) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.95) THEN true WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.88) THEN false ELSE false END THEN 84.76958263140128 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1655)) THEN false WHEN (surname#336 = surname#1655) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.88) THEN true ELSE false END THEN 57.648261658181795 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1655)) THEN false WHEN (surname#336 = surname#1655) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.88) THEN false ELSE true END THEN 0.2827410382590715 END) * CASE WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN true WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN true WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 224.04335331944097 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN true WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 93.3426724371058 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN true WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 13.284421514226679 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN true WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 2.014287367664165 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN true ELSE false END THEN 0.41016658710113535 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE true END THEN 8.919181060305974E-7 END) * CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN true WHEN (city#338 = city#1657) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN true ELSE false END THEN 10.269631237504209 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN false ELSE true END THEN 0.45896561950688874 END) * CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN true WHEN (city#338 = city#1657) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN true ELSE false END THEN CASE WHEN isnotnull(coalesce(tf_city#341, tf_city#1660)) THEN POWER((0.0551475711801453 / CASE WHEN (coalesce(tf_city#341, tf_city#1660) &gt;= coalesce(tf_city#1660, tf_city#341)) THEN coalesce(tf_city#341, tf_city#1660) ELSE coalesce(tf_city#1660, tf_city#341) END), 1.0) ELSE 1.0 END WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN false ELSE true END THEN 1.0 END) * CASE WHEN CASE WHEN (isnull(email#339) OR isnull(email#1658)) THEN true WHEN (email#339 = email#1658) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 2) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1658)) THEN false WHEN (email#339 = email#1658) THEN true WHEN (levenshtein(email#339, email#1658) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 2) THEN false ELSE false END THEN 256.78780963847476 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1658)) THEN false WHEN (email#339 = email#1658) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 1) THEN true WHEN (levenshtein(email#339, email#1658) &lt;= 2) THEN false ELSE false END THEN 236.76780078519445 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1658)) THEN false WHEN (email#339 = email#1658) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 2) THEN true ELSE false END THEN 207.73789452299874 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1658)) THEN false WHEN (email#339 = email#1658) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 2) THEN false ELSE true END THEN 0.13449069645050282 END) as string) as double)) END) END AS match_probability#1667, unique_id#334 AS unique_id_l#1547, unique_id#1653 AS unique_id_r#1548, first_name#335 AS first_name_l#1549, first_name#1654 AS first_name_r#1550, CASE WHEN (isnull(first_name#335) OR isnull(first_name#1654)) THEN -1 WHEN (first_name#335 = first_name#1654) THEN 3 WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.95) THEN 2 WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.88) THEN 1 ELSE 0 END AS gamma_first_name#1661, CASE WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1654)) THEN true WHEN (first_name#335 = first_name#1654) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.88) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1654)) THEN false WHEN (first_name#335 = first_name#1654) THEN true WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.88) THEN false ELSE false END THEN 86.2360850776162 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1654)) THEN false WHEN (first_name#335 = first_name#1654) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.95) THEN true WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.88) THEN false ELSE false END THEN 70.92401859013204 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1654)) THEN false WHEN (first_name#335 = first_name#1654) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.88) THEN true ELSE false END THEN 51.01612009575941 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1654)) THEN false WHEN (first_name#335 = first_name#1654) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.88) THEN false ELSE true END THEN 0.2793316230167143 END AS bf_first_name#1668, surname#336 AS surname_l#1551, surname#1655 AS surname_r#1552, CASE WHEN (isnull(surname#336) OR isnull(surname#1655)) THEN -1 WHEN (surname#336 = surname#1655) THEN 3 WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.95) THEN 2 WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.88) THEN 1 ELSE 0 END AS gamma_surname#1662, CASE WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1655)) THEN true WHEN (surname#336 = surname#1655) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.88) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1655)) THEN false WHEN (surname#336 = surname#1655) THEN true WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.88) THEN false ELSE false END THEN 91.08622270105276 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1655)) THEN false WHEN (surname#336 = surname#1655) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.95) THEN true WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.88) THEN false ELSE false END THEN 84.76958263140128 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1655)) THEN false WHEN (surname#336 = surname#1655) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.88) THEN true ELSE false END THEN 57.648261658181795 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1655)) THEN false WHEN (surname#336 = surname#1655) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.88) THEN false ELSE true END THEN 0.2827410382590715 END AS bf_surname#1669, dob#337 AS dob_l#1553, dob#1656 AS dob_r#1554, CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN -1 WHEN (dob#337 = dob#1656) THEN 5 WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN 4 WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN 3 WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN 2 WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN 1 ELSE 0 END AS gamma_dob#1663, CASE WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN true WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN true WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 224.04335331944097 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN true WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 93.3426724371058 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN true WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 13.284421514226679 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN true WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 2.014287367664165 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN true ELSE false END THEN 0.41016658710113535 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE true END THEN 8.919181060305974E-7 END AS bf_dob#1670, city#338 AS city_l#1555, city#1657 AS city_r#1556, CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN -1 WHEN (city#338 = city#1657) THEN 1 ELSE 0 END AS gamma_city#1664, tf_city#341 AS tf_city_l#1557, tf_city#1660 AS tf_city_r#1558, CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN true WHEN (city#338 = city#1657) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN true ELSE false END THEN 10.269631237504209 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN false ELSE true END THEN 0.45896561950688874 END AS bf_city#1671, CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN true WHEN (city#338 = city#1657) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN true ELSE false END THEN CASE WHEN isnotnull(coalesce(tf_city#341, tf_city#1660)) THEN POWER((0.0551475711801453 / CASE WHEN (coalesce(tf_city#341, tf_city#1660) &gt;= coalesce(tf_city#1660, tf_city#341)) THEN coalesce(tf_city#341, tf_city#1660) ELSE coalesce(tf_city#1660, tf_city#341) END), 1.0) ELSE 1.0 END WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN false ELSE true END THEN 1.0 END AS bf_tf_adj_city#1672, email#339 AS email_l#1559, ... 4 more fields]\n+- *(4) BroadcastHashJoin [surname#336], [surname#1655], Inner, BuildRight, (((unique_id#334 &lt; unique_id#1653) AND (CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN true WHEN (city#338 = city#1657) THEN false ELSE false END THEN false WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN true ELSE false END THEN CASE WHEN isnotnull(coalesce(tf_city#341, tf_city#1660)) THEN (POWER((0.0551475711801453 / CASE WHEN (coalesce(tf_city#341, tf_city#1660) &gt;= coalesce(tf_city#1660, tf_city#341)) THEN coalesce(tf_city#341, tf_city#1660) ELSE coalesce(tf_city#1660, tf_city#341) END), 1.0) = Infinity) ELSE false END WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN false ELSE true END THEN false ELSE false END OR (LOG2(cast(cast((((((CASE WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1654)) THEN true WHEN (first_name#335 = first_name#1654) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.88) THEN false ELSE false END THEN 0.003902390004522083 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1654)) THEN false WHEN (first_name#335 = first_name#1654) THEN true WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.88) THEN false ELSE false END THEN 0.3365268364360054 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1654)) THEN false WHEN (first_name#335 = first_name#1654) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.95) THEN true WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.88) THEN false ELSE false END THEN 0.27677318122666966 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1654)) THEN false WHEN (first_name#335 = first_name#1654) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.88) THEN true ELSE false END THEN 0.1990847971311897 WHEN CASE WHEN (isnull(first_name#335) OR isnull(first_name#1654)) THEN false WHEN (first_name#335 = first_name#1654) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.95) THEN false WHEN (jaro_winkler(first_name#335, first_name#1654) &gt;= 0.88) THEN false ELSE true END THEN 0.0010900609336073567 END * CASE WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1655)) THEN true WHEN (surname#336 = surname#1655) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.88) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1655)) THEN false WHEN (surname#336 = surname#1655) THEN true WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.88) THEN false ELSE false END THEN 91.08622270105276 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1655)) THEN false WHEN (surname#336 = surname#1655) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.95) THEN true WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.88) THEN false ELSE false END THEN 84.76958263140128 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1655)) THEN false WHEN (surname#336 = surname#1655) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.88) THEN true ELSE false END THEN 57.648261658181795 WHEN CASE WHEN (isnull(surname#336) OR isnull(surname#1655)) THEN false WHEN (surname#336 = surname#1655) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.95) THEN false WHEN (jaro_winkler(surname#336, surname#1655) &gt;= 0.88) THEN false ELSE true END THEN 0.2827410382590715 END) * CASE WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN true WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN true WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 224.04335331944097 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN true WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 93.3426724371058 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN true WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 13.284421514226679 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN true WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE false END THEN 2.014287367664165 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN true ELSE false END THEN 0.41016658710113535 WHEN CASE WHEN (isnull(dob#337) OR isnull(dob#1656)) THEN false WHEN (dob#337 = dob#1656) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 1) THEN false WHEN (levenshtein(dob#337, dob#1656) &lt;= 2) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 1) THEN false WHEN (FLOOR(abs((months_between(gettimestamp(dob#337, yyyy-MM-dd, TimestampType, Some(Europe/London), false), gettimestamp(dob#1656, yyyy-MM-dd, TimestampType, Some(Europe/London), false), true, Some(Europe/London)) / 12.0), false)) &lt;= 10) THEN false ELSE true END THEN 8.919181060305974E-7 END) * CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN true WHEN (city#338 = city#1657) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN true ELSE false END THEN 10.269631237504209 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN false ELSE true END THEN 0.45896561950688874 END) * CASE WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN true WHEN (city#338 = city#1657) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN true ELSE false END THEN CASE WHEN isnotnull(coalesce(tf_city#341, tf_city#1660)) THEN POWER((0.0551475711801453 / CASE WHEN (coalesce(tf_city#341, tf_city#1660) &gt;= coalesce(tf_city#1660, tf_city#341)) THEN coalesce(tf_city#341, tf_city#1660) ELSE coalesce(tf_city#1660, tf_city#341) END), 1.0) ELSE 1.0 END WHEN CASE WHEN (isnull(city#338) OR isnull(city#1657)) THEN false WHEN (city#338 = city#1657) THEN false ELSE true END THEN 1.0 END) * CASE WHEN CASE WHEN (isnull(email#339) OR isnull(email#1658)) THEN true WHEN (email#339 = email#1658) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 2) THEN false ELSE false END THEN 1.0 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1658)) THEN false WHEN (email#339 = email#1658) THEN true WHEN (levenshtein(email#339, email#1658) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 2) THEN false ELSE false END THEN 256.78780963847476 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1658)) THEN false WHEN (email#339 = email#1658) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 1) THEN true WHEN (levenshtein(email#339, email#1658) &lt;= 2) THEN false ELSE false END THEN 236.76780078519445 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1658)) THEN false WHEN (email#339 = email#1658) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 2) THEN true ELSE false END THEN 207.73789452299874 WHEN CASE WHEN (isnull(email#339) OR isnull(email#1658)) THEN false WHEN (email#339 = email#1658) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 1) THEN false WHEN (levenshtein(email#339, email#1658) &lt;= 2) THEN false ELSE true END THEN 0.13449069645050282 END) as string) as double)) &gt;= 3.1699250014423126))) AND NOT coalesce((first_name#335 = first_name#1654), false)), false\n   :- *(4) Filter (isnotnull(unique_id#334) AND isnotnull(surname#336))\n   :  +- *(4) ColumnarToRow\n   :     +- FileScan parquet [unique_id#334,first_name#335,surname#336,dob#337,city#338,email#339,tf_city#341] Batched: true, DataFilters: [isnotnull(unique_id#334), isnotnull(surname#336)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/rosskennedy/splink_demos/tmp_checkpoints/4a3d6387-4dc2-4a2..., PartitionFilters: [], PushedFilters: [IsNotNull(unique_id), IsNotNull(surname)], ReadSchema: struct&lt;unique_id:string,first_name:string,surname:string,dob:string,city:string,email:string,tf_c...\n   +- BroadcastQueryStage 1\n      +- BroadcastExchange HashedRelationBroadcastMode(List(input[2, string, false]),false), [id=#4864]\n         +- *(2) Filter (isnotnull(unique_id#1653) AND isnotnull(surname#1655))\n            +- *(2) ColumnarToRow\n               +- FileScan parquet [unique_id#1653,first_name#1654,surname#1655,dob#1656,city#1657,email#1658,tf_city#1660] Batched: true, DataFilters: [isnotnull(unique_id#1653), isnotnull(surname#1655)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/rosskennedy/splink_demos/tmp_checkpoints/4a3d6387-4dc2-4a2..., PartitionFilters: [], PushedFilters: [IsNotNull(unique_id), IsNotNull(surname)], ReadSchema: struct&lt;unique_id:string,first_name:string,surname:string,dob:string,city:string,email:string,tf_c...\n\n23/05/04 09:25:46 WARN DataSource: All paths were ignored:                      \n  file:/Users/rosskennedy/splink_demos/tmp_checkpoints/4a3d6387-4dc2-4a2b-8726-37954ed09b3a/__splink__df_predict_8b906362e\n</code>\n</pre> <pre><code>results.as_pandas_dataframe(limit=5)\n</code></pre> match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name bf_first_name surname_l surname_r ... gamma_city tf_city_l tf_city_r bf_city bf_tf_adj_city email_l email_r gamma_email bf_email match_key 0 7.442390 0.994284 101 103 Alfie Alfie 3 86.236085 Griffiths Griffihs ... 0 0.014760 0.001230 0.458966 1.000000 a.griffiths@garner-bridges.com None -1 1.000000 0 1 6.319954 0.987638 366 369 Samuel Samuel 3 86.236085 None Campbell ... 0 0.001230 0.022140 0.458966 1.000000 samuelcampbell35@hebert.com samuelcampbell35@hebert.com 3 256.787810 0 2 12.763094 0.999856 417 419 Florence Florence 3 86.236085 Brown Brown ... 1 0.212792 0.212792 10.269631 0.259162 fb@reose.cem fb@reese.com 1 207.737895 0 3 13.709128 0.999925 607 609 Amelia Amelia 3 86.236085 Porter Porter ... 1 0.017220 0.017220 10.269631 3.202498 None None -1 1.000000 0 4 16.264552 0.999987 708 709 Maya Maya 3 86.236085 Curtsi Curtis ... -1 NaN 0.022140 1.000000 1.000000 mcurtis53@simpsoun.com micurtis53@simpson.com 1 207.737895 0 <p>5 rows \u00d7 28 columns</p>"},{"location":"demos/example_simple_pyspark.html#linking-in-spark","title":"Linking in Spark","text":""},{"location":"demos/example_transactions.html","title":"Linking financial transactions","text":"<pre><code># Use arrow to read in data to ensure date types are correct\nfrom pyarrow import parquet as pq\nfrom splink.duckdb.duckdb_linker import DuckDBLinker\nimport altair as alt\nalt.renderers.enable('mimetype')\n\ndf_origin = pq.read_table(\"./data/transactions_left.parquet\")\ndf_origin = df_origin.slice(length=1_000)\ndf_destination = pq.read_table(\"./data/transactions_right.parquet\")\ndf_destination = df_destination.slice(length=1_000)\nf\"There are {df_origin.num_rows:,.0f} records to match\"\n</code></pre> <pre>\n<code>'There are 1,000 records to match'</code>\n</pre> <pre><code># Some sample records\ndf_origin.to_pandas().head(2)\n</code></pre> ground_truth memo transaction_date amount unique_id 0 0 MATTHIAS C paym 2022-03-28 36.36 0 1 1 M CORVINUS dona 2022-02-14 221.91 1 <p>In the following chart, we can see this is a challenging dataset to link: - There are only 151 distinct transaction dates, with strong skew - Some 'memos' are used multiple times (up to 48 times) - There is strong skew in the 'amount' column, with 1,400 transactions of around 60.00</p> <pre><code># Simple settings just for exploratory analysis\nsettings = {\"link_type\": \"link_only\"}\nlinker = DuckDBLinker([df_origin, df_destination], settings,input_table_aliases=[\"__ori\", \"_dest\"])\nlinker.profile_columns([\"transaction_date\", \"memo\", \"round(amount/5, 0)*5\"])\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code># Design blocking rules that allow for differences in transaction date and amounts\n\nblocking_rule_date_1 = \"\"\"\n    strftime(l.transaction_date, '%Y%m') = strftime(r.transaction_date, '%Y%m')\n    and substr(l.memo, 1,3) = substr(r.memo,1,3)\n    and l.amount/r.amount &gt; 0.7   and l.amount/r.amount &lt; 1.3\n\"\"\"\n\n# Offset by half a month to ensure we capture case when the dates are e.g. 31st Jan and 1st Feb\nblocking_rule_date_2 = \"\"\"\n    strftime(l.transaction_date+15, '%Y%m') = strftime(r.transaction_date, '%Y%m')\n    and substr(l.memo, 1,3) = substr(r.memo,1,3)\n    and l.amount/r.amount &gt; 0.7   and l.amount/r.amount &lt; 1.3\n\"\"\"\n\nblocking_rule_memo = \"\"\"\nsubstr(l.memo,1,9) = substr(r.memo,1,9)\n\"\"\"\n\nblocking_rule_amount_1 = \"\"\"\nround(l.amount/2,0)*2 = round(r.amount/2,0)*2 and yearweek(r.transaction_date) = yearweek(l.transaction_date)\n\"\"\"\n\nblocking_rule_amount_2 = \"\"\"\nround(l.amount/2,0)*2 = round((r.amount+1)/2,0)*2 and yearweek(r.transaction_date) = yearweek(l.transaction_date + 4)\n\"\"\"\n\nblocking_rule_cheat = \"\"\"\nl.unique_id = r.unique_id\n\"\"\"\n\n\nlinker.cumulative_num_comparisons_from_blocking_rules_chart(\n    [\n        blocking_rule_date_1,\n        blocking_rule_date_2,\n        blocking_rule_memo,\n        blocking_rule_amount_1,\n        blocking_rule_amount_2,\n        blocking_rule_cheat,\n    ]\n)\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code># Full settings for linking model\nimport splink.duckdb.duckdb_comparison_level_library as cl\n\ncomparison_amount = {\n    \"output_column_name\": \"amount\",\n    \"comparison_levels\": [\n        cl.null_level(\"amount\"),\n        cl.exact_match_level(\"amount\"),\n        cl.percentage_difference_level(\"amount\",0.01),\n        cl.percentage_difference_level(\"amount\",0.03),\n        cl.percentage_difference_level(\"amount\",0.1),\n        cl.percentage_difference_level(\"amount\",0.3),\n        cl.else_level()\n    ],\n    \"comparison_description\": \"Amount percentage difference\",\n}\n\n\nfrom splink.duckdb import duckdb_comparison_library as cl\n\nsettings = {\n    \"link_type\": \"link_only\",\n    \"probability_two_random_records_match\": 1 / len(df_origin),\n    \"blocking_rules_to_generate_predictions\": [\n        blocking_rule_date_1,\n        blocking_rule_date_2,\n        blocking_rule_memo,\n        blocking_rule_amount_1,\n        blocking_rule_amount_2,\n        blocking_rule_cheat\n    ],\n    \"comparisons\": [\n        comparison_amount,\n        cl.jaccard_at_thresholds(\n            \"memo\", [0.9, 0.7]\n        ),\n        cl.datediff_at_thresholds(\"transaction_date\", \n                                date_thresholds = [1, 4, 10, 30],\n                                date_metrics = [\"day\", \"day\", \"day\", \"day\"],\n                                include_exact_match_level=False\n                                )\n    ],\n    \"retain_intermediate_calculation_columns\": True,\n    \"retain_matching_columns\": True,\n}\n</code></pre> <pre><code>linker = DuckDBLinker([df_origin, df_destination], settings,input_table_aliases=[\"__ori\", \"_dest\"])\n</code></pre> <pre><code>linker.estimate_u_using_random_sampling(max_pairs=1e6)\n</code></pre> <pre>\n<code>----- Estimating u probabilities using random sampling -----\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n/Users/rosskennedy/splink_demos/venv/lib/python3.9/site-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  warn(\n</code>\n</pre> <pre>\n<code>FloatProgress(value=0.0, layout=Layout(width='100%'), style=ProgressStyle(bar_color='black'))</code>\n</pre> <pre>\n<code>\nEstimated u probabilities using random sampling\n\nYour model is not yet fully trained. Missing estimates for:\n    - amount (no m values are trained).\n    - memo (no m values are trained).\n    - transaction_date (no m values are trained).\n</code>\n</pre> <pre><code>linker.estimate_parameters_using_expectation_maximisation(\"l.memo = r.memo\")\n</code></pre> <pre>\n<code>\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.memo = r.memo\n\nParameter estimates will be made for the following comparison(s):\n    - amount\n    - transaction_date\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - memo\n\nIteration 1: Largest change in params was -0.558 in the m_probability of amount, level `Exact match`\nIteration 2: Largest change in params was -0.213 in the m_probability of transaction_date, level `Within 1 day`\nIteration 3: Largest change in params was 0.0138 in probability_two_random_records_match\nIteration 4: Largest change in params was 0.00099 in the m_probability of transaction_date, level `Within 30 days`\nIteration 5: Largest change in params was 0.000107 in the m_probability of transaction_date, level `Within 30 days`\nIteration 6: Largest change in params was -8.04e-05 in the m_probability of amount, level `&lt; 30.00% diff`\n\nEM converged after 6 iterations\n\nYour model is not yet fully trained. Missing estimates for:\n    - memo (no m values are trained).\n</code>\n</pre> <pre>\n<code>&lt;EMTrainingSession, blocking on l.memo = r.memo, deactivating comparisons memo&gt;</code>\n</pre> <pre><code>session = linker.estimate_parameters_using_expectation_maximisation(\"l.amount = r.amount\")\n</code></pre> <pre>\n<code>\n----- Starting EM training session -----\n\nEstimating the m probabilities of the model by blocking on:\nl.amount = r.amount\n\nParameter estimates will be made for the following comparison(s):\n    - memo\n    - transaction_date\n\nParameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n    - amount\n\nIteration 1: Largest change in params was -0.357 in the m_probability of memo, level `Exact match`\nIteration 2: Largest change in params was 0.131 in the m_probability of memo, level `All other comparisons`\nIteration 3: Largest change in params was 0.103 in probability_two_random_records_match\nIteration 4: Largest change in params was 0.03 in probability_two_random_records_match\nIteration 5: Largest change in params was 0.00814 in probability_two_random_records_match\nIteration 6: Largest change in params was 0.00252 in probability_two_random_records_match\nIteration 7: Largest change in params was 0.00082 in probability_two_random_records_match\nIteration 8: Largest change in params was 0.000271 in probability_two_random_records_match\nIteration 9: Largest change in params was 9.02e-05 in probability_two_random_records_match\n\nEM converged after 9 iterations\n\nYour model is fully trained. All comparisons have at least one estimate for their m and u values\n</code>\n</pre> <pre><code>linker.match_weights_chart()\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code>df_predict = linker.predict(threshold_match_probability=0.001)\n</code></pre> <pre><code>linker.comparison_viewer_dashboard(df_predict,\"comparison_viewer_transactions.html\", overwrite=True)\nfrom IPython.display import IFrame\nIFrame(\n    src=\"./comparison_viewer_transactions.html\", width=\"100%\", height=1200\n)\n</code></pre> <pre><code>pred_errors =  linker.prediction_errors_from_labels_column(\"ground_truth\", include_false_positives=True, include_false_negatives=False)\nlinker.waterfall_chart(pred_errors.as_record_dict(limit=5))\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre> <pre><code>pred_errors =  linker.prediction_errors_from_labels_column(\"ground_truth\", include_false_positives=False, include_false_negatives=True)\nlinker.waterfall_chart(pred_errors.as_record_dict(limit=5))\n</code></pre> <pre>\n<code>&lt;VegaLite 4 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/troubleshooting.html\n</code>\n</pre>"},{"location":"demos/example_transactions.html#linking-banking-transactions","title":"Linking banking transactions","text":"<p>This example shows how to perform a one-to-one link on banking transactions.  </p> <p>The data is fake data, and was generated has the following features:</p> <ul> <li>Money shows up in the destination account with some time delay</li> <li>The amount sent and the amount received are not always the same - there are hidden fees and foreign exchange effects</li> <li>The memo is sometimes truncated and content is sometimes missing</li> </ul> <p>Since each origin payment should end up in the destination account, the <code>probability_two_random_records_match</code> of the model is known.</p>"},{"location":"dev_guides/caching.html","title":"Caching and pipelining","text":""},{"location":"dev_guides/caching.html#caching-and-pipelining","title":"Caching and pipelining","text":"<p>Splink is able to run against multiple SQL backends because all of the core data linking calculations are implemented in SQL. This SQL can therefore be submitted to a chosen SQL backend for execution.</p> <p>Computations in Splink often take the form of a number of <code>select</code> statements run in sequence.</p> <p>For example, the <code>predict()</code> step:</p> <ul> <li>Inputs <code>__splink__df_concat_with_tf</code> and outputs <code>__splink__df_blocked</code></li> <li>Inputs <code>__splink__df_blocked</code> and outputs <code>__splink__df_comparison_vectors</code></li> <li>Inputs <code>__splink__df_comparison_vectors</code> and outputs <code>__splink__df_match_weight_parts</code></li> <li>Inputs <code>__splink__df_match_weight_parts</code> and outputs <code>__splink__df_predict</code></li> </ul> <p>To make this run faster, two key optimisations are implmented:</p> <ul> <li>Pipelining - combining multiple <code>select</code> statements into a single statemenet using <code>WITH</code>(CTE) queries</li> <li>Caching: saving the results of calculations so they don't need recalculating. This is especially useful because some intermediate calculations are reused multiple times during a typical Splink session</li> </ul> <p>This article discusses the general implementation of caching and pipelining. The implementation needs some alterations for certain backends like Spark, which lazily evaluate SQL by default.</p>"},{"location":"dev_guides/caching.html#implementation-pipelining","title":"Implementation: Pipelining","text":"<p>A <code>SQLPipeline</code> class manages SQL pipelining.</p> <p>A <code>SQLPipeline</code> is composed of a number of <code>SQLTask</code> objects, each of which represents a select statement.</p> <p>The code is fairly straightforward: Given a sequence of <code>select</code> statements, <code>[a,b,c]</code> they are combined into a single query as follows:</p> <pre><code>with\na as (a_sql),\nb as (b_sql),\nc_sql\n</code></pre> <p>To make this work, each statement (a,b,c) in the pipeline must refer to the previous step by name. For example, <code>b_sql</code> probably selects from the <code>a_sql</code> table, which has been aliased <code>a</code>. So <code>b_sql</code> must use the table name <code>a</code> to refer to the result of <code>a_sql</code>.</p> <p>To make this tractable, each <code>SQLTask</code> has an <code>output_table_name</code>. For example, the <code>output_table_name</code> for <code>a_sql</code> in the above example is <code>a</code>.</p> <p>For instance, in the <code>predict()</code> pipeline above, the first <code>output_table_name</code> is <code>__splink__df_blocked</code>. By giving each task a meaningful <code>output_table_name</code>, subsequent tasks can reference previous outputs in a way which is semantically clear.</p>"},{"location":"dev_guides/caching.html#implementation-caching","title":"Implementation: Caching","text":"<p>When a SQL pipeline is executed, it has two output names:</p> <ul> <li>A <code>physical_name</code>, which is the name of the materialised table in the output database e.g. <code>__splink__df_predict_cbc9833</code></li> <li>A <code>templated_name</code>, which is a descriptive name of what the table represents e.g. <code>__splink__df_predict</code></li> </ul> <p>Each time Splink runs a SQL pipeline, the SQL string is hashed. This creates a unique identifier for that particular SQL string, which serves to identify the output.</p> <p>When Splink is asked to execute a SQL string, before execution, it checks whether the resultant table already exists. If it does, it returns the table rather than recomputing it.</p> <p>For example, when we run <code>linker.predict()</code>, Splink:</p> <ul> <li>Generates the SQL tasks</li> <li>Pipelines them into a single SQL statement</li> <li>Hashes the statement to create a physical name for the outputs <code>__splink__df_predict_cbc9833</code></li> <li>Checks whether a table with physical name <code>__splink__df_predict_cbc9833</code> alredy exists in the database</li> <li>If not, executes the SQL statement, creating table <code>__splink__df_predict_cbc9833</code> in the database.</li> </ul> <p>In terms of implementation, the following happens:</p> <ul> <li>SQL statements are generated an put in the queue - see here</li> <li>Once all the tasks have been added to the queue, we call <code>_execute_sql_pipeline()</code> see here</li> <li>The SQL is combined into a single pipelined statement here</li> <li>We call <code>_sql_to_splink_dataframe()</code> which returns the table (from the cache if it already exists, or it executes the sql)</li> <li>The table is returned as a <code>SplinkDataframe</code>, an abstraction over a table in a database. See here.</li> </ul>"},{"location":"dev_guides/caching.html#some-cached-tables-do-not-need-a-hash","title":"Some cached tables do not need a hash","text":"<p>A hash is required to uniquely identify some outputs. For example, blocking is used in several places in Splink, with different results. For example, the <code>__splink__df_blocked</code> needed to estimate parameters is different to the <code>__splink__df_blocked</code> needed in the <code>predict()</code> step.</p> <p>As a result, we cannot materialise a single table called <code>__splink__df_blocked</code> in the database and reues it multiple times. This is why we append the hash of the SQL, so that we can uniquely identify the different versions of <code>__splink__df_blocked</code> which are needed in different contexts.</p> <p>There are, however, some tables which are globally unique. They only take a single form, and if they exist in the cache they never need recomputing.</p> <p>An example of this is <code>__splink__df_concat_with_tf</code>, which represents the concatenation of the input dataframes.</p> <p>To create this table, we can execute <code>_sql_to_splink_dataframe</code> with <code>materialise_as_hash</code> set to <code>False</code>. The resultant materialised table will not have a hash appended, and will simply be called <code>__splink__df_concat_with_tf</code>. This is useful, because when performing calculations Splink can now check the cache for <code>__splink__df_concat_with_tf</code> each time it is needed.</p> <p>In fact, many Splink pipelines begin with the assumption that this table exists in the database, because the first <code>SQLTask</code> in the pipeline refers to a table named <code>__splink__df_concat_with_tf</code>. To ensure this is the case, a function is used to create this table if it doesn't exist.</p>"},{"location":"dev_guides/caching.html#using-pipelining-to-optimise-splink-workloads","title":"Using pipelining to optimise Splink workloads","text":"<p>At what point should a pipeline of <code>SQLTask</code>s be executed (materialised into a physical table)?</p> <p>For any individual output, it will usually be fastest to pipeline the full linage of tasks, right from raw data through to the end result.</p> <p>However, there are many intermediate outputs which are used by many different Splink operations.</p> <p>Performance can therefore be improved by computing and saving these intermediate outputs to a cache, to ensure they don't need to be computed repeatedly.</p> <p>This is achieved by enqueueing SQL to a pipline and strategically calling <code>execute_sql_pipeline</code> to materialise results that need to cached.</p>"},{"location":"dev_guides/debug_modes.html","title":"Understanding and debugging Splink","text":""},{"location":"dev_guides/debug_modes.html#understanding-and-debugging-splinks-computations","title":"Understanding and debugging Splink's computations","text":"<p>Splink contains tooling to help developers understand the underlying computations, how caching and pipelining is working, and debug problems.</p> <p>There are two main mechanisms: <code>debug_mode</code>, and setting different logging levels</p>"},{"location":"dev_guides/debug_modes.html#debug-mode","title":"Debug mode","text":"<p>You can turn on debug mode by setting <code>linker.debug_mode = True</code>.</p> <p>This has the following effects:</p> <ul> <li>Each step of Splink's calculations are executed in turn. That is, pipelining is switched off.</li> <li>The SQL statements being executed by Splink are displayed</li> <li>The results of the SQL statements are displayed in tabular format</li> </ul> <p>This is probably the best way to understand each step of the calculations being performed by Splink - because a lot of the implementation gets 'hidden' within pipelines for performance reasons.</p> <p>Note that enabling debug mode will dramatically reduce Splink's performance!</p>"},{"location":"dev_guides/debug_modes.html#logging","title":"Logging","text":"<p>Splink has a range of logging modes that output information about what Splink is doing at different levels of verbosity.</p> <p>Unlike debug mode, logging doesn't affect the performance of Splink.</p>"},{"location":"dev_guides/debug_modes.html#logging-levels","title":"Logging levels","text":"<p>You can set the logging level with code like <code>logging.getLogger(\"splink\").setLevel(desired_level)</code> although see notes below about gotyas.</p> <p>The logging levels in Splink are:</p> <ul> <li><code>logging.INFO</code> (<code>20</code>): This outputs user facing messages about the training status of Splink models</li> <li><code>15</code>: Outputs additional information about time taken and parameter estimation</li> <li><code>logging.DEBUG</code> (<code>10</code>): Outputs information about the names of the SQL statements executed</li> <li><code>logging.DEBUG</code> (<code>7</code>): Outputs information about the names of the components of the SQL pipelines</li> <li><code>logging.DEBUG</code> (<code>5</code>): Outputs the SQL statements themselves</li> </ul>"},{"location":"dev_guides/debug_modes.html#how-to-control-logging","title":"How to control logging","text":"<p>Note that by default Splink sets the logging level to <code>INFO</code> on initialisation</p>"},{"location":"dev_guides/debug_modes.html#with-basic-logging","title":"With basic logging","text":"<pre><code>import logging\nlinker = DuckDBLinker(df, settings, set_up_basic_logging=False)\n\n# This must come AFTER the linker is intialised, because the logging level\n# will be set to INFO\nlogging.getLogger(\"splink\").setLevel(logging.DEBUG)\n</code></pre>"},{"location":"dev_guides/debug_modes.html#without-basic-logging","title":"Without basic logging","text":"<pre><code># This code can be anywhere since set_up_basic_logging is False\nimport logging\nlogging.basicConfig(format=\"%(message)s\")\nsplink_logger = logging.getLogger(\"splink\")\nsplink_logger.setLevel(logging.INFO)\n\nlinker = DuckDBLinker(df, settings, set_up_basic_logging=False)\n</code></pre>"},{"location":"dev_guides/spark_pipelining_and_caching.html","title":"Spark caching","text":""},{"location":"dev_guides/spark_pipelining_and_caching.html#caching-and-pipelining-in-spark","title":"Caching and pipelining in Spark","text":"<p>This article assumes you've read the general guide to caching and pipelining.</p> <p>In Spark, some additions have to be made to this general pattern because all transformation in Spark are lazy.</p> <p>That is, when we call <code>df = spark.sql(sql)</code>, the <code>df</code> is not immediately computed.</p> <p>Furthermore, even when an action is called, the results aren't automatically persisted by Spark to disk. This differs from other backends, which execute SQL as a <code>create table</code> statement, meaning that the result is automatically saved.</p> <p>This interferes with caching, because Splink assumes that when the the function <code>_execute_sql_against_backend()</code> is called, this will be evaluted greedily (immediately evaluated) AND the results will be saved to the 'database'.</p> <p>Another quirk of Spark is that it chunks work up into tasks. This is relevant for two reasons:</p> <ul> <li>Tasks can suffer from skew, meaning some take longer than others, which can be bad from a performance point of view.</li> <li>The number of tasks and how data is partitioned controls how many files are output when results are saved. Some Splink operations results in a very large number of small files which can take a long time to read and write, relative to the same data stored in fewer files.</li> </ul> <p>Repartitioning can be used to rebalance workloads (reduce skew) and to avoid the 'many small files' problem.</p>"},{"location":"dev_guides/spark_pipelining_and_caching.html#spark-specific-modifications","title":"Spark-specific modifications","text":"<p>The logic for Spark is captured in the implementation of <code>_execute_sql_against_backend()</code> in the spark_linker.py.</p> <p>This has three roles:</p> <ul> <li>It determines how to save result - using either <code>persist</code>, <code>checkpoint</code> or saving to <code>.parquet</code>, with <code>.parquet</code> being the default.</li> <li>It determines which results to save. Some small results such <code>__splink__m_u_counts</code> are immediately converted using <code>toPandas()</code> rather than being saved. This is because saving to disk and reloading is expensive and unnecessary.</li> <li>It chooses which Spark dataframes to repartition to reduce the number of files which are written/read</li> </ul> <p>Note that repartitioning and saving is independent. Some dataframes are saved without repartitioning. Some dataframes are repartitioned without being saved.</p>"},{"location":"dev_guides/transpilation.html","title":"SQL Transpilation in Splink, and how we support multiple SQL backends","text":"<p>In Splink, all the core data linking algorithms are implemented in SQL. This allows computation to be offloaded to a SQL backend of the users choice.</p> <p>One difficulty with this paradigm is that SQL implementations differ - the functions available in (say) the Spark dialect of SQL differ from those available in DuckDB SQL. And to make matters worse, functions with the same name may behave differently (e.g. different arguments, arguments in different orders, etc.).</p> <p>Splink therefore needs a mechanism of writing SQL statements that are able to run against all the target SQL backends (engines).</p> <p>Details are as follows:</p>"},{"location":"dev_guides/transpilation.html#1-core-data-linking-algorithms-are-splink","title":"1. Core data linking algorithms are Splink","text":"<p>Core data linking algorithms are implmented in 'backend agnostic' SQL. So they're written using basic SQL functions that are common across the available in all the target backends, and don't need any translation.</p> <p>It has been possible to write all of the core Splink logic in SQL that is consistent between dialects.</p> <p>However, this is not the case with <code>Comparisons</code>, which tend to use backend specific SQL functions like <code>jaro_winker</code>, whose functino names and signatures differ between backends.</p>"},{"location":"dev_guides/transpilation.html#2-user-provided-sql-is-interpolated-into-these-dialect-agnostic-sql-statements","title":"2. User-provided SQL is interpolated into these dialect-agnostic SQL statements","text":"<p>The user provides custom SQL is two places in Splink:</p> <ol> <li>Blocking rules</li> <li>The <code>sql_condition</code> (see here) provided as part of a <code>Comparison</code></li> </ol> <p>The user is free to write this SQL however they want.</p> <p>It's up to the user to ensure the SQL they provide will execute successfully in their chosen backend. So the <code>sql_condition</code> must use functions that exist in the target execution engine</p> <p>The custom SQL is interpolated into the the SQL statements generated by Splink.</p> <p>Users are also able to use the <code>comparison_level_library</code> and <code>comparison_library</code> for their chosen backend.</p> <p>These are backend specific and are imported like <code>from splink.spark.spark_comparison_level_library import jaro_winkler_level</code>. This ensures that the syntax matches the chosen execution backend</p>"},{"location":"dev_guides/transpilation.html#3-backends-can-implement-transpilation-and-or-dielct-steps-to-further-transform-the-sql-if-needed","title":"3. Backends can implement transpilation and or dielct steps to further transform the SQL if needed","text":"<p>Occasionally some modifications are needed to the SQL to ensure it executes against the target backend.</p> <p><code>sqlglot</code> is used for this purpose. For instance, a custom dialect is implemented in the sparklinker.</p> <p>A transformer is implemented in the Athena linker.</p>"},{"location":"dev_guides/udfs.html","title":"User Defined Functions","text":"<p>User Defined Functions (UDFs) are functions that can be created to add functionality to a given SQL backend that does not already exist. These are particularly useful within Splink as it supports multiple SQL engines each with different inherent functionalty. UDFs are an important tool for creating consistent functionality across backends.</p> <p>For example, DuckDB has an in-built string comparison function for Jaccard similarity whereas Spark SQL doesn't have an equivalent function. Therefore, a UDF is required to use functions like jaccard_at_thresholds() and jaccard_level() with a Spark backend.</p>"},{"location":"dev_guides/udfs.html#spark","title":"Spark","text":"<p>Spark supports UDFs written in Scala and Java.</p> <p>Splink currently uses UDFs written in Scala and are implemented as follows:</p> <ul> <li>The UDFs are created in a separate repository, splink_scalaudfs, with the Scala functions being defined in Similarity.scala. </li> <li>The functions are then stored in a Java Archive (JAR) file - for more on JAR files, see the java documentation.</li> <li>Once the JAR file containing the UDFs has been created, it is copied across to the spark_jars folder in Splink.</li> <li>Specify the the correct jar location within Splink.</li> <li>UDFS are then registered within the Spark Linker.</li> </ul> <p>Now the Spark UDFs have been successfully registered, they can be used in SparkSQL. For example, </p> <pre><code>jaccard(\"name_column_1\", \"name_column_2\") &gt;= 0.9\n</code></pre> <p>which provides the basis for functions such as jaccard_at_thresholds() and jaccard_level().</p>"},{"location":"dev_guides/udfs.html#duckdb","title":"DuckDB","text":"<p>DuckDB supports UDFs written in C++, however these have not yet been implemented in Splink.</p>"},{"location":"dev_guides/udfs.html#athena","title":"Athena","text":"<p>Athena supports UDFs written in Java, however these have not yet been implemented in Splink.</p>"},{"location":"dev_guides/udfs.html#sqlite","title":"SQLite","text":"<p>SQLite supports UDFs written in C and C++, however these have not yet been implemented in Splink.</p>"},{"location":"dev_guides/changing_splink/build_docs_locally.html","title":"Building Docs","text":""},{"location":"dev_guides/changing_splink/build_docs_locally.html#building-docs-locally","title":"Building docs locally","text":"<p>To rapidly build the documentation and immediately see changes you've made you can use this script:</p> <pre><code>source scripts/make_docs_locally.sh\n</code></pre> <p>This is much faster than waiting for github actions to run if you're trying to make fiddly changes to formatting etc.</p> <p>The Splink repo contains a working <code>requirements.txt</code> for building the docs, or a more complete version:</p> <pre><code>attrs==22.2.0\nbeautifulsoup4==4.11.2\nbleach==6.0.0\ncertifi==2022.12.7\ncharset-normalizer==3.0.1\nclick==8.1.3\ncolorama==0.4.6\ndefusedxml==0.7.1\nEditorConfig==0.12.3\nfastjsonschema==2.16.2\nghp-import==2.1.0\ngitdb==4.0.10\nGitPython==3.1.31\ngriffe==0.25.5\nidna==3.4\nimportlib-metadata==6.0.0\nJinja2==3.0.3\njsbeautifier==1.14.7\njsonschema==4.17.3\njsonschema2md==0.4.0\njupyter_client==8.0.3\njupyter_core==5.2.0\njupyterlab-pygments==0.2.2\nMarkdown==3.3.7\nMarkupSafe==2.1.2\nmergedeep==1.3.4\nmistune==2.0.5\nmkdocs==1.4.2\nmkdocs-autorefs==0.4.1\nmkdocs-click==0.8.0\nmkdocs-gen-files==0.4.0\nmkdocs-include-markdown-plugin==4.0.3\nmkdocs-material==8.5.11\nmkdocs-material-extensions==1.1.1\nmkdocs-mermaid2-plugin==0.6.0\nmkdocs-monorepo-plugin==1.0.4\nmkdocs-schema-reader==0.11.1\nmkdocs-semiliterate==0.7.0\nmkdocs-simple-plugin==2.1.2\nmkdocstrings==0.20.0\nmkdocstrings-python==0.8.3\nmkdocstrings-python-legacy==0.2.3\nmknotebooks==0.7.1\nnbclient==0.7.2\nnbconvert==7.2.9\nnbformat==5.7.3\npackaging==23.0\npandocfilters==1.5.0\nplatformdirs==3.0.0\nPygments==2.14.0\npymdown-extensions==9.9.2\npyrsistent==0.19.3\npython-dateutil==2.8.2\npython-slugify==8.0.0\npytkdocs==0.16.1\nPyYAML==6.0\npyyaml_env_tag==0.1\npyzmq==25.0.0\nrequests==2.28.2\nsix==1.16.0\nsmmap==5.0.0\nsoupsieve==2.4\ntext-unidecode==1.3\ntinycss2==1.2.1\ntornado==6.2\ntraitlets==5.9.0\nurllib3==1.26.14\nwatchdog==2.2.1\nwebencodings==0.5.1\nzipp==3.14.0\n</code></pre>"},{"location":"dev_guides/changing_splink/building_env_locally.html","title":"Building a Virtual Environment","text":""},{"location":"dev_guides/changing_splink/building_env_locally.html#building-a-virtual-environment-for-splink","title":"Building a virtual environment for Splink","text":"<p>Splink uses <code>poetry</code> to track and manage our core dependencies and additionally resolve any package conflicts that these may result in.</p> <p>A list of the core dependencies used within Splink can be found in pyproject.toml.</p> <p>To automatically create a simple virtual environment using <code>venv</code>, simply run this script: <pre><code>source scripts/create_venv.sh\n</code></pre></p>"},{"location":"dev_guides/changing_splink/lint.html","title":"Linting","text":""},{"location":"dev_guides/changing_splink/lint.html#linting-your-code","title":"Linting your code","text":"<p>For linting, we currently make use of both ruff and black.</p> <p>These are used to ensure we produce readable, maintainable, and more consistent code.</p> <p>To quickly run both linters, simply run this bash script. The -f flag is called to run an automatic fix with ruff. If you simply wish for ruff to print the errors it finds to the console, remove this flag.</p> <p><pre><code>source scripts/lint.sh -f  # with the fix flag\n</code></pre> <pre><code>source scripts/lint.sh  # without\n</code></pre></p> <p>Alternatively, you can run ruff and black separately from a terminal instance.</p> <p>For black, you need to run: <code>black .</code></p> <p>and ruff requires: <code>ruff --fix .</code> for automatic fixes and error printouts or <code>ruff --show-source .</code> for a more thorough breakdown.</p>"},{"location":"dev_guides/changing_splink/releases.html","title":"Releasing a new version of Splink","text":"<p>Splink is regularly updated with releases to add new features or bug fixes to the package.</p> <p>Below are the steps for releasing a new version of Splink:</p> <ol> <li>On a new branch, update pyproject.toml and init.py with the latest version.</li> <li>Open a pull request to merge the new branch with the master branch (the base branch).</li> <li> <p>Once the pull request has been approved, merge the changes and generate a new release in the releases section of the repo, including:</p> </li> <li> <p>Choosing a new release tag (which matches your updates to pyproject.toml and init.py). Ensure that your release tag follows semantic versioning. The target branch should be set to master.</p> </li> </ol> <p>)</p> <ul> <li>Generating release notes. This can be done automatically by pressing the   button. </li> </ul> <p>This will give you release notes based off the Pull Requests which have been merged since the last release.</p> <p>For example </p> <ul> <li>Publish as the latest release</li> </ul> <p></p> <p>Now your release should be published to pypi.</p>"},{"location":"dev_guides/changing_splink/testing.html","title":"Testing in Splink","text":"<p>Tests in Splink make use of the pytest framework. You can find the tests themselves in the tests folder.</p> <p>Splink tests can be broadly categorised into three sets:</p> <ul> <li>'Core' tests - these are tests which test some specific bit of functionality which does not depend on any specific SQL dialect. They are usually unit tests - examples are testing <code>InputColumn</code> and testing the latitude-longitude distance calculation.</li> <li>Backend-agnostic tests - these are tests which run against some SQL backend, but which are written in such a way that they can run against many backends by making use of the backend-agnostic testing framework. The majority of tests are of this type.</li> <li>Backend-specific tests - these are tests which run against a specific SQL backend, and test some feature particular to this backend. There are not many of these, as Splink is designed to run very similarly independent of the backend used.</li> </ul> <p>Info</p> <p>We currently do not have support for testing the <code>athena</code> backend, due to the complication of needing a connection to an AWS account. All other backends have testing available.</p>","tags":["Testing","Pytest","Backends"]},{"location":"dev_guides/changing_splink/testing.html#running-tests","title":"Running tests","text":"","tags":["Testing","Pytest","Backends"]},{"location":"dev_guides/changing_splink/testing.html#running-tests-locally","title":"Running tests locally","text":"<p>To run tests locally, simply run: <pre><code>python3 -m pytest tests/\n</code></pre> or alternatively <pre><code>pytest tests/\n</code></pre></p> <p>To run a single test file, append the filename to the <code>tests/</code> folder call, for example: <pre><code>pytest tests/test_u_train.py\n</code></pre> or for a single test, additionally append the test name after a pair of colons, as: <pre><code>pytest tests/test_u_train.py::test_u_train_multilink\n</code></pre></p> Further useful pytest options <p>There may be many warnings emitted, for instance by library dependencies, cluttering your output in which case you can use <code>--disable-pytest-warnings</code> or <code>-W ignore</code> so that these will not be displayed. Some additional command-line options that may be useful:</p> <ul> <li><code>-s</code> to disable output capture, so that test output is displayed in the terminal in all cases</li> <li><code>-v</code> for verbose mode, where each test instance will be displayed on a separate line with status</li> <li><code>-q</code> for quiet mode, where output is extremely minimal</li> <li><code>-x</code> to fail on first error/failure rather than continuing to run all selected tests<ul> <li></li> </ul> </li> <li><code>-m some_mark</code> run only those tests marked with <code>some_mark</code> - see below for useful options here</li> </ul> <p>For instance usage might be: <pre><code># ignore warnings, display output\npytest -W ignore -s tests/\n</code></pre></p> <p>or <pre><code># ignore warnings, verbose output, fail on first error/failure\npytest -W ignore -v -x tests/\n</code></pre></p> <p>You can find a host of other available options using pytest's in-built help: <pre><code>pytest -h\n</code></pre></p>","tags":["Testing","Pytest","Backends"]},{"location":"dev_guides/changing_splink/testing.html#running-tests-for-specific-backends-or-backend-groups","title":"Running tests for specific backends or backend groups","text":"<p>You may wish to run tests relating to to specific backends, tests which are backend-independent, or any combinations of these. Splink allows for various combinations by making use of <code>pytest</code>'s <code>mark</code> feature.</p> <p>If when you invoke pytest you pass no marks explicitly, there will be an implicit mark of <code>default</code>, as per the pyproject.toml pytest.ini configuration.</p> <p>The available options are:</p>","tags":["Testing","Pytest","Backends"]},{"location":"dev_guides/changing_splink/testing.html#run-core-tests","title":"Run core tests","text":"<p>Option for running only the backend-independent 'core' tests:</p> <ul> <li><code>pytest tests/ -m core</code> - run only the 'core' tests, meaning those without dialect-dependence. In practice this means any test that hasn't been decorated using <code>mark_with_dialects_excluding</code> or <code>mark_with_dialects_including</code>.</li> </ul>","tags":["Testing","Pytest","Backends"]},{"location":"dev_guides/changing_splink/testing.html#run-tests-on-a-specific-backend","title":"Run tests on a specific backend","text":"<p>Options for running tests on one backend only - this includes tests written specifically for that backend, as well as backend-agnostic tests supported for that backend.</p> <ul> <li><code>pytest tests/ -m duckdb</code> - run all <code>duckdb</code> tests, and all <code>core</code> tests<ul> <li>&amp; similarly for other dialects</li> </ul> </li> <li><code>pytest tests/ -m duckdb_only</code> - run all <code>duckdb</code> tests only, and not the <code>core</code> tests<ul> <li>&amp; similarly for other dialects</li> </ul> </li> </ul>","tags":["Testing","Pytest","Backends"]},{"location":"dev_guides/changing_splink/testing.html#run-tests-across-multiple-backends","title":"Run tests across multiple backends","text":"<p>Options for running tests on multiple backends (including all backends) - this includes tests written specifically for those backends, as well as backend-agnostic tests supported for those backends.</p> <ul> <li><code>pytest tests/ -m default</code> or equivalently <code>pytest tests/</code> - run all tests in the <code>default</code> group. The <code>default</code> group consists of the <code>core</code> tests, and those dialects in the <code>default</code> group - currently <code>spark</code> and <code>duckdb</code>.<ul> <li>Other groups of dialects can be added and will similarly run with <code>pytest tests/ -m new_dialect_group</code>. Dialects within the current scope of testing and the groups they belong to are defined in the <code>dialect_groups</code> dictionary in tests/decorator.py</li> </ul> </li> <li><code>pytest tests/ -m all</code> run all tests for all available dialects</li> </ul> <p>These all work alongside all the other pytest options, so for instance to run the tests for training <code>probability_two_random_records_match</code> for only <code>duckdb</code>, ignoring warnings, with quiet output, and exiting on the first failure/error: <pre><code>pytest -W ignore -q -x -m duckdb tests/test_estimate_prob_two_rr_match.py\n</code></pre></p> Running tests with docker \ud83d\udc33 <p>If you want to test Splink against a specific version of python, the easiest method is to utilise docker \ud83d\udc33.</p> <p>Docker allows you to more quickly and easily install a specific version of python and run the existing test library against it.</p> <p>This is particularly useful if you're using py &gt; 3.9.10 (which is currently in use in our tests github action) and need to run a secondary set of tests.</p> <p>A pre-built Dockerfile for running tests against python version 3.9.10 can be located within scripts/run_tests.Dockerfile.</p> <p>To run, simply use the following docker command from within a terminal and the root folder of a splink clone: <pre><code>docker build -t run_tests:testing -f scripts/run_tests.Dockerfile . &amp;&amp; docker run --rm --name splink-test run_tests:testing\n</code></pre></p> <p>This will both build and run the tests library.</p> <p>Feel free to replace <code>run_tests:testing</code> with an image name and tag you're happy with.</p> <p>Reusing the same image and tag will overwrite your existing image.</p> <p>You can also overwrite the default <code>CMD</code> if you want a different set of <code>pytest</code> command-line options, for example <pre><code>docker run --rm --name splink-test run_tests:testing pytest -W ignore -m spark tests/test_u_train.py\n</code></pre></p>","tags":["Testing","Pytest","Backends"]},{"location":"dev_guides/changing_splink/testing.html#tests-in-ci","title":"Tests in CI","text":"<p>Splink utilises github actions to run tests for each pull request. This consists of a few independent checks:</p> <ul> <li>The full test suite is run separately against several different python versions</li> <li>The example notebooks are checked to ensure they run without error</li> <li>The tutorial notebooks are checked to ensure they run without error</li> </ul>","tags":["Testing","Pytest","Backends"]},{"location":"dev_guides/changing_splink/testing.html#writing-tests","title":"Writing tests","text":"","tags":["Testing","Pytest","Backends"]},{"location":"dev_guides/changing_splink/testing.html#core-tests","title":"Core tests","text":"<p>Core tests are treated the same way as ordinary pytest tests. Any test is marked as <code>core</code> by default, and will only be excluded from being a core test if it is decorated using either:</p> <ul> <li><code>@mark_with_dialects_excluding</code> for backend-agnostic tests, or</li> <li><code>@mark_with_dialects_including</code> for backend-specific tests</li> </ul> <p>from the test decorator file.</p>","tags":["Testing","Pytest","Backends"]},{"location":"dev_guides/changing_splink/testing.html#backend-agnostic-testing","title":"Backend-agnostic testing","text":"<p>The majority of tests should be written using the backend-agnostic testing framework. This just provides some small tools which allow tests to be written in a backend-independent way. This means the tests can then by run against all available SQL backends (or a subset, if some lack necessary features for the test).</p> <p>As an example, let's consider a test that will run on all dialects, and then break down the various parts to see what each is doing.</p> <pre><code>from tests.decorator import mark_with_dialects_excluding\n\n@mark_with_dialects_excluding()\ndef test_feature_that_works_for_all_backends(test_helpers, dialect, some_other_test_fixture):\n    helper = test_helpers[dialect]\n\n    df = helper.load_frame_from_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\")\n\n    settings_dict = {\n        \"link_type\": \"dedupe_only\",\n        \"blocking_rules_to_generate_predictions\": [\"l.city = r.city\", \"l.surname = r.surname\", \"l.dob = r.dob\"],\n        \"comparisons\": [\n            helper.cl.exact_match(\"city\"),\n            helper.cl.levenshtein_at_thresholds(\"first_name\", [1, 2]),\n            helper.cl.levenshtein_at_thresholds(\"surname\"),\n            {\n                \"output_column_name\": \"email\",\n                \"comparison_description\": \"Email\",\n                \"comparison_levels\": [\n                    helper.cll.null_level(\"email\"),\n                    helper.cll.exact_match_level(\"email\"),\n                    helper.cll.levenshtein_level(\"email\", 2),\n                    {\n                        \"sql_condition\": \"substr(email_l, 1) = substr(email_r, 1)\",\n                        \"label_for_charts\": \"email first character matches\",\n                    },\n                    helper.cll.else_level(),\n                ]\n            }\n        ]\n    }\n\n    linker = helper.Linker(df, settings_dict, **helper.extra_linker_args())\n\n    # and then some actual testing logic\n</code></pre> <p>Firstly you should import the decorator-factory <code>mark_with_dialects_excluding</code>, which will decorate each test function:</p> <pre><code>from tests.decorator import mark_with_dialects_excluding\n</code></pre> <p>Then we define the function, and pass parameters:</p> <pre><code>@mark_with_dialects_excluding()\ndef test_feature_that_works_for_all_backends(test_helpers, dialect, some_other_test_fixture):\n</code></pre> <p>The decorator <code>@mark_with_dialects_excluding()</code> will do two things:</p> <ul> <li>marks the test it decorates with the appropriate custom <code>pytest</code> marks. This ensures that it will be run with tests for each dialect, excluding any that are passed as arguments; in this case it will be run for all dialects, as we have passed no arguments.</li> <li>parameterises the test with a string parameter <code>dialect</code>, which will be used to configure the test for that dialect. The test will run for each value of <code>dialect</code> possible, excluding any passed to the decorator (none in this case).</li> </ul> <p>You should aim to exclude as few dialects as possible - consider if you really need to exclude any. Dialects should only be excluded if the test doesn't make sense for them due to features they lack. The default choice should be the decorator with no arguments <code>@mark_with_dialects_excluding()</code>, meaning the test runs for all dialects.</p> <pre><code>@mark_with_dialects_excluding()\ndef test_feature_that_works_for_all_backends(test_helpers, dialect, some_other_test_fixture):\n</code></pre> <p>As well as the parameter <code>dialect</code> (which is provided by the decorator), we must also pass the helper-factory fixture <code>test_helpers</code>. We can additionally pass further fixtures if needed - in this case <code>some_other_test_fixture</code>. We could similarly provide an explicit parameterisation to the test, in which case we would also pass these parameters - see the pytest docs on parameterisation for more information.</p> <pre><code>    helper = test_helpers[dialect]\n</code></pre> <p>The fixture <code>test_helpers</code> is simply a dictionary of the specific-dialect test helpers - here we pick the appropriate one for our test.</p> <p>Each helper has the same set of methods and properties, which encapsulate all of the dialect-dependencies. You can find the full set of properties and methods by examining the source for the base class <code>TestHelper</code>.</p> <pre><code>    df = helper.load_frame_from_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\")\n</code></pre> <p>Here we are now actually using a method of the test helper - in this case we are loading a table from a csv to the database and returning it in a form suitable for passing to a Splink linker.</p> <p><pre><code>    \"comparisons\": [\nhelper.cl.exact_match(\"city\"),\nhelper.cl.levenshtein_at_thresholds(\"first_name\", [1, 2]),\nhelper.cl.levenshtein_at_thresholds(\"surname\"),\n{\n            \"output_column_name\": \"email\",\n</code></pre> We reference the dialect-specific comparison library as <code>helper.cl</code>,</p> <p><pre><code>    {\n        \"output_column_name\": \"email\",\n        \"comparison_description\": \"Email\",\n        \"comparison_levels\": [\nhelper.cll.null_level(\"email\"),\nhelper.cll.exact_match_level(\"email\"),\nhelper.cll.levenshtein_level(\"email\", 2),\n{\n                \"sql_condition\": \"substr(email_l, 1) = substr(email_r, 1)\",\n                \"label_for_charts\": \"email first character matches\",\n            }\nhelper.cll.else_level(),\n]\n    }\n</code></pre> and the dialect-specific comparison level library as <code>helper.cll</code>.</p> <p><pre><code>    {\n\"sql_condition\": \"substr(email_l, 1) = substr(email_r, 1)\",\n\"label_for_charts\": \"email first character matches\",\n    },\n</code></pre> We can include raw SQL statements, but we must ensure they are valid for all dialects we are considering, so we should avoid any unusual functions that are not likely to be universal.</p> <p><pre><code>    linker = helper.Linker(df, settings_dict, **helper.extra_linker_args())\n</code></pre> Finally we instantiate the linker, passing any default set of extra arguments provided by the helper, which some dialects require.</p> <p>From this point onwards we will be working with the instantiated <code>linker</code>, and so will not need to refer to <code>helper</code> any more - the rest of the test can be written as usual.</p>","tags":["Testing","Pytest","Backends"]},{"location":"dev_guides/changing_splink/testing.html#excluding-some-backends","title":"Excluding some backends","text":"<p>Now let's have a small look at a similar example - only this time we are going to exclude the <code>sqlite</code> backend, as the test relies on features not directly available for that backend. In this example that will be the SQL function <code>split_part</code> which does not exist in the <code>sqlite</code> dialect.</p> <p>Warning</p> <p>Tests should be made available to the widest range of backends possible. Only exclude backends if features not shared by all backends are crucial to the test-logic - otherwise consider rewriting things so that all backends are covered.</p> <pre><code>from tests.decorator import mark_with_dialects_excluding\n\n@mark_with_dialects_excluding(\"sqlite\")\ndef test_feature_that_doesnt_work_with_sqlite(test_helpers, dialect, some_other_test_fixture):\n    helper = test_helpers[dialect]\n\n    df = helper.load_frame_from_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\")\n\n    settings_dict = {\n        \"link_type\": \"dedupe_only\",\n        \"blocking_rules_to_generate_predictions\": [\"l.city = r.city\", \"l.surname = r.surname\", \"l.dob = r.dob\"],\n        \"comparisons\": [\n            helper.cl.exact_match(\"city\"),\n            helper.cl.levenshtein_at_thresholds(\"first_name\", [1, 2]),\n            helper.cl.levenshtein_at_thresholds(\"surname\"),\n            {\n                \"output_column_name\": \"email\",\n                \"comparison_description\": \"Email\",\n                \"comparison_levels\": [\n                    helper.cll.null_level(\"email\"),\n                    helper.cll.exact_match_level(\"email\"),\n                    helper.cll.levenshtein_level(\"email\", 2),\n                    {\n                        \"sql_condition\": \"split_part(email_l, '@', 1) = split_part(email_r, '@', 1)\",\n                        \"label_for_charts\": \"email local-part matches\",\n                    },\n                    helper.cll.else_level(),\n                ]\n            }\n        ]\n    }\n\n    linker = helper.Linker(df, settings_dict, **helper.extra_linker_args())\n\n    # and then some actual testing logic\n</code></pre> <p>The key difference is the argument we pass to the decorator: <pre><code>@mark_with_dialects_excluding(\"sqlite\")\ndef test_feature_that_doesnt_work_with_sqlite(test_helpers, dialect, some_other_test_fixture):\n</code></pre> As above this marks the test it decorates with the appropriate custom <code>pytest</code> marks, but in this case it ensures that it will be run with tests for each dialect excluding sqlite. Again <code>dialect</code> is passed as a parameter, and the test will run in turn for each value of <code>dialect</code> except for 'sqlite'.</p> <p><pre><code>    {\n\"sql_condition\": \"split_part(email_l, '@', 1) = split_part(email_r, '@', 1)\",\n\"label_for_charts\": \"email local-part matches\",\n    }\n</code></pre> This line is why we cannot allow <code>sqlite</code> for this test - we make use of the function <code>split_part</code> which is not available in the <code>sqlite</code> dialect, hence its exclusion. We suppose that this particular comparison level is crucial for the test to make sense, otherwise we would rewrite this line to make it run universally. When you come to run the tests, this test will not run on the <code>sqlite</code> backend.</p> <p>If you need to exclude multiple dialects this is also possible - just pass each as an argument. For example, to decorate a test that is not supported on <code>spark</code> or <code>sqlite</code>, use the decorator <code>@mark_with_dialects_excluding(\"sqlite\", \"spark\")</code>.</p>","tags":["Testing","Pytest","Backends"]},{"location":"dev_guides/changing_splink/testing.html#backend-specific-tests","title":"Backend-specific tests","text":"<p>If you intend to write a test for a specific backend, first consider whether it is definitely specific to that backend - if not then a backend-agnostic test would be preferable, as then your test will be run against many backends. If you really do need to test features peculiar to one backend, then you can write it simply as you would an ordinary <code>pytest</code> test. The only difference is that you should decorate it with <code>@mark_with_dialects_including</code> (from tests/decorator.py) - for example:</p> DuckDBSparkSQLite <pre><code>@mark_with_dialects_including(\"duckdb\")\ndef test_some_specific_duckdb_feature():\n    ...\n</code></pre> <pre><code>@mark_with_dialects_including(\"spark\")\ndef test_some_specific_spark_feature():\n    ...\n</code></pre> <pre><code>@mark_with_dialects_including(\"sqlite\")\ndef test_some_specific_sqlite_feature():\n    ...\n</code></pre> <p>This ensures that the test gets marked appropriately for running when the <code>Spark</code> tests should be run, and excludes it from the set of <code>core</code> tests.</p> <p>Note that unlike the exclusive <code>mark_with_dialects_excluding</code>, this decorator will not paramaterise the test with the <code>dialect</code> argument. This is because usage of the inclusive form is largely designed for single-dialect tests. If you wish to override this behaviour and parameterise the test you can use the argument <code>pass_dialect</code>, for example <code>@mark_with_dialects_including(\"spark\", \"sqlite\", pass_dialect=True)</code>, in which case you would need to write the test in a backend-independent manner.</p>","tags":["Testing","Pytest","Backends"]},{"location":"dev_guides/comparisons/extending_library_comparisons_and_levels.html","title":"Extending existing comparisons and comparison levels","text":"<p>Creating a linkage (or deduplication) model necessitates making various comparisons between (or within) your data sets. There is some choice available in what kind of comparisons you will wish to do for the linkage problem you are dealing with. Splink comes with several comparisons ready to use directly, as well as several comparison levels that you can use to construct your own comparison. You may find that within these you find yourself using a specialised version repeatedly, and would like to make a shorthand for this and contribute it to the Splink library for other users to benefit from - this page will aid you in this process.</p> <p>This guide supplements the guide for adding entirely new comparisons and comparison levels to show how things work when you are extending existing entries.</p>","tags":["API","comparisons","fuzzy-matching"]},{"location":"dev_guides/comparisons/extending_library_comparisons_and_levels.html#subclassing-existing-library-comparison-levels","title":"Subclassing existing library comparison levels","text":"<p>For this example, let's consider a comparison level that returns a match on two strings within a fixed Hamming distance. This is a specific example of the generic string distance function comparison level.</p> <p>In this case, working in <code>splink/comparison_level_library.py</code>, we simply subclass the appropriate level, and call its constructor, fixing whatever properties we need (using dialect-specific properties where appropriate - in this case the name of the function which calculates Hamming distance, which will be stored in the property <code>self._hamming_name</code>):</p> <pre><code>class HammingLevelBase(DistanceFunctionLevelBase):\n    def __init__(\n        self,\n        col_name: str,\n        distance_threshold: int,\n        m_probability=None,\n    ):\n\"\"\"Represents a comparison using a Hamming distance function,\n\n        Args:\n            col_name (str): Input column name\n            distance_threshold (Union[int, float]): The threshold to use to assess\n                similarity\n            m_probability (float, optional): Starting value for m probability.\n                Defaults to None.\n\n        Returns:\n            ComparisonLevel: A comparison level that evaluates the\n                Hamming similarity\n        \"\"\"\n        super().__init__(\n            col_name,\n            self._hamming_name,\n            distance_threshold,\n            higher_is_more_similar=False,\n            m_probability=m_probability,\n        )\n</code></pre> <p>The rest of the process is identical to that described for creating brand-new comparison levels.</p>","tags":["API","comparisons","fuzzy-matching"]},{"location":"dev_guides/comparisons/extending_library_comparisons_and_levels.html#subclassing-existing-library-comparisons","title":"Subclassing existing library comparisons","text":"<p>As in our <code>hamming_level</code> example above, you can similarly subclass existing library <code>Comparisons</code> to create e.g. <code>hamming_at_thresholds</code> from the more generic <code>distance_function_at_thresholds</code>, similarly to how we create new comparisons. The main difficulty here is that the subclassed comparison levels will have different function arguments, so we need to check within the constructor if we are in the generic version (<code>distance_function_at_thresholds</code>) or a specific version (<code>hamming_level</code>). See the source code for <code>DistanceFunctionAtThreholdsComparisonBase</code> for an example.</p>","tags":["API","comparisons","fuzzy-matching"]},{"location":"dev_guides/comparisons/new_library_comparisons_and_levels.html","title":"Creating new comparisons and comparison levels for libraries","text":"<p>The Fellegi-Sunter model that Splink implements depends on having several comparisons, which are each composed of two or more comparison levels. Splink provides several ready-made comparisons and comparison levels to use out-of-the-box, but you may find in your particular application that you have to create your own custom versions if there is not a suitable comparison/level for the SQL dialect you are working with (or for any available dialects).</p> <p>Having created a custom comparison you may decide that your use case is common enough that you want to contribute it to Splink for other users to benefit from. This guide will take you through the process of doing so. Looking at existing examples should also prove to be useful for further guidance, and to perhaps serve as a starting template.</p> <p>After creating your new levels/comparisons, be sure to add some tests to help ensure that the code runs without error and behaves as expected.</p> <p>This guide is for adding new comparisons and comparison levels from scratch. If you instead want to create specialised versions of existing levels, be sure to also have a look at the guide for extending existing library entries.</p>","tags":["API","comparisons"]},{"location":"dev_guides/comparisons/new_library_comparisons_and_levels.html#creating-new-comparison-levels","title":"Creating new comparison levels","text":"<p>For this example, let's consider a comparison level that compares if the length of two arrays are within <code>n</code> of one another (without reference to the contents of these arrays) for some non-negative integer <code>n</code>. An example of this might be if we were linking people with partial address information in two tables --- in one table we have an array of postcodes, and the other table we have an array of road-names. We don't expect them to match, but we are probably interested if the count of the number of objects within each array are similar - each corresponding to the number of addresses per person.</p> <p>To create a new comparison level, you must create a new subclass of <code>ComparisonLevel</code> which will serve as the base comparison level for any SQL dialects that will allow this level, in the file <code>splink/comparison_level_library.py</code>. It will contain the full logic for creating the comparison level - any dialect dependencies will be implemented as properties on the specific dialect-dependent object. In this case we will need to refer to a property <code>_array_length_function_name</code>, as this can vary by dialect. We will not define the property directly on this object, as our dialect-dependent versions will inherit this property from elsewhere. We also include any customisable parameters for our level - in this case we will allow options for the maximum number of elements the array may differ by.</p> <pre><code>class ArrayLengthLevelBase(ComparisonLevel):\n    def __init__(\n        self,\n        col_name: str,\n        length_difference: int,\n        m_probability=None,\n    ):\n\"\"\"Compares two arrays whose sizes are within a fixed distance\n        | length(arr_1) - length(arr_2) | &lt;= (length_difference)\n\n        Arguments:\n            col_name (str): Input column name\n            length_difference (int): Maximum difference in array lengths\n            m_probability (float, optional): Starting value for m probability.\n                Defaults to None.\n\n        Returns:\n            ComparisonLevel: A comparison level that evaluates the size difference\n                between two arrays\n        \"\"\"\n        col = InputColumn(col_name, sql_dialect=self._sql_dialect)\n        col_l, col_r = col.names_l_r()\n\n        sql_exp = (\n            f\"abs(\"\n            f\"{self._array_length_function_name}({col_l}) - \"\n            f\"{self._array_length_function_name}({col_r})\"\n            f\") &lt;= {length_difference}\"\n        )\n        level_dict = {\n            \"sql_condition\": sql_exp,\n            \"label_for_charts\": f\"Array sizes differ by at most {length_difference)\",\n        }\n\n        if m_probability:\n            level_dict[\"m_probability\"] = m_probability\n\n        super().__init__(level_dict, sql_dialect=self._sql_dialect)\n</code></pre> <p>If you are using a new dialect-dependent property (as we are in this case), then it should be added as a property on <code>DialectBase</code> in <code>splink.dialect_base.py</code>, with either a sensible default value, or raising a <code>NotImplementedError</code>:</p> <pre><code>class DialectBase():\n    ...\n    @property\n    def _array_length_function_name():\n        raise NotImplementedError(\n            \"`array_length_function_name` not implemented on base class\"\n        )\n</code></pre> <p>Then any dialects that use a different value can override this (e.g in <code>splink.spark.spark_helpers.spark_base</code>):</p> <pre><code>class SparkBase(DialectBase):\n    ...\n    @property\n    def _array_length_function_name():\n        return \"array_size\"\n</code></pre> <p>Then any dialects where this comparison level can be used can simply inherit from this dialect-specific base, along with the comparison level base <code>ArrayLengthLevelBase</code> - here in <code>splink.spark.spark_helpers.spark_comparison_imports</code>:</p> <pre><code>class array_length_level(SparkBase, ArrayLengthLevelBase):\n    pass\n</code></pre> <p>Similarly for DuckDB define the appropriate function name in the base <code>splink.duckdb.duckdb_helpers.duckdb_base</code></p> <pre><code>class DuckDBBase(DialectBase):\n    ...\n    @property\n    def _array_length_function_name():\n        return \"array_length\"\n</code></pre> <p>and then simply create the level in the corresponding library <code>splink.duckdb.duckdb_helpers.duckdb_comparison_imports</code>:</p> <pre><code>class array_length_level(DuckDBBase, ArrayLengthLevelBase):\n    pass\n</code></pre> <p>The names of these should be the same for all dialects (and written in snake-case), with them being distinguished solely by path.</p>","tags":["API","comparisons"]},{"location":"dev_guides/comparisons/new_library_comparisons_and_levels.html#creating-new-comparisons","title":"Creating new comparisons","text":"<p>The process for creating new library <code>Comparison</code>s is similar to the <code>ComparisonLevel</code> case, but slightly more involved. This is due to the fact that dialect-specific <code>Comparison</code>s need to 'know' about the dialect-specific <code>ComparisonLevel</code>s that they employ.</p> <p>As an example, we will consider a new <code>Comparison</code> that makes use of our new <code>array_length_level</code> above. Specifically, it will have the following levels:</p> <ul> <li>an optional <code>exact_match_level</code></li> <li>one or more <code>array_length_level</code>s with different values of <code>length_difference</code>, as specified</li> <li>an <code>else_level</code></li> </ul> <pre><code>class ArrayLengthAtThresholdsComparisonBase(Comparison):\n    def __init__(\n        self,\n        col_name: str,\n        length_thresholds: Union[int, list] = [0],\n        include_exact_match_level=True,\n        term_frequency_adjustments=False,\n        m_probability_exact_match=None,\n        m_probability_or_probabilities_sizes: Union[float, list] = None,\n        m_probability_else=None,\n    ):\n\"\"\"A comparison of the data in the array column `col_name` with various\n        size difference thresholds to assess similarity levels.\n\n        An example of the output with default arguments and settings\n        `length_thresholds = [0]` would be\n        - An exact match\n        - The two arrays are the same length\n        - Anything else (i.e. the arrays are difference lengths)\n\n        Args:\n            col_name (str): The name of the array column to compare.\n            length_thresholds (Union[int, list], optional): The difference(s) between\n                array sizes of thresholds, to assess whether two arrays are within a\n                given length difference.\n            include_exact_match_level (bool, optional): If True, include an exact match\n                level. Defaults to True.\n            term_frequency_adjustments (bool, optional): If True, apply term frequency\n                adjustments to the exact match level. Defaults to False.\n            m_probability_exact_match (float, optional): If provided, overrides the\n                default m probability for the exact match level. Defaults to None.\n            m_probability_or_probabilities_sizes (Union[float, list], optional):\n                _description_. If provided, overrides the default m probabilities\n                for the sizes specified. Defaults to None.\n            m_probability_else (float, optional): If provided, overrides the\n                default m probability for the 'anything else' level. Defaults to None.\n\n        Returns:\n            Comparison: A comparison that can be inclued in the Splink settings\n                dictionary.\n        \"\"\"\n\n        thresholds = ensure_is_iterable(length_thresholds)\n\n        if m_probability_or_probabilities_sizes is None:\n            m_probability_or_probabilities_sizes = [None] * len(thresholds)\n        m_probabilities = ensure_is_iterable(m_probability_or_probabilities_sizes)\n\n        comparison_levels = []\n        comparison_levels.append(self._null_level(col_name))\n        if include_exact_match_level:\n            level = self._exact_match_level(\n                col_name,\n                term_frequency_adjustments=term_frequency_adjustments,\n                m_probability=m_probability_exact_match,\n            )\n            comparison_levels.append(level)\n\n        for length_thres, m_prob in zip(thresholds, m_probabilities):\n            level = self._array_length_level(\n                col_name,\n                length_difference=length_thres,\n                m_probability=m_prob,\n            )\n            comparison_levels.append(level)\n\n        comparison_levels.append(\n            self._else_level(m_probability=m_probability_else),\n        )\n\n        comparison_desc = \"\"\n        if include_exact_match_level:\n            comparison_desc += \"Exact match vs. \"\n\n        thres_desc = \", \".join(thresholds)\n        plural = \"\" if len(thresholds) == 1 else \"s\"\n        comparison_desc += (\n            f\"Array length differences with threshold{plural} {thres_desc} vs. \"\n        )\n        comparison_desc += \"anything else\"\n\n        comparison_dict = {\n            \"comparison_description\": comparison_desc,\n            \"comparison_levels\": comparison_levels,\n        }\n        super().__init__(comparison_dict)\n</code></pre> <p>Crucially we needed to use <code>self._null_level</code>, <code>self._exact_match_level</code>, <code>self._else_level</code> which already exist, but also the new <code>self._array_length_level</code> which relates to our new comparison level. We will need to make sure that the dialect-specific comparisons which will actually be used will have this property.</p> <p>Each dialect has a comparison properties base, which stores information about all of the dialect-specific comparison levels used by all comparisons. We will need to add our new level to this, which we referred to above in <code>ArrayLengthAtThresholdsComparisonBase</code> - for this example in <code>splink.spark.spark_helpers.spark_comparison_imports</code>: <pre><code>from splink.spark.spark_comparison_level_library import (\n    exact_match_level,\n    ...\n    array_length_level,\n)\n...\n\nclass SparkComparisonProperties(SparkBase):\n    @property\n    def _exact_match_level(self):\n        return exact_match_level\n    ...\n    @property\n    def _array_length_level(self):\n        return array_length_level\n</code></pre></p> <p>Any dialect-specific version of comparisons will inherit from this (where it learns about the dialect-specific comparison levels), and the comparison itself; in our case, in the same file:</p> <pre><code>...\nclass array_length_at_thresholds(\n    SparkComparisonProperties, ArrayLengthAtThresholds\n):\n    pass\n</code></pre> <p>This is now ready to import and be used, just as any other pre-existing comparisons.</p>","tags":["API","comparisons"]},{"location":"includes/tags.html","title":"Tags","text":"<p>Following is a list of relevant tags:</p> <p>[TAGS]</p>"},{"location":"includes/generated_files/comparison_level_library_dialect_table.html","title":"Comparison level library dialect table","text":"duckdb spark athena sqlite <code>array_intersect_level</code> \u2713 \u2713 \u2713 <code>columns_reversed_level</code> \u2713 \u2713 \u2713 \u2713 <code>damerau_levenshtein_level</code> \u2713 \u2713 <code>datediff_level</code> \u2713 \u2713 <code>distance_function_level</code> \u2713 \u2713 \u2713 \u2713 <code>distance_in_km_level</code> \u2713 \u2713 \u2713 <code>else_level</code> \u2713 \u2713 \u2713 \u2713 <code>exact_match_level</code> \u2713 \u2713 \u2713 \u2713 <code>jaccard_level</code> \u2713 \u2713 <code>jaro_level</code> \u2713 \u2713 <code>jaro_winkler_level</code> \u2713 \u2713 <code>levenshtein_level</code> \u2713 \u2713 \u2713 \u2713 <code>null_level</code> \u2713 \u2713 \u2713 \u2713 <code>percentage_difference_level</code> \u2713 \u2713 \u2713 \u2713"},{"location":"includes/generated_files/comparison_library_dialect_table.html","title":"Comparison library dialect table","text":"duckdb spark athena sqlite <code>array_intersect_at_sizes</code> \u2713 \u2713 \u2713 <code>damerau_levenshtein_at_thresholds</code> \u2713 \u2713 <code>datediff_at_thresholds</code> \u2713 \u2713 <code>distance_function_at_thresholds</code> \u2713 \u2713 \u2713 \u2713 <code>distance_in_km_at_thresholds</code> \u2713 \u2713 \u2713 <code>exact_match</code> \u2713 \u2713 \u2713 \u2713 <code>jaccard_at_thresholds</code> \u2713 \u2713 <code>jaro_at_thresholds</code> \u2713 \u2713 <code>jaro_winkler_at_thresholds</code> \u2713 \u2713 <code>levenshtein_at_thresholds</code> \u2713 \u2713 \u2713 \u2713"},{"location":"includes/generated_files/comparison_template_library_dialect_table.html","title":"Comparison template library dialect table","text":"duckdb spark athena sqlite <code>date_comparison</code> \u2713 \u2713 <code>forename_surname_comparison</code> \u2713 \u2713 <code>name_comparison</code> \u2713 \u2713 <code>postcode_comparison</code> \u2713 \u2713 \u2713"},{"location":"settingseditor/editor.html","title":"Settings Editor","text":"","tags":["settings"]},{"location":"topic_guides/backends.html","title":"Splink's SQL backends: Spark, DuckDB, etc","text":"<p>Splink is a Python library. It implements all data linking computations by generating SQL, and submitting the SQL statements to a backend of the user's chosing for execution.</p> <p>For smaller input datasets of up to 1-2 million records, users can link data in Python on their laptop using the DuckDB backend. This is the recommended approach because the DuckDB backend is installed automatically when the user installs Splink using <code>pip install splink</code>. No additional configuration is needed.</p> <p>Linking larger datasets requires highly computationally intensive calculations, and generates datasets which are too large to be processed on a standard laptop. For these scenarios, we recommend using one of Splink's big data backend - currently Spark or AWS Athena. When these backends are used, the SQL generated by Splink is sent to the chosen backend for execution.</p> <p>The Splink code you write is almost identical between backends, so it's straightforward to migrate between backends. Often, it's a good idea to start working using DuckDB on a sample of data, because it will produce results very quickly. When you're comfortable with your model, you may wish to migrate to a big data backend to estimate/predict on the full dataset.</p>","tags":["Spark","DuckDB","Athena","SQLite","Backends"]},{"location":"topic_guides/backends.html#choosing-a-backend","title":"Choosing a backend","text":"<p>Import the linker from the backend of your choosing, and the backend-specific comparison libraries.</p> <p>Once you have initialised the <code>linker</code> object, there is no difference in the subequent code between backends.</p> <p>Note however, that not all comparison functions are available in all backends. There are tables detailing the available functions for each backend on the comparison library API page and the comparison level library API page.</p> DuckDBSparkAWS AthenaSQLite <pre><code>from splink.duckdb.duckdb_linker import DuckDBLinker\nimport splink.duckdb.duckdb_comparison_library as cl\nimport splink.duckdb.duckdb_comparison_level_library as cll\n\nlinker = DuckDBLinker(your_args)\n</code></pre> <pre><code>from splink.spark.spark_linker import SparkLinker\nimport splink.spark.spark_comparison_library as cl\nimport splink.spark.spark_comparison_level_library as cll\n\nlinker = SparkLinker(your_args)\n</code></pre> <pre><code>from splink.athena.athena_linker import AthenaLinker\nimport splink.athena.athena_comparison_library as cl\nimport splink.athena.athena_comparison_level_library as cll\n\nlinker = AthenaLinker(your_args)\n</code></pre> <pre><code>from splink.sqlite.sqlite_linker import SQLiteLinker\nimport splink.sqlite.sqlite_comparison_library as cl\nimport splink.sqlite.sqlite_comparison_level_library as cll\n\nlinker = SQLiteLinker(your_args)\n</code></pre>","tags":["Spark","DuckDB","Athena","SQLite","Backends"]},{"location":"topic_guides/blocking_rules.html","title":"Difference between <code>blocking_rules_to_generate_predictions</code> vs blocking rules for estimation","text":"<p>What is the difference between the list of <code>blocking_rules_to_generate_predictions</code> specifed in the Splink settings dictionary, and the blocking rule that must be provided as an argument to <code>estimate_parameters_using_expectation_maximisation</code>?</p> <p>These two kinds of blocking rules can be seen in the following code snippet:</p> DuckDBSparkAthena <pre><code>import splink.duckdb.duckdb_comparison_library as cl\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name and substr(l.surname,1,1) = substr(r.surname,1,1)\",\n        \"l.dob = r.dob\",\n    ],\n    \"comparisons\": [\n        cl.levenshtein_at_thresholds(\"first_name\", 2),\n        cl.exact_match(\"surname\"),\n        cl.exact_match(\"dob\"),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        cl.exact_match(\"email\"),\n    ],\n}\n\n\nlinker = DuckDBLinker(df, settings)\nlinker.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = \"l.first_name = r.first_name and l.surname = r.surname\"\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\nblocking_rule_for_training = \"l.dob = r.dob and l.city = r.city\"\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n</code></pre> <pre><code>import splink.spark.spark_comparison_library as cl\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name and substr(l.surname,1,1) = substr(r.surname,1,1)\",\n        \"l.dob = r.dob\",\n    ],\n    \"comparisons\": [\n        cl.levenshtein_at_thresholds(\"first_name\", 2),\n        cl.exact_match(\"surname\"),\n        cl.exact_match(\"dob\"),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        cl.exact_match(\"email\"),\n    ],\n}\n\n\nlinker = SparkLinker(df, settings)\nlinker.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = \"l.first_name = r.first_name and l.surname = r.surname\"\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\nblocking_rule_for_training = \"l.dob = r.dob and l.city = r.city\"\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n</code></pre> <pre><code>import splink.athena.athena_comparison_library as cl\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name and substr(l.surname,1,1) = substr(r.surname,1,1)\",\n        \"l.dob = r.dob\",\n    ],\n    \"comparisons\": [\n        cl.levenshtein_at_thresholds(\"first_name\", 2),\n        cl.exact_match(\"surname\"),\n        cl.exact_match(\"dob\"),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        cl.exact_match(\"email\"),\n    ],\n}\n\n\nlinker = AthenaLinker(df, settings)\nlinker.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = \"l.first_name = r.first_name and l.surname = r.surname\"\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\nblocking_rule_for_training = \"l.dob = r.dob and l.city = r.city\"\nlinker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n</code></pre> <p>The answer is that they serve different purposes.</p>","tags":["Blocking","Performance","Model Training","M Probability","Expectation Maximisation"]},{"location":"topic_guides/blocking_rules.html#what-is-a-blocking-rule","title":"What is a blocking rule?","text":"<p>Blocking rules are needed because it is usually computationally intractable to compare every record with every other.</p> <p>A blocking rule specifies a constraint on how Splink generates pairwise record comparisons, dramatically reducing the total number of comparisons generated.</p> <p>For example, the blocking rule <code>\"l.first_name = r.first_name and l.surname = r.surname\"</code> will generate pairwise record comparisons amongst pairwise comparisons where first name and surname match.</p>","tags":["Blocking","Performance","Model Training","M Probability","Expectation Maximisation"]},{"location":"topic_guides/blocking_rules.html#the-purpose-of-blocking_rules_to_generate_predictions","title":"The purpose of <code>blocking_rules_to_generate_predictions</code>","text":"<p><code>blocking_rules_to_generate_predictions</code> are used by Splink when the user called <code>linker.predict()</code>.</p> <p>The purpose of these blocking rules is to try and ensure that pairwise record comparisons are generated for all true matches.</p> <p>For example,</p> <pre><code>settings = {\n    \"blocking_rules_to_generate_predictions\" [\n        \"l.first_name = r.first_name and l.surname = r.surname\"\n        ]\n}\n</code></pre> <p>will generate comparisons for all true matches where names match. But it would miss a true match where there was a typo in (say) the first name.</p> <p>In general, it is usually impossible to find a single rule which both:</p> <ul> <li> <p>Reduces the number of comparisons generated to a computatally tractable number</p> </li> <li> <p>Ensures comparisons are generated for all true matches</p> </li> </ul> <p>This is why <code>blocking_rules_to_generate_predictions</code> is a list. Suppose we also block on <code>postcode</code>:</p> <pre><code>settings = {\n    \"blocking_rules_to_generate_predictions\" [\n        \"l.first_name = r.first_name and l.surname = r.surname\",\n        \"l.postcode = r.postcode\"\n        ]\n}\n</code></pre> <p>We will now generate a pairwise comparison for the record where there was a typo in the first name, so long as there isn't also a difference in the postcode.</p> <p>By specifying a variety of <code>blocking_rules_to_generate_predictions</code>, it becomes implausible that a truly matching record would not be captured by at least one of the rules.</p> <p>Note that Splink automatically deduplicates the record comparisons it generates. So, in the example above, the <code>\"l.postcode = r.postcode\"</code> blocking rule generates only records comparisons that were not already captured by the <code>first_name</code> and <code>surname</code> rule.</p>","tags":["Blocking","Performance","Model Training","M Probability","Expectation Maximisation"]},{"location":"topic_guides/blocking_rules.html#the-purpose-of-the-blocking_rule-parameter-on-estimate_parameters_using_expectation_maximisation","title":"The purpose of the <code>blocking_rule</code> parameter on <code>estimate_parameters_using_expectation_maximisation</code>","text":"<p>The purpose of this blocking rule is to reduce the number of pairwise generated to a computationally-tractable number to enable the expectation maximisation algorithm to work.</p> <p>The expectation maximisation algorithm seems to work best when the pairwise record comparisons are a mix of anywhere between around 0.1% and 99.9% true matches. It works less effectively if there are very few examples of either matches or non-matches. It works less efficiently if there is a huge imbalance between the two (e.g. a billion non matches and only a hundred matches).</p> <p>It does not matter if this blocking rule excludes some true matches - it just needs to generate examples of matches and non matches.</p> <p>Since they serve different purposes, the blocking rules most appropriate to use with <code>blocking_rules_to_generate_predictions</code> will often be different to those for <code>estimate_parameters_using_expectation_maximisation</code>, but it is also common for the same rule to be used in both places.</p>","tags":["Blocking","Performance","Model Training","M Probability","Expectation Maximisation"]},{"location":"topic_guides/choosing_comparators.html","title":"Choosing comparators and thresholds","text":"<pre><code>import splink.comparison_helpers as ch\n\nch.comparator_score(\"Richard\", \"iRchard\")\n</code></pre> string1 string2 levenshtein_distance damerau_levenshtein_distance jaro_similarity jaro_winkler_similarity jaccard_similarity 0 Richard iRchard 2 1 0.95 0.95 1.0 <p>Now consider a collection of common variations of the name \"Richard\" - which comparators will consider these variations as sufficiently similar to \"Richard\"?</p> <pre><code>import pandas as pd\n\ndata = {\n    \"string1\": [\n        \"Richard\",\n        \"Richard\",\n        \"Richard\",\n        \"Richard\",\n        \"Richard\",\n        \"Richard\",\n        \"Richard\",\n        \"Richard\",\n        \"Richard\",\n        \"Richard\",\n        \"Richard\",\n        \"Richard\",\n    ],\n    \"string2\": [\n        \"Richard\",\n        \"ichard\",\n        \"Richar\",\n        \"iRchard\",\n        \"Richadr\",\n        \"Rich\",\n        \"Rick\",\n        \"Ricky\",\n        \"Dick\",\n        \"Rico\",\n        \"Rachael\",\n        \"Stephen\",\n    ],\n    \"error_type\": [\n        \"None\",\n        \"Deletion\",\n        \"Deletion\",\n        \"Transposition\",\n        \"Transposition\",\n        \"Shortening\",\n        \"Nickname/Alias\",\n        \"Nickname/Alias\",\n        \"Nickname/Alias\",\n        \"Nickname/Alias\",\n        \"Different Name\",\n        \"Different Name\",\n    ],\n}\ndf = pd.DataFrame(data)\ndf\n</code></pre> string1 string2 error_type 0 Richard Richard None 1 Richard ichard Deletion 2 Richard Richar Deletion 3 Richard iRchard Transposition 4 Richard Richadr Transposition 5 Richard Rich Shortening 6 Richard Rick Nickname/Alias 7 Richard Ricky Nickname/Alias 8 Richard Dick Nickname/Alias 9 Richard Rico Nickname/Alias 10 Richard Rachael Different Name 11 Richard Stephen Different Name <p>The <code>comparator_score_chart</code> function allows you to compare two lists of strings and how similar the elements are according to the available string similarity and distance metrics.</p> <pre><code>ch.comparator_score_chart(data, \"string1\", \"string2\")\n</code></pre> <p></p> <p>Here we can see that all of the metrics are fairly sensitive to transcriptions errors (\"Richadr\", \"Richar\", \"iRchard\"). However, considering nicknames/aliases (\"Rick\", \"Ricky\", \"Rico\"), simple metrics such as Jaccard, Levenshtein and Damerau-Levenshtein tend to be less useful. The same can be said for name shortenings (\"Rich\"), but to a lesser extent than more complex nicknames. However, even more subtle metrics like Jaro and Jaro-Winkler still struggle to identify less obvious nicknames/aliases such as \"Dick\". </p> <p>If you would prefer the underlying dataframe instead of the chart, there is the <code>comparator_score_df</code> function.</p> <pre><code>ch.comparator_score_df(data, \"string1\", \"string2\")\n</code></pre> string1 string2 levenshtein_distance damerau_levenshtein_distance jaro_similarity jaro_winkler_similarity jaccard_similarity 0 Richard Richard 0 0 1.00 1.00 1.00 1 Richard ichard 1 1 0.95 0.95 0.86 2 Richard Richar 1 1 0.95 0.97 0.86 3 Richard iRchard 2 1 0.95 0.95 1.00 4 Richard Richadr 2 1 0.95 0.97 1.00 5 Richard Rich 3 3 0.86 0.91 0.57 6 Richard Rick 4 4 0.73 0.81 0.38 7 Richard Ricky 4 4 0.68 0.68 0.33 8 Richard Dick 5 5 0.60 0.60 0.22 9 Richard Rico 4 4 0.73 0.81 0.38 10 Richard Rachael 3 3 0.71 0.74 0.44 11 Richard Stephen 7 7 0.43 0.43 0.08 <pre><code>ch.comparator_score_threshold_chart(\n    data, \"string1\", \"string2\", distance_threshold=2, similarity_threshold=0.8\n)\n</code></pre> <p></p> <p>To class our variations on \"Richard\" in the same <code>Comparison Level</code>, a good choice of metric could be Jaro-Winkler with a threshold of 0.8. Lowering the threshold any more could increase the chances for false positives. </p> <p>For example, consider a single Jaro-Winkler <code>Comparison Level</code> threshold of 0.7 would lead to \"Rachael\" being considered as providing the same amount evidence for a record matching as \"iRchard\".</p> <p>An alternative way around this is to construct a <code>Comparison</code> with multiple levels, each corresponding to a different threshold of Jaro-Winkler similarity. For example, below we construct a <code>Comparison</code> using the <code>Comparison Library</code> function jaro_winkler_at_thresholds with multiple levels for different match thresholds.:</p> <pre><code>import splink.duckdb.duckdb_comparison_library as cl\n\nfirst_name_comparison = cl.jaro_winkler_at_thresholds(\"first_name\", [0.9, 0.8, 0.7])\n</code></pre> <p>If we print this comparison as a dictionary we can see the underlying SQL.</p> <pre><code>first_name_comparison.as_dict()\n</code></pre> <pre>\n<code>{'output_column_name': 'first_name',\n 'comparison_levels': [{'sql_condition': '\"first_name_l\" IS NULL OR \"first_name_r\" IS NULL',\n   'label_for_charts': 'Null',\n   'is_null_level': True},\n  {'sql_condition': '\"first_name_l\" = \"first_name_r\"',\n   'label_for_charts': 'Exact match'},\n  {'sql_condition': 'jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &gt;= 0.9',\n   'label_for_charts': 'Jaro_winkler_similarity &gt;= 0.9'},\n  {'sql_condition': 'jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &gt;= 0.8',\n   'label_for_charts': 'Jaro_winkler_similarity &gt;= 0.8'},\n  {'sql_condition': 'jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &gt;= 0.7',\n   'label_for_charts': 'Jaro_winkler_similarity &gt;= 0.7'},\n  {'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'}],\n 'comparison_description': 'Exact match vs. First_Name within jaro_winkler_similarity thresholds 0.9, 0.8, 0.7 vs. anything else'}</code>\n</pre> <p>Where:  </p> <ul> <li>Exact Match level will catch perfect matches (\"Richard\").  </li> <li>The 0.9 threshold will catch Shortenings and Typos (\"ichard\", \"Richar\", \"iRchard\", \"Richadr\",  \"Rich\").  </li> <li>The 0.8 threshold will catch simple Nicknames/Aliases (\"Rick\", \"Rico\").  </li> <li>The 0.7 threshold will catch more complex Nicknames/Aliases (\"Ricky\"), but will also include less relevant names (e.g. \"Rachael\"). However, this should not be a concern as the model should give less predictive power (i.e. Match Weight) to this level of evidence.  </li> <li>All other comparisons will end up in the \"Else\" level  </li> </ul> <pre><code>import splink.comparison_helpers\n\nch.phonetic_transform(\"Richard\")\n</code></pre> <pre>\n<code>{'soundex': 'R02063', 'metaphone': 'RXRT', 'dmetaphone': ('RXRT', 'RKRT')}</code>\n</pre> <pre><code>ch.phonetic_transform(\"Steven\")\n</code></pre> <pre>\n<code>{'soundex': 'S30105', 'metaphone': 'STFN', 'dmetaphone': ('STFN', '')}</code>\n</pre> <p>Now consider a collection of common variations of the name \"Stephen\". Which phonetic transforms will consider these as sufficiently similar to \"Stephen\"?</p> <pre><code>data = {\n    \"string1\": [\n        \"Stephen\",\n        \"Stephen\",\n        \"Stephen\",\n        \"Stephen\",\n        \"Stephen\",\n        \"Stephen\",\n        \"Stephen\",\n        \"Stephen\",\n        \"Stephen\",\n        \"Stephen\",\n        \"Stephen\",\n    ],\n    \"string2\": [\n        \"Stephen\",\n        \"Steven\",\n        \"Stephan\",\n        \"Steve\",\n        \"Stehpen\",\n        \"tSephen\",\n        \"Stephne\",\n        \"Stphen\",\n        \"Stepheb\",\n        \"Stephanie\",\n        \"Richard\",\n    ],\n    \"error_type\": [\n        \"None\",\n        \"Spelling Variation\",\n        \"Spelling Variation/Similar Name\",\n        \"Nickname/Alias\",\n        \"Transposition\",\n        \"Transposition\",\n        \"Transposition\",\n        \"Deletion\",\n        \"Replacement\",\n        \"Different Name\",\n        \"Different Name\",\n    ],\n}\n\ndf = pd.DataFrame(data)\ndf\n</code></pre> string1 string2 error_type 0 Stephen Stephen None 1 Stephen Steven Spelling Variation 2 Stephen Stephan Spelling Variation/Similar Name 3 Stephen Steve Nickname/Alias 4 Stephen Stehpen Transposition 5 Stephen tSephen Transposition 6 Stephen Stephne Transposition 7 Stephen Stphen Deletion 8 Stephen Stepheb Replacement 9 Stephen Stephanie Different Name 10 Stephen Richard Different Name <p>The <code>phonetic_match_chart</code> function allows you to compare two lists of strings and how similar the elements are according to the available string similarity and distance metrics.</p> <pre><code>ch.phonetic_match_chart(data, \"string1\", \"string2\")\n</code></pre> <p></p> <p>Here we can see that all of the algorithms recognise simple phonetically similar names (\"Stephen\", \"Steven\"). However, there is some variation when it comes to transposition errors (\"Stehpen\", \"Stephne\") with soundex and metaphone-esque giving different results. There is also different behaviour considering different names (\"Stephanie\").</p> <p>Given there is no clear winner that captures all of the similar names, it is recommended that phonetic matches are used as a single <code>Comparison Level</code> within in a <code>Comparison</code> which also includes string comparators in the other levels. To see an example of this, see the Combining String scores and Phonetic matching section of this topic guide.</p> <p>If you would prefer the underlying dataframe instead of the chart, there is the <code>phonetic_transform_df</code> function.</p> <pre><code>ch.phonetic_transform_df(data, \"string1\", \"string2\")\n</code></pre> string1 string2 soundex metaphone dmetaphone 0 Stephen Stephen [S30105, S30105] [STFN, STFN] [(STFN, ), (STFN, )] 1 Stephen Steven [S30105, S30105] [STFN, STFN] [(STFN, ), (STFN, )] 2 Stephen Stephan [S30105, S30105] [STFN, STFN] [(STFN, ), (STFN, )] 3 Stephen Steve [S30105, S3010] [STFN, STF] [(STFN, ), (STF, )] 4 Stephen Stehpen [S30105, S30105] [STFN, STPN] [(STFN, ), (STPN, )] 5 Stephen tSephen [S30105, t50105] [STFN, TSFN] [(STFN, ), (TSFN, )] 6 Stephen Stephne [S30105, S301050] [STFN, STFN] [(STFN, ), (STFN, )] 7 Stephen Stphen [S30105, S3105] [STFN, STFN] [(STFN, ), (STFN, )] 8 Stephen Stepheb [S30105, S30101] [STFN, STFP] [(STFN, ), (STFP, )] 9 Stephen Stephanie [S30105, S301050] [STFN, STFN] [(STFN, ), (STFN, )] 10 Stephen Richard [S30105, R02063] [STFN, RXRT] [(STFN, ), (RXRT, RKRT)] <pre><code>import splink.duckdb.duckdb_comparison_template_library as ctl\n\nfirst_name_comparison = ctl.name_comparison(\n    \"first_name\",\n    phonetic_col_name=\"first_name_dm\",\n    damerau_levenshtein_thresholds=[],\n    levenshtein_thresholds=[2],\n    jaro_winkler_thresholds=[0.8],\n)\n\nfirst_name_comparison.as_dict()\n</code></pre> <pre>\n<code>{'output_column_name': 'custom_first_name_first_name_dm',\n 'comparison_levels': [{'sql_condition': '\"first_name_l\" IS NULL OR \"first_name_r\" IS NULL',\n   'label_for_charts': 'Null',\n   'is_null_level': True},\n  {'sql_condition': '\"first_name_l\" = \"first_name_r\"',\n   'label_for_charts': 'Exact match first_name'},\n  {'sql_condition': '\"first_name_dm_l\" = \"first_name_dm_r\"',\n   'label_for_charts': 'Exact match first_name_dm'},\n  {'sql_condition': 'levenshtein(\"first_name_l\", \"first_name_r\") &lt;= 2',\n   'label_for_charts': 'Levenshtein &lt;= 2'},\n  {'sql_condition': 'jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &gt;= 0.8',\n   'label_for_charts': 'Jaro_winkler_similarity &gt;= 0.8'},\n  {'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'}],\n 'comparison_description': 'Exact match vs. Names with phonetic exact match vs. First_Name within levenshtein threshold 2 vs. First_Name within jaro_winkler threshold 0.8 vs. anything else'}</code>\n</pre> <p>where <code>first_name_dm</code> refers to a column in the dataset which has been created during the feature engineering step to give the <code>Dmetaphone</code> transform of <code>first_name</code>.</p>"},{"location":"topic_guides/choosing_comparators.html#choosing-string-comparators","title":"Choosing String Comparators","text":"<p>When building a Splink model, one of the most important aspects is defining the <code>Comparisons</code> and <code>Comparison Levels</code> that the model will train on. Each <code>Comparison Level</code> within a <code>Comparison</code> should contain a different amount of evidence that two records are a match, which the model can assign a Match Weight to. When considering different amounts of evidence for the model, it is helpful to explore fuzzy matching as a way of distinguishing strings that are similar, but not the same, as one another.</p> <p>This guide is intended to show how Splink's string comparators perform in different situations in order to help choosing the most appropriate comparator for a given column as well as the most appropriate threshold (or thresholds). For descriptions and examples of each string comparators available in Splink, see the dedicated topic guide.</p>"},{"location":"topic_guides/choosing_comparators.html#what-options-are-available-when-comparing-strings","title":"What options are available when comparing strings?","text":"<p>There are three main classes of string comparator that are considered within Splink:</p> <ol> <li>String Similarity Scores </li> <li>String Distance Scores </li> <li>Phonetic Matching </li> </ol> <p>where  </p> <p>String Similarity Scores are scores between 0 and 1 indicating how similar two strings are. 0 represents two completely dissimilar strings and 1 represents identical strings. E.g. Jaro-Winkler Similarity.  </p> <p>String Distance Scores are integer distances, counting the number of operations to convert one string into another. A lower string distance indicates more similar strings. E.g. Levenshtein Distance.  </p> <p>Phonetic Matching is whether two strings are phonetically similar. The two strings are passed through a phonetic transformation algorithm and then the resulting phonetic codes are matched. E.g. Double Metaphone.</p>"},{"location":"topic_guides/choosing_comparators.html#comparing-string-similarity-and-distance-scores","title":"Comparing String Similarity and Distance Scores","text":"<p>Splink contains a <code>comparison_helpers</code> module which includes some helper functions for comparing the string similarity and distance scores that can help when choosing the most appropriate fuzzy matching function.</p> <p>For comparing two strings the <code>comparator_score</code> function returns the scores for all of the available comparators. E.g. consider a simple inversion \"Richard\" vs \"iRchard\":</p>"},{"location":"topic_guides/choosing_comparators.html#choosing-thresholds","title":"Choosing thresholds","text":"<p>We can add distance and similarity thresholds to the comparators to see what strings would be included in a given comparison level:</p>"},{"location":"topic_guides/choosing_comparators.html#phonetic-matching","title":"Phonetic Matching","text":"<p>There are similar functions available within splink to help users get familiar with phonetic transformations. You can create similar visualisations to string comparators.</p> <p>To see the phonetic transformations for a single string, there is the <code>phonetic_transform</code> function:</p>"},{"location":"topic_guides/choosing_comparators.html#combining-string-scores-and-phonetic-matching","title":"Combining String scores and Phonetic matching","text":"<p>Once you have considered all of the string comparators and phonetic transforms for a given column, you may decide that you would like to have multiple comparison levels including a combination of options.</p> <p>For this you can construct a custom comparison to catch all of the edge cases you want. For example, if you decide that the comparison for <code>first_name</code> in the model should consider:</p> <ol> <li>A <code>Dmetaphone</code> level for phonetic similarity</li> <li>A <code>Levenshtein</code> level with distance of 2 for typos</li> <li>A <code>Jaro-Winkler</code> level with similarity 0.8 for fuzzy matching</li> </ol> <p>The name_comparison function from the <code>Comparison Template Library</code> can be configured as follows:</p>"},{"location":"topic_guides/comparators.html","title":"String Comparators","text":"<p>A Splink model contains a collection of <code>Comparisons</code> and <code>ComparisonLevels</code> organised in a hierarchy.  For example:</p> <pre><code>Data Linking Model\n\u251c\u2500-- Comparison: Date of birth\n\u2502    \u251c\u2500-- ComparisonLevel: Exact match\n\u2502    \u251c\u2500-- ComparisonLevel: Up to one character difference\n\u2502    \u251c\u2500-- ComparisonLevel: Up to three character difference\n\u2502    \u251c\u2500-- ComparisonLevel: All other\n\u251c\u2500-- Comparison: Name\n\u2502    \u251c\u2500-- ComparisonLevel: Exact match on first name and surname\n\u2502    \u251c\u2500-- ComparisonLevel: Exact match on first name\n\u2502    \u251c\u2500-- etc.\n</code></pre> <p>For more detail on how comparisons are constructed, see the dedicated topic guide as well as fuller descriptions of <code>Comparisons</code> and <code>Comparison Levels</code>. </p> <p>Within <code>Comparisons</code> it is useful for different <code>Comparison Levels</code> to allow for different styles (and levels) fuzzy match. Each of these <code>Comparison Levels</code> indicates a different class of match between two records and therefore a different type (and amount) of evidence for or against the two records being a match. Once these <code>Comparison Levels</code> have been defined, the Splink model is trained to estimate the Match Weight to assign to each <code>Comparison Level</code>.</p> <p>There are a number of string comparator functions available in Splink that allow fuzzy matching for strings within <code>Comparisons</code> and <code>Comparison Levels</code>. For each of these fuzzy matching functions, below you will find explanations of how they work, worked examples and recommendations for the types of data they are useful for.</p> <p>For guidance on how to choose the most suitable string comparator, and associated threshold, see the dedicated topic guide.</p>","tags":["API","comparisons","Levenstein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparators.html#levenshtein-distance","title":"Levenshtein Distance","text":"<p>At a glance</p> <p>Useful for: Data entry errors e.g. character miskeys. Splink comparison functions: levenshtein_level() and levenshtein_at_thresholds() Returns: An integer (lower is more similar).</p>","tags":["API","comparisons","Levenstein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparators.html#description","title":"Description","text":"<p>Levenshtein distance, also known as edit distance, is a measure of the difference between two strings. It represents the minimum number of insertions, deletions, or substitutions of characters required to transform one string into the other.</p> <p>Or, as a formula,</p> \\[Levenstein(s_1, s_2) = \\min \\lbrace \\begin{array}{l} \\text{insertion} \\ , \\text{deletion} , \\text{substitution}  \\end{array} \\rbrace \\]","tags":["API","comparisons","Levenstein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparators.html#examples","title":"Examples","text":"<p>\"KITTEN\" vs \"SITTING\"</p> <p>The minimum number of operations to convert \"KITTEN\" into \"SITTING\" are:</p> <ul> <li>Substitute \"K\" in \"KITTEN\" with \"S\" to get \"SITTEN.\"</li> <li>Substitute \"E\" in \"SITTEN\" with \"I\" to get \"SITTIN.\"</li> <li>Insert \"G\" after \"N\" in \"SITTIN\" to get \"SITTING.\"</li> </ul> <p>Therefore, </p> \\[Levenstein(KITTEN, SITTING) = 3\\] <p>\"CAKE\" vs \"ACKE\"</p> <p>The minimum number of operations to convert \"CAKE\" into \"ACKE\" are:</p> <ul> <li>Substitute \"C\" in \"CAKE\" with \"A\" to get \"AAKE.\"</li> <li>substitute the second \"A\" in \"AAKE\" with \"C\" to get \"ACKE.\"</li> </ul> <p>Therefore, </p> \\[Levenstein(CAKE, ACKE) = 2\\]","tags":["API","comparisons","Levenstein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparators.html#sample-code","title":"Sample code","text":"<p>You can test out the Levenshtein distance between two strings through the jellyfish package.</p> <pre><code>import jellyfish\nlevenshtein_distance(\"CAKE\", \"ACKE)\n</code></pre> <p>2</p>","tags":["API","comparisons","Levenstein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparators.html#damerau-levenshtein-distance","title":"Damerau-Levenshtein Distance","text":"<p>At a glance</p> <p>Useful for: Data entry errors e.g. character transpositions and miskeys Splink comparison functions: damerau_levenshtein_level() and damerau_levenshtein_at_thresholds() Returns: An integer (lower is more similar).</p>","tags":["API","comparisons","Levenstein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparators.html#description_1","title":"Description","text":"<p>Damerau-Levenshtein distance is a variation of Levenshtein distance that also includes transposition operations, which are the interchange of adjacent characters. This distance measures the minimum number of operations required to transform one string into another by allowing insertions, deletions, substitutions, and transpositions of characters.</p> <p>Or, as a formula,</p> \\[DamerauLevenstein(s_1, s_2) = \\min \\lbrace \\begin{array}{l} \\text{insertion} \\ , \\text{deletion} , \\text{substitution} , \\text{transposition} \\end{array} \\rbrace \\]","tags":["API","comparisons","Levenstein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparators.html#examples_1","title":"Examples","text":"<p>\"KITTEN\" vs \"SITTING\"</p> <p>The minimum number of operations to convert \"KITTEN\" into \"SITTING\" are:</p> <ul> <li>Substitute \"K\" in \"KITTEN\" with \"S\" to get \"SITTEN\".</li> <li>Substitute \"E\" in \"SITTEN\" with \"I\" to get \"SITTIN\".</li> <li>Insert \"G\" after \"T\" in \"SITTIN\" to get \"SITTING\".</li> </ul> <p>Therefore, </p> \\[DamerauLevenstein(KITTEN, SITTING) = 3\\] <p>\"CAKE\" vs \"ACKE\"</p> <p>The minimum number of operations to convert \"CAKE\" into \"ACKE\" are:</p> <ul> <li>Transpose \"C\" and \"A\" in \"CAKE\" with \"A\" to get \"ACKE.\"</li> </ul> <p>Therefore, </p> \\[DamerauLevenstein(CAKE, ACKE) = 1\\]","tags":["API","comparisons","Levenstein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparators.html#sample-code_1","title":"Sample code","text":"<p>You can test out the Damerau-Levenshtein distance between two strings through the jellyfish package.</p> <pre><code>import jellyfish\ndamerau_levenshtein_distance(\"CAKE\", \"ACKE)\n</code></pre> <p>1</p>","tags":["API","comparisons","Levenstein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparators.html#jaro-similarity","title":"Jaro Similarity","text":"<p>At a glance</p> <p>Useful for:  Strings where all characters are considered equally important, regardless of order e.g. ID numbers Splink comparison functions: jaro_level() and jaro_at_thresholds() Returns:  A score between 0 and 1 (higher is more similar).</p>","tags":["API","comparisons","Levenstein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparators.html#description_2","title":"Description","text":"<p>Jaro similarity is a measure of similarity between two strings. It takes into account the number and order of matching characters, as well as the number of transpositions needed to make the strings identical.</p> <p>Jaro similarity considers:</p> <ul> <li>The number of matching characters (characters in the same position in both strings).</li> <li>The number of transpositions (pairs of characters that are not in the same position in both strings).</li> </ul> <p>Or, as a formula,</p> \\[Jaro = \\frac{1}{3} \\left[ \\frac{m}{|s_1|} + \\frac{m}{|s_2|} + \\frac{m-t}{m} \\right]\\] <p>where \\(s_1\\) and \\(s_2\\) are the two strings being compared, </p> <p>\\(m\\) is the number of common characters (which are considered matching only if they are the same and not farther than \\(\\left\\lfloor \\frac{\\min(|s_1|,|s_2|)}{2} \\right\\rfloor - 1\\) characters apart), </p> <p>and \\(t\\) is the number of transpositions (which is calculated as the number of matching characters that are not in the right order divided by two).</p>","tags":["API","comparisons","Levenstein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparators.html#examples_2","title":"Examples","text":"<p>\"MARTHA\" vs \"MARHTA\":</p> <ul> <li>There are four matching characters: \"M\", \"A\", \"R\", and \"T\".</li> <li>There is one transposition: the fifth character in \"MARTHA\" (\"H\") is not in the same position as the fifth character in \"MARHTA\" (\"T\").</li> <li>We calculate the Jaro similarity using the formula:</li> </ul> \\[Jaro(MARTHA, MARHTA) = \\frac{1}{3} \\left[ \\frac{4}{6} + \\frac{(4)}{6} + \\frac{4-1}{4} \\right] = 0.944\\] <p>\"MARTHA\" vs \"AMRTHA\":</p> <ul> <li>There are four matching characters: \"M\", \"A\", \"R\", and \"T\".</li> <li>There is one transposition: the first character in \"MARTHA\" (\"M\") is not in the same position as the first character in \"AMRTHA\" (\"T\").</li> <li>We calculate the Jaro similarity using the formula:</li> </ul> \\[Jaro(MARTHA, AMRTHA) = \\frac{1}{3} \\left[ \\frac{4}{6} + \\frac{(4)}{6} + \\frac{4-1}{4} \\right] = 0.944\\] <p>Noting that transpositions of strings gives the same Jaro similarity regardless of order.</p>","tags":["API","comparisons","Levenstein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparators.html#sample-code_2","title":"Sample code","text":"<p>You can test out the Jaro similarity between two strings through the jellyfish package.</p> <pre><code>import jellyfish\njellyfish.jaro_similarity(\"MARTHA\", \"AMRTHA)\n</code></pre> <p>0.944</p>","tags":["API","comparisons","Levenstein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparators.html#jaro-winkler-similarity","title":"Jaro-Winkler Similarity","text":"<p>At a glance</p> <p>Useful for: Strings where importance is weighted towards the first 4 characters e.g. Names Splink comparison functions: jaro_winkler_level() and jaro_winkler_at_thresholds() Returns:  A score between 0 and 1 (higher is more similar).</p>","tags":["API","comparisons","Levenstein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparators.html#description_3","title":"Description","text":"<p>Jaro-Winkler similarity is a variation of Jaro similarity that gives extra weight to matching prefixes of the strings. It is particularly useful for names</p> <p>The Jaro-Winkler similarity is calculated as follows:</p> \\[Jaro Winkler = Jaro + p \\cdot l \\cdot (1 - Jaro)\\] <p>where \\(Jaro\\) is the Jaro similarity between the two strings. \\(l\\) is the length of the common prefix between the two strings, up to a maximum of four characters. \\(p\\) is a prefix scale factor, commonly set to 0.1.</p>","tags":["API","comparisons","Levenstein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparators.html#examples_3","title":"Examples","text":"<p>\"MARTHA\" vs \"MARHTA\"</p> <p>The common prefix between the two strings is \"MAR\", which has a length of 3. We calculate the Jaro-Winkler similarity using the formula:</p> \\[Jaro Winkler(MARTHA, MARHTA) = 0.944 + 0.1 \\cdot 3 \\cdot (1 - 0.944) = 0.9612\\] <p>The Jaro-Winkler similarity is slightly higher than the Jaro similarity, due to the matching prefix. </p> <p>\"MARTHA\" vs \"AMRTHA\":</p> <p>There is no common prefix, so the Jaro-Winkler similarity formula gives:</p> \\[Jaro Winkler(MARTHA, AMRTHA) = 0.944 + 0.1 \\cdot 0 \\cdot (1 - 0.944) = 0.944\\] <p>Which is the same as the Jaro score.</p> <p>Note that the Jaro-Winkler similarity should be used with caution, as it may not always provide better results than the standard Jaro similarity, especially when dealing with short strings or strings that have no common prefix.</p>","tags":["API","comparisons","Levenstein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparators.html#sample-code_3","title":"Sample code","text":"<p>You can test out the Jaro similarity between two strings through the jellyfish package.</p> <pre><code>import jellyfish\njellyfish.jaro_winkler_similarity(\"MARTHA\", \"MARHTA)\n</code></pre> <p>0.9612</p>","tags":["API","comparisons","Levenstein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparators.html#jaccard-similarity","title":"Jaccard Similarity","text":"<p>At a glance</p> <p>Useful for: Splink comparison functions: jaccard_level() and jaccard_at_thresholds() Returns:  A score between 0 and 1 (higher is more similar).</p>","tags":["API","comparisons","Levenstein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparators.html#description_4","title":"Description","text":"<p>Jaccard similarity is a measure of similarity between two sets of items, based on the size of their intersection (elements in common) and union (total elements across both sets). For strings, it considers the overlap of characters within each string. Mathematically, it can be represented as:</p> \\[Jaccard=\\frac{|A \\cap B|}{|A \\cup B|}\\] <p>where A and B are two strings, and |A| and |B| represent their cardinalities (i.e., the number of elements in each set).</p> <p>In practice, Jaccard is more useful with strings that can be split up into multiple words as opposed to characters within a single word or string. E.g. tokens within addresses:</p> <p>Address1: {\"flat\", \"2\", \"123\", \"high\", \"street\", \"london\", \"sw1\", \"1ab\"} Address2: {\"2\", \"high\", \"street\", \"london\", \"sw1a\", \"1ab\"}</p> <p>Where </p> <ul> <li>There are 9 unique tokens across the addresses: \"flat\", \"2\", \"123\", \"high\", \"street\", \"london\", \"sw1\", \"sw1a\", \"1ab\"  </li> <li>There are 5 tokens found in both addresses: \"2\", \"high\", \"street\", \"london\", \"1ab\"</li> </ul> <p>We calculate the Jaccard similarity using the formula:</p> \\[Jaccard(Address1, Address2)=\\frac{5}{9}=0.5556\\] <p>However, this functionality is not curently implemented within Splink</p>","tags":["API","comparisons","Levenstein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparators.html#examples_4","title":"Examples","text":"<p>\"DUCK\" vs \"LUCK\"</p> <ul> <li>There are five unique characters across the strings: \"D\", \"U\", \"C\", \"K\", \"L\"</li> <li>Three are found in both strings: \"U\", \"C\", \"K\"</li> </ul> <p>We calculate the Jaccard similarity using the formula:</p> \\[Jaccard(DUCK, LUCK)=\\frac{3}{5}=0.6\\] <p>\"MARTHA\" vs \"MARHTA\"</p> <ul> <li>There are five unique characters across the strings: \"M\", \"A\", \"R\", \"T\", \"H\"</li> <li>Five are found in both strings: \"M\", \"A\", \"R\", \"T\", \"H\"</li> </ul> <p>We calculate the Jaccard similarity using the formula:</p> \\[Jaccard(MARTHA, MARHTA)=\\frac{5}{5}=1\\]","tags":["API","comparisons","Levenstein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparators.html#sample-code_4","title":"Sample code","text":"<p>You can test out the Jaccard similarity between two strings with the function below:</p> <pre><code>def jaccard_similarity(str1, str2):\n        set1 = set(str1)\n        set2 = set(str2)\n        return len(set1 &amp; set2) / len(set1 | set2)\n\njaccard_similarity(\"DUCK\", \"LUCK\")\n</code></pre> <p>0.6</p>","tags":["API","comparisons","Levenstein","Damerau-Levenshtein","Jaro","Jaro-Winkler","Jaccard"]},{"location":"topic_guides/comparison_templates.html","title":"Out-of-the-box comparisons","text":"<pre><code>from splink.duckdb.duckdb_comparison_template_library import date_comparison\n\ndate_of_birth_comparison = date_comparison(\"date_of_birth\")\n</code></pre> <p>Gives a comparison structured as follows:</p> <pre><code>Comparison: Date of birth\n\u251c\u2500-- ComparisonLevel: Exact match\n\u251c\u2500-- ComparisonLevel: Up to one character difference\n\u251c\u2500-- ComparisonLevel: Dates within 1 month of each other\n\u251c\u2500-- ComparisonLevel: Dates within 1 year of each other\n\u251c\u2500-- ComparisonLevel: Dates within 10 years of each other\n\u251c\u2500-- ComparisonLevel: All other\n</code></pre> <p>Or, using <code>human_readable_description</code> to generate automatically from <code>date_of_birth_comparison</code>:</p> <pre><code>print(date_of_birth_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match vs. Date_Of_Birth within damerau-levenshtein threshold 1 vs. Dates within the following thresholds Month(s): 1, Year(s): 1, Year(s): 10 vs. anything else' of \"date_of_birth\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: \"date_of_birth_l\" IS NULL OR \"date_of_birth_r\" IS NULL\n    - 'Exact match' with SQL rule: \"date_of_birth_l\" = \"date_of_birth_r\"\n    - 'Damerau_levenshtein &lt;= 1' with SQL rule: damerau_levenshtein(\"date_of_birth_l\", \"date_of_birth_r\") &lt;= 1\n    - 'Within 1 month' with SQL rule: \n            abs(date_diff('month', \"date_of_birth_l\",\n              \"date_of_birth_r\")) &lt;= 1\n\n    - 'Within 1 year' with SQL rule: \n            abs(date_diff('year', \"date_of_birth_l\",\n              \"date_of_birth_r\")) &lt;= 1\n\n    - 'Within 10 years' with SQL rule: \n            abs(date_diff('year', \"date_of_birth_l\",\n              \"date_of_birth_r\")) &lt;= 10\n\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <p>The date_comparison function also allows the user flexibility to change the parameters and/or fuzzy matching comparison levels.</p> <p>For example:</p> <pre><code>date_of_birth_comparison = date_comparison(\n    \"date_of_birth\",\n    levenshtein_thresholds=[2],\n    damerau_levenshtein_thresholds=[],\n    datediff_thresholds=[7, 1, 1],\n    datediff_metrics=[\"day\", \"month\", \"year\"],\n)\nprint(date_of_birth_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match vs. Date_Of_Birth within levenshtein threshold 2 vs. Dates within the following thresholds Day(s): 7, Month(s): 1, Year(s): 1 vs. anything else' of \"date_of_birth\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: \"date_of_birth_l\" IS NULL OR \"date_of_birth_r\" IS NULL\n    - 'Exact match' with SQL rule: \"date_of_birth_l\" = \"date_of_birth_r\"\n    - 'Levenshtein &lt;= 2' with SQL rule: levenshtein(\"date_of_birth_l\", \"date_of_birth_r\") &lt;= 2\n    - 'Within 7 days' with SQL rule: \n            abs(date_diff('day', \"date_of_birth_l\",\n              \"date_of_birth_r\")) &lt;= 7\n\n    - 'Within 1 month' with SQL rule: \n            abs(date_diff('month', \"date_of_birth_l\",\n              \"date_of_birth_r\")) &lt;= 1\n\n    - 'Within 1 year' with SQL rule: \n            abs(date_diff('year', \"date_of_birth_l\",\n              \"date_of_birth_r\")) &lt;= 1\n\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <p>To see this as a specifications dictionary you can call</p> <pre><code>date_of_birth_comparison.as_dict()\n</code></pre> <pre>\n<code>{'output_column_name': 'date_of_birth',\n 'comparison_levels': [{'sql_condition': '\"date_of_birth_l\" IS NULL OR \"date_of_birth_r\" IS NULL',\n   'label_for_charts': 'Null',\n   'is_null_level': True},\n  {'sql_condition': '\"date_of_birth_l\" = \"date_of_birth_r\"',\n   'label_for_charts': 'Exact match'},\n  {'sql_condition': 'levenshtein(\"date_of_birth_l\", \"date_of_birth_r\") &lt;= 2',\n   'label_for_charts': 'Levenshtein &lt;= 2'},\n  {'sql_condition': '\\n            abs(date_diff(\\'day\\', \"date_of_birth_l\",\\n              \"date_of_birth_r\")) &lt;= 7\\n        ',\n   'label_for_charts': 'Within 7 days'},\n  {'sql_condition': '\\n            abs(date_diff(\\'month\\', \"date_of_birth_l\",\\n              \"date_of_birth_r\")) &lt;= 1\\n        ',\n   'label_for_charts': 'Within 1 month'},\n  {'sql_condition': '\\n            abs(date_diff(\\'year\\', \"date_of_birth_l\",\\n              \"date_of_birth_r\")) &lt;= 1\\n        ',\n   'label_for_charts': 'Within 1 year'},\n  {'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'}],\n 'comparison_description': 'Exact match vs. Date_Of_Birth within levenshtein threshold 2 vs. Dates within the following thresholds Day(s): 7, Month(s): 1, Year(s): 1 vs. anything else'}</code>\n</pre> <p>which can be used as the basis for a more custom comparison, as shown in the Defining and Customising Comparisons topic guide , if desired.</p> <pre><code>from splink.duckdb.duckdb_comparison_template_library import name_comparison\n\nfirst_name_comparison = name_comparison(\"first_name\")\n</code></pre> <p>Gives a comparison structured as follows:</p> <pre><code>Comparison: First Name\n\u251c\u2500-- ComparisonLevel: Exact match\n\u251c\u2500-- ComparisonLevel: Up to one character difference\n\u251c\u2500-- ComparisonLevel: First Names with Jaro-Winkler similarity of 0.9 or greater \n\u251c\u2500-- ComparisonLevel: First Names with Jaro-Winkler similarity of 0.8 or greater\n\u251c\u2500-- ComparisonLevel: All other\n</code></pre> <p>Or, using <code>human_readable_description</code> to generate automatically from <code>first_name_comparison</code>:</p> <pre><code>print(first_name_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match vs. First_Name within levenshtein threshold 1 vs. First_Name within damerau-levenshtein threshold 1 vs. First_Name within jaro_winkler thresholds 0.9, 0.8 vs. anything else' of \"first_name\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: \"first_name_l\" IS NULL OR \"first_name_r\" IS NULL\n    - 'Exact match first_name' with SQL rule: \"first_name_l\" = \"first_name_r\"\n    - 'Damerau_levenshtein &lt;= 1' with SQL rule: damerau_levenshtein(\"first_name_l\", \"first_name_r\") &lt;= 1\n    - 'Jaro_winkler_similarity &gt;= 0.9' with SQL rule: jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &gt;= 0.9\n    - 'Jaro_winkler_similarity &gt;= 0.8' with SQL rule: jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &gt;= 0.8\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <p>The name_comparison function also allowing flexibility to change the parameters and/or fuzzy matching comparison levels.</p> <p>For example:</p> <pre><code>surname_comparison = name_comparison(\n    \"surname\",\n    phonetic_col_name=\"surname_dm\",\n    term_frequency_adjustments_name=True,\n    levenshtein_thresholds=[2],\n    damerau_levenshtein_thresholds=[],\n    jaro_winkler_thresholds=[],\n    jaccard_thresholds=[1],\n)\nprint(surname_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match vs. Names with phonetic exact match vs. Surname within levenshtein threshold 2 vs. Surname within jaccard threshold 1 vs. anything else' of \"surname\" and \"surname_dm\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: \"surname_l\" IS NULL OR \"surname_r\" IS NULL\n    - 'Exact match surname' with SQL rule: \"surname_l\" = \"surname_r\"\n    - 'Exact match surname_dm' with SQL rule: \"surname_dm_l\" = \"surname_dm_r\"\n    - 'Levenshtein &lt;= 2' with SQL rule: levenshtein(\"surname_l\", \"surname_r\") &lt;= 2\n    - 'Jaccard &gt;= 1' with SQL rule: jaccard(\"surname_l\", \"surname_r\") &gt;= 1\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <p>Where <code>surname_dm</code> refers to a column which has used the DoubleMetaphone algorithm on <code>surname</code> to give a phonetic spelling. This helps to catch names which sounds the same but have different spellings (e.g. Stephens vs Stevens). For more on Phonetic Transformations, see the topic guide.</p> <p>To see this as a specifications dictionary you can call</p> <pre><code>surname_comparison.as_dict()\n</code></pre> <pre>\n<code>{'output_column_name': 'custom_surname_surname_dm',\n 'comparison_levels': [{'sql_condition': '\"surname_l\" IS NULL OR \"surname_r\" IS NULL',\n   'label_for_charts': 'Null',\n   'is_null_level': True},\n  {'sql_condition': '\"surname_l\" = \"surname_r\"',\n   'label_for_charts': 'Exact match surname',\n   'tf_adjustment_column': 'surname',\n   'tf_adjustment_weight': 1.0},\n  {'sql_condition': '\"surname_dm_l\" = \"surname_dm_r\"',\n   'label_for_charts': 'Exact match surname_dm'},\n  {'sql_condition': 'levenshtein(\"surname_l\", \"surname_r\") &lt;= 2',\n   'label_for_charts': 'Levenshtein &lt;= 2'},\n  {'sql_condition': 'jaccard(\"surname_l\", \"surname_r\") &gt;= 1',\n   'label_for_charts': 'Jaccard &gt;= 1'},\n  {'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'}],\n 'comparison_description': 'Exact match vs. Names with phonetic exact match vs. Surname within levenshtein threshold 2 vs. Surname within jaccard threshold 1 vs. anything else'}</code>\n</pre> <p>which can be used as the basis for a more custom comparison, as shown in the Defining and Customising Comparisons topic guide , if desired.</p> <pre><code>from splink.duckdb.duckdb_comparison_template_library import postcode_comparison\n\npc_comparison = postcode_comparison(\"postcode\")\n</code></pre> <p>Gives a comparison structured as follows:</p> <pre><code>Comparison: Postcode\n\u251c\u2500-- ComparisonLevel: Exact match\n\u251c\u2500-- ComparisonLevel: Exact match on sector\n\u251c\u2500-- ComparisonLevel: Exact match on district\n\u251c\u2500-- ComparisonLevel: Exact match on area\n\u251c\u2500-- ComparisonLevel: All other\n</code></pre> <p>Or, using <code>human_readable_description</code> to generate automatically from <code>pc_comparison</code>:</p> <pre><code>print(pc_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match on full postcode vs. exact match on sector vs. exact match on district vs. exact match on area vs. all other comparisons' of \"postcode\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: \"postcode_l\" IS NULL OR \"postcode_r\" IS NULL\n    - 'Exact match postcode' with SQL rule: \"postcode_l\" = \"postcode_r\"\n    - 'Exact match Postcode Sector' with SQL rule: \n        regexp_extract(\"postcode_l\", '^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9]')\n     = \n        regexp_extract(\"postcode_r\", '^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9]')\n\n    - 'Exact match Postcode District' with SQL rule: \n        regexp_extract(\"postcode_l\", '^[A-Z]{1,2}[0-9][A-Z0-9]?')\n     = \n        regexp_extract(\"postcode_r\", '^[A-Z]{1,2}[0-9][A-Z0-9]?')\n\n    - 'Exact match Postcode Area' with SQL rule: \n        regexp_extract(\"postcode_l\", '^[A-Z]{1,2}')\n     = \n        regexp_extract(\"postcode_r\", '^[A-Z]{1,2}')\n\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <p>where individual postcode components are extracted under-the-hood using the <code>regex_extract</code> argument.</p> <p>Note that the 'Exact match Postcode District' level also captures matches on subdistricts where they exist in the data.</p> <p>Performing comparisons based on substrings alone doesn't always give the best sense of whether two postcodes are close together since locations which are geographically close can be in different postcode regions e.g. London postcodes starting 'N' vs 'SW'. Given this, the postcode_comparison function also allows the user flexibility to include cll.distance_in_km_level() by supplying <code>lat_col</code>, <code>long_col</code> and <code>km_thresholds</code> arguments. This can help to improve results. (See Feature Enginnering for more details.)</p> <p>Users also have the option to set <code>invalid_postcodes_as_null</code> to <code>True</code>. If <code>True</code>, postcodes that do not adhere to a valid postcode format as determined by <code>valid_postcode_regex</code> will be included in the null level. <code>valid_postcode_regex</code> defaults to <code>\"^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9][A-Z]{2}$\"</code>.</p> <p>For example:</p> <pre><code>pc_comparison = postcode_comparison(\n    \"postcode\",\n    invalid_postcodes_as_null=True,\n    lat_col=\"lat\",\n    long_col=\"long\",\n    km_thresholds=[1, 10, 50]\n)\nprint(pc_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match on full postcode vs. exact match on sector vs. exact match on district vs. exact match on area vs. Postcode within km_distance thresholds 1, 10, 50 vs. all other comparisons' of \"postcode\", \"long\" and \"lat\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: \n        regexp_extract(\"postcode_l\", '^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9][A-Z]{2}$')\n     IS NULL OR \n        regexp_extract(\"postcode_r\", '^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9][A-Z]{2}$')\n     IS NULL OR\n\n        regexp_extract(\"postcode_l\", '^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9][A-Z]{2}$')\n    =='' OR \n        regexp_extract(\"postcode_r\", '^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9][A-Z]{2}$')\n     ==''\n    - 'Exact match postcode' with SQL rule: \"postcode_l\" = \"postcode_r\"\n    - 'Exact match Postcode Sector' with SQL rule: \n        regexp_extract(\"postcode_l\", '^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9]')\n     = \n        regexp_extract(\"postcode_r\", '^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9]')\n\n    - 'Exact match Postcode District' with SQL rule: \n        regexp_extract(\"postcode_l\", '^[A-Z]{1,2}[0-9][A-Z0-9]?')\n     = \n        regexp_extract(\"postcode_r\", '^[A-Z]{1,2}[0-9][A-Z0-9]?')\n\n    - 'Exact match Postcode Area' with SQL rule: \n        regexp_extract(\"postcode_l\", '^[A-Z]{1,2}')\n     = \n        regexp_extract(\"postcode_r\", '^[A-Z]{1,2}')\n\n    - 'Distance less than 1km' with SQL rule: \n\n        cast(\n            acos(\n\n        case\n            when (\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n            * cos( radians(\"long_r\" - \"long_l\") )\n    ) &gt; 1 then 1\n            when (\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n            * cos( radians(\"long_r\" - \"long_l\") )\n    ) &lt; -1 then -1\n            else (\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n            * cos( radians(\"long_r\" - \"long_l\") )\n    )\n        end\n\n            ) * 6371\n            as float\n        )\n     &lt;= 1\n\n    - 'Distance less than 10km' with SQL rule: \n\n        cast(\n            acos(\n\n        case\n            when (\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n            * cos( radians(\"long_r\" - \"long_l\") )\n    ) &gt; 1 then 1\n            when (\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n            * cos( radians(\"long_r\" - \"long_l\") )\n    ) &lt; -1 then -1\n            else (\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n            * cos( radians(\"long_r\" - \"long_l\") )\n    )\n        end\n\n            ) * 6371\n            as float\n        )\n     &lt;= 10\n\n    - 'Distance less than 50km' with SQL rule: \n\n        cast(\n            acos(\n\n        case\n            when (\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n            * cos( radians(\"long_r\" - \"long_l\") )\n    ) &gt; 1 then 1\n            when (\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n            * cos( radians(\"long_r\" - \"long_l\") )\n    ) &lt; -1 then -1\n            else (\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n            * cos( radians(\"long_r\" - \"long_l\") )\n    )\n        end\n\n            ) * 6371\n            as float\n        )\n     &lt;= 50\n\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <p>To see this as a specifications dictionary you can call</p> <pre><code>pc_comparison.as_dict()\n</code></pre> <pre>\n<code>{'output_column_name': 'postcode',\n 'comparison_levels': [{'sql_condition': '\\n        regexp_extract(\"postcode_l\", \\'^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9][A-Z]{2}$\\')\\n     IS NULL OR \\n        regexp_extract(\"postcode_r\", \\'^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9][A-Z]{2}$\\')\\n     IS NULL OR\\n                      \\n        regexp_extract(\"postcode_l\", \\'^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9][A-Z]{2}$\\')\\n    ==\\'\\' OR \\n        regexp_extract(\"postcode_r\", \\'^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9][A-Z]{2}$\\')\\n     ==\\'\\'',\n   'label_for_charts': 'Null',\n   'is_null_level': True},\n  {'sql_condition': '\"postcode_l\" = \"postcode_r\"',\n   'label_for_charts': 'Exact match postcode'},\n  {'sql_condition': '\\n        regexp_extract(\"postcode_l\", \\'^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9]\\')\\n     = \\n        regexp_extract(\"postcode_r\", \\'^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9]\\')\\n    ',\n   'label_for_charts': 'Exact match Postcode Sector'},\n  {'sql_condition': '\\n        regexp_extract(\"postcode_l\", \\'^[A-Z]{1,2}[0-9][A-Z0-9]?\\')\\n     = \\n        regexp_extract(\"postcode_r\", \\'^[A-Z]{1,2}[0-9][A-Z0-9]?\\')\\n    ',\n   'label_for_charts': 'Exact match Postcode District'},\n  {'sql_condition': '\\n        regexp_extract(\"postcode_l\", \\'^[A-Z]{1,2}\\')\\n     = \\n        regexp_extract(\"postcode_r\", \\'^[A-Z]{1,2}\\')\\n    ',\n   'label_for_charts': 'Exact match Postcode Area'},\n  {'sql_condition': '\\n        \\n        cast(\\n            acos(\\n                \\n        case\\n            when (\\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\\n            * cos( radians(\"long_r\" - \"long_l\") )\\n    ) &gt; 1 then 1\\n            when (\\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\\n            * cos( radians(\"long_r\" - \"long_l\") )\\n    ) &lt; -1 then -1\\n            else (\\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\\n            * cos( radians(\"long_r\" - \"long_l\") )\\n    )\\n        end\\n    \\n            ) * 6371\\n            as float\\n        )\\n     &lt;= 1\\n        ',\n   'label_for_charts': 'Distance less than 1km'},\n  {'sql_condition': '\\n        \\n        cast(\\n            acos(\\n                \\n        case\\n            when (\\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\\n            * cos( radians(\"long_r\" - \"long_l\") )\\n    ) &gt; 1 then 1\\n            when (\\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\\n            * cos( radians(\"long_r\" - \"long_l\") )\\n    ) &lt; -1 then -1\\n            else (\\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\\n            * cos( radians(\"long_r\" - \"long_l\") )\\n    )\\n        end\\n    \\n            ) * 6371\\n            as float\\n        )\\n     &lt;= 10\\n        ',\n   'label_for_charts': 'Distance less than 10km'},\n  {'sql_condition': '\\n        \\n        cast(\\n            acos(\\n                \\n        case\\n            when (\\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\\n            * cos( radians(\"long_r\" - \"long_l\") )\\n    ) &gt; 1 then 1\\n            when (\\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\\n            * cos( radians(\"long_r\" - \"long_l\") )\\n    ) &lt; -1 then -1\\n            else (\\n        sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\\n        cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\\n            * cos( radians(\"long_r\" - \"long_l\") )\\n    )\\n        end\\n    \\n            ) * 6371\\n            as float\\n        )\\n     &lt;= 50\\n        ',\n   'label_for_charts': 'Distance less than 50km'},\n  {'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'}],\n 'comparison_description': 'Exact match on full postcode vs. exact match on sector vs. exact match on district vs. exact match on area vs. Postcode within km_distance thresholds 1, 10, 50 vs. all other comparisons'}</code>\n</pre> <p>which can be used as the basis for a more custom comparison, as shown in the Defining and Customising Comparisons topic guide , if desired.</p> <pre><code>from splink.duckdb.duckdb_comparison_template_library import forename_surname_comparison\n\nname_comparison = forename_surname_comparison(\"forename\", \"surname\")\n</code></pre> <p>Gives a comparison structured as follows:</p> <pre><code>Comparison: First Name\n\u251c\u2500-- ComparisonLevel: Exact match Forename and Surname\n\u251c\u2500-- ComparisonLevel: Exact match Forename and Surname swapped\n\u251c\u2500-- ComparisonLevel: Exact match Surname\n\u251c\u2500-- ComparisonLevel: Exact match Forename\n\u251c\u2500-- ComparisonLevel: Surnames with Jaro-Winkler similarity greater than 0.88\n\u251c\u2500-- ComparisonLevel: Forenames with Jaro-Winkler similarity greater than 0.88\n\u251c\u2500-- ComparisonLevel: All other\n</code></pre> <p>Or, using <code>human_readable_description</code> to generate automatically from <code>first_name_comparison</code>:</p> <pre><code>print(name_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match vs. Forename and surname columns reversed vs. Surname exact match vs. Forename exact match vs. Surname within jaro-winkler threshold 0.88 vs. Forename within jaro-winkler threshold 0.88 vs. anything else' of \"forename\" and \"surname\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: (\"forename_l\" IS NULL OR \"forename_r\" IS NULL) AND (\"surname_l\" IS NULL OR \"surname_r\" IS NULL)\n    - 'Full name exact match' with SQL rule: \"forename_l\" = \"forename_r\" AND \"surname_l\" = \"surname_r\"\n    - 'Exact match on reversed cols' with SQL rule: \"forename_l\" = \"surname_r\" and \"forename_r\" = \"surname_l\"\n    - 'Exact match surname' with SQL rule: \"surname_l\" = \"surname_r\"\n    - 'Exact match forename' with SQL rule: \"forename_l\" = \"forename_r\"\n    - 'Jaro_winkler_similarity surname &gt;= 0.88' with SQL rule: jaro_winkler_similarity(\"surname_l\", \"surname_r\") &gt;= 0.88\n    - 'Jaro_winkler_similarity forename &gt;= 0.88' with SQL rule: jaro_winkler_similarity(\"forename_l\", \"forename_r\") &gt;= 0.88\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <p>The forename_surname_comparison function also allowing flexibility to change the parameters and/or fuzzy matching comparison levels.</p> <p>For example:</p> <pre><code>full_name_comparison = forename_surname_comparison(\n    \"forename\",\n    \"surname\",\n    term_frequency_adjustments=True,\n    tf_adjustment_col_forename_and_surname=\"full_name\",\n    phonetic_forename_col_name=\"forename_dm\",\n    phonetic_surname_col_name=\"surname_dm\",\n    levenshtein_thresholds=[2],\n    jaro_winkler_thresholds=[],\n    jaccard_thresholds=[1],\n)\nprint(full_name_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match vs. Phonetic match forename and surname vs. Forename and surname columns reversed vs. Surname exact match vs. Forename exact match vs. Surname within levenshtein threshold 2 vs. Surname within jaccard threshold 1 vs. Forename within levenshtein threshold 2 vs. Forename within jaccard threshold 1 vs. anything else' of \"forename\", \"surname\", \"forename_dm\" and \"surname_dm\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: (\"forename_l\" IS NULL OR \"forename_r\" IS NULL) AND (\"surname_l\" IS NULL OR \"surname_r\" IS NULL)\n    - 'Full name exact match' with SQL rule: \"forename_l\" = \"forename_r\" AND \"surname_l\" = \"surname_r\"\n    - 'Full name phonetic match' with SQL rule: \"forename_dm_l\"=\"forename_dm_r\" AND \"surname_dm_l\" = \"surname_dm_r\"\n    - 'Exact match on reversed cols' with SQL rule: \"forename_l\" = \"surname_r\" and \"forename_r\" = \"surname_l\"\n    - 'Exact match surname' with SQL rule: \"surname_l\" = \"surname_r\"\n    - 'Exact match forename' with SQL rule: \"forename_l\" = \"forename_r\"\n    - 'Levenshtein surname &lt;= 2' with SQL rule: levenshtein(\"surname_l\", \"surname_r\") &lt;= 2\n    - 'Jaccard surname &gt;= 1' with SQL rule: jaccard(\"surname_l\", \"surname_r\") &gt;= 1\n    - 'Levenshtein forename &lt;= 2' with SQL rule: levenshtein(\"forename_l\", \"forename_r\") &lt;= 2\n    - 'Jaccard forename &gt;= 1' with SQL rule: jaccard(\"forename_l\", \"forename_r\") &gt;= 1\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <p>Where:</p> <ul> <li> <p><code>forename_dm</code> and <code>surname_dm</code> refer to columns which have used the DoubleMetaphone algorithm on <code>forename</code> and <code>surname</code> to give a phonetic spelling. This helps to catch names which sounds the same but have different spellings (e.g. Stephens vs Stevens). For more on Phonetic Transformations, see the topic guide. These columns will have to already exist in the dataset, or be created in the feature engineering stage when preparing datasets for linking.</p> </li> <li> <p><code>full_name</code> is a column containing <code>forename</code> and <code>surname</code> so that the model can consider the term-frequency of the full name, as well as <code>forename</code> and <code>surname</code> individually. These columns will have to already exist in the dataset, or be created in the feature engineering stage when preparing datasets for linking.</p> </li> </ul> <p>To see this as a specifications dictionary you can call</p> <pre><code>full_name_comparison.as_dict()\n</code></pre> <pre>\n<code>{'output_column_name': 'custom_forename_surname_forename_dm_surname_dm',\n 'comparison_levels': [{'sql_condition': '(\"forename_l\" IS NULL OR \"forename_r\" IS NULL) AND (\"surname_l\" IS NULL OR \"surname_r\" IS NULL)',\n   'label_for_charts': 'Null',\n   'is_null_level': True},\n  {'sql_condition': '\"forename_l\" = \"forename_r\" AND \"surname_l\" = \"surname_r\"',\n   'label_for_charts': 'Full name exact match',\n   'tf_adjustment_column': 'full_name',\n   'tf_adjustment_weight': 1.0},\n  {'sql_condition': '\"forename_dm_l\"=\"forename_dm_r\" AND \"surname_dm_l\" = \"surname_dm_r\"',\n   'label_for_charts': 'Full name phonetic match',\n   'tf_adjustment_column': 'full_name',\n   'tf_adjustment_weight': 1.0},\n  {'sql_condition': '\"forename_l\" = \"surname_r\" and \"forename_r\" = \"surname_l\"',\n   'label_for_charts': 'Exact match on reversed cols',\n   'tf_adjustment_column': 'full_name',\n   'tf_adjustment_weight': 1.0},\n  {'sql_condition': '\"surname_l\" = \"surname_r\"',\n   'label_for_charts': 'Exact match surname',\n   'tf_adjustment_column': 'surname',\n   'tf_adjustment_weight': 1.0},\n  {'sql_condition': '\"forename_l\" = \"forename_r\"',\n   'label_for_charts': 'Exact match forename',\n   'tf_adjustment_column': 'forename',\n   'tf_adjustment_weight': 1.0},\n  {'sql_condition': 'levenshtein(\"surname_l\", \"surname_r\") &lt;= 2',\n   'label_for_charts': 'Levenshtein surname &lt;= 2'},\n  {'sql_condition': 'jaccard(\"surname_l\", \"surname_r\") &gt;= 1',\n   'label_for_charts': 'Jaccard surname &gt;= 1'},\n  {'sql_condition': 'levenshtein(\"forename_l\", \"forename_r\") &lt;= 2',\n   'label_for_charts': 'Levenshtein forename &lt;= 2'},\n  {'sql_condition': 'jaccard(\"forename_l\", \"forename_r\") &gt;= 1',\n   'label_for_charts': 'Jaccard forename &gt;= 1'},\n  {'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'}],\n 'comparison_description': 'Exact match vs. Phonetic match forename and surname vs. Forename and surname columns reversed vs. Surname exact match vs. Forename exact match vs. Surname within levenshtein threshold 2 vs. Surname within jaccard threshold 1 vs. Forename within levenshtein threshold 2 vs. Forename within jaccard threshold 1 vs. anything else'}</code>\n</pre> <p>Which can be used as the basis for a more custom comparison, as shown in the Defining and Customising Comparisons topic guide , if desired.</p>"},{"location":"topic_guides/comparison_templates.html#out-of-the-box-comparisons-for-specific-data-types","title":"Out-of-the-box Comparisons for specific data types","text":"<p>Similarity is defined differently for types of data (e.g. names, dates of birth, postcodes, addresses, ids). The Comparison Template Library contains functions to generate ready-made comparisons for a variety of data types.</p> <p>Below are examples of how to structure comparisons for a variety of data types.</p>"},{"location":"topic_guides/comparison_templates.html#date-comparisons","title":"Date Comparisons","text":"<p>Date comparisons are generally structured as: </p> <ul> <li>Null level  </li> <li>Exact match  </li> <li>Fuzzy match (using metric of choice)  </li> <li>Interval match (within X days/months/years)  </li> <li>Else level</li> </ul> <p>The comparison_template_library contains the date_comparison function which gives this structure, with some pre-defined parameters, out-of-the-box.</p>"},{"location":"topic_guides/comparison_templates.html#name-comparisons","title":"Name Comparisons","text":"<p>Name comparisons for an individual name column (e.g. forename, surname) are generally structured as: </p> <ul> <li>Null level  </li> <li>Exact match  </li> <li>Fuzzy match (using metric of choice)  </li> <li>Else level</li> </ul> <p>The comparison_template_library contains the name_comparison function which gives this structure, with some pre-defined parameters, out-of-the-box.</p>"},{"location":"topic_guides/comparison_templates.html#postcode-comparisons","title":"Postcode Comparisons","text":"<p>The comparison_template_library contains the postcode_comparison function which provides a sensible approach to comparing postcodes in terms of their constituent components, out-of-the-box. See Feature Engineering for more details.</p>"},{"location":"topic_guides/comparison_templates.html#forename-and-surname-comparisons","title":"Forename and Surname Comparisons","text":"<p>It can be helpful to construct a single comparison for for comparing the forename and surname of two records as:</p> <ol> <li> <p>The Fellegi-Sunter model assumes that comparisons are independent. We know that forename and surname are usually correlated given the regional variation of names etc, so considering then in a single comparison can help to create better models.</p> </li> <li> <p>Term-frequencies of individual forename and surname individually does not necessarily reflect how common the combination of forename and surname are.  For example, in the UK population \u201cMohammed Khan\u201d is a relatively common full name despite neither name occurring frequently. For more information on term-frequencies, see the dedicated topic guide. Addressing forename and surname in a single comparison can allows the model to consider the joint term-frequency as well as individual.</p> </li> <li> <p>It is common for some records to have swapped forename and surname by mistake. Addressing forename and surname in a single comparison can allows the model to consider these name inversions.</p> </li> </ol> <p>Forename and Surname comparisons for an individual name column (e.g. forename, surname) are generally structured as: </p> <ul> <li>Null level  </li> <li>Exact match Forename and Surname</li> <li>Exact match Forename and Surname swapped</li> <li>Exact match Surname</li> <li>Exact match Forename</li> <li>Fuzzy match Surname (using metric of choice)</li> <li>Fuzzy match Forename (using metric of choice)</li> <li>Else level</li> </ul> <p>The comparison_template_library contains the forename_surname_comparison function which gives this structure, with some pre-defined parameters, out-of-the-box.</p>"},{"location":"topic_guides/customising_comparisons.html","title":"Defining and customising comparisons","text":"<pre><code>import splink.duckdb.duckdb_comparison_library as cl\n\nfirst_name_comparison = cl.exact_match(\"first_name\")\nprint(first_name_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match vs. anything else' of \"first_name\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: \"first_name_l\" IS NULL OR \"first_name_r\" IS NULL\n    - 'Exact match' with SQL rule: \"first_name_l\" = \"first_name_r\"\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <p>Note that, under the hood, these functions generate a Python dictionary, which conforms to the underlying <code>.json</code> specification of a model:</p> <pre><code>first_name_comparison.as_dict()\n</code></pre> <pre>\n<code>{'output_column_name': 'first_name',\n 'comparison_levels': [{'sql_condition': '\"first_name_l\" IS NULL OR \"first_name_r\" IS NULL',\n   'label_for_charts': 'Null',\n   'is_null_level': True},\n  {'sql_condition': '\"first_name_l\" = \"first_name_r\"',\n   'label_for_charts': 'Exact match'},\n  {'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'}],\n 'comparison_description': 'Exact match vs. anything else'}</code>\n</pre> <p>We can now generate a second, more complex comparison:</p> <pre><code>import splink.duckdb.duckdb_comparison_library as cl\n\ndob_comparison = cl.levenshtein_at_thresholds(\"dob\", [1, 2])\nprint(dob_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match vs. Dob within levenshtein thresholds 1, 2 vs. anything else' of \"dob\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: \"dob_l\" IS NULL OR \"dob_r\" IS NULL\n    - 'Exact match' with SQL rule: \"dob_l\" = \"dob_r\"\n    - 'Levenshtein &lt;= 1' with SQL rule: levenshtein(\"dob_l\", \"dob_r\") &lt;= 1\n    - 'Levenshtein &lt;= 2' with SQL rule: levenshtein(\"dob_l\", \"dob_r\") &lt;= 2\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <p>These <code>Comparisons</code> can be specified in a data linking model as follows:</p> <pre><code>settings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": [\n        exact_match(\"first_name\"),\n        levenshtein_at_thresholds(\"dob\", [1, 2]),\n    ],\n}\n</code></pre> <pre><code>import splink.duckdb.duckdb_comparison_template_library as ctl\n\ndate_of_birth_comparison = ctl.date_comparison(\"date_of_birth\")\nprint(date_of_birth_comparison.human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'Exact match vs. Date_Of_Birth within damerau-levenshtein threshold 1 vs. Dates within the following thresholds Month(s): 1, Year(s): 1, Year(s): 10 vs. anything else' of \"date_of_birth\".\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: \"date_of_birth_l\" IS NULL OR \"date_of_birth_r\" IS NULL\n    - 'Exact match' with SQL rule: \"date_of_birth_l\" = \"date_of_birth_r\"\n    - 'Damerau_levenshtein &lt;= 1' with SQL rule: damerau_levenshtein(\"date_of_birth_l\", \"date_of_birth_r\") &lt;= 1\n    - 'Within 1 month' with SQL rule: \n            abs(date_diff('month', \"date_of_birth_l\",\n              \"date_of_birth_r\")) &lt;= 1\n\n    - 'Within 1 year' with SQL rule: \n            abs(date_diff('year', \"date_of_birth_l\",\n              \"date_of_birth_r\")) &lt;= 1\n\n    - 'Within 10 years' with SQL rule: \n            abs(date_diff('year', \"date_of_birth_l\",\n              \"date_of_birth_r\")) &lt;= 10\n\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <p>These <code>Comparisons</code> can be specified in a data linking model as follows:</p> <pre><code>settings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": [\n        exact_match(\"first_name\"),\n        date_comparison(\"dob\"),\n    ],\n}\n</code></pre> <p>You can customise a <code>ComparisonTemplate</code> by choosing your own values for the function parameters, but for anything more bespoke you will want to construct a <code>Comparison</code> with <code>ComparisonLevels</code> or provide the spec as a dictionary.</p> <p>For a deep dive on Comparison Templates, see the dedicated topic guide.</p> <pre><code>import splink.spark.spark_comparison_level_library as cll\n\ncomparison_first_name = {\n    \"output_column_name\": \"first_name\",\n    \"comparison_description\": \"First name jaro dmeta\",\n    \"comparison_levels\": [\n        cll.null_level(\"first_name\"),\n        cll.exact_match_level(\"first_name\", term_frequency_adjustments=True),\n        cll.exact_match_level(\"dmeta_first_name\", term_frequency_adjustments=True),\n        cll.else_level(),\n    ],\n}\n\n\nfrom splink.comparison import Comparison\n\nprint(Comparison(comparison_first_name).human_readable_description)\n</code></pre> <pre>\n<code>Comparison 'First name jaro dmeta' of `first_name` and `dmeta_first_name`.\nSimilarity is assessed using the following ComparisonLevels:\n    - 'Null' with SQL rule: `first_name_l` IS NULL OR `first_name_r` IS NULL\n    - 'Exact match' with SQL rule: `first_name_l` = `first_name_r`\n    - 'Exact match' with SQL rule: `dmeta_first_name_l` = `dmeta_first_name_r`\n    - 'All other comparisons' with SQL rule: ELSE\n\n</code>\n</pre> <p>This can now be specified in the settings dictionary as follows:</p> <pre><code>import splink.spark.spark_comparison_library as cl\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": [\n        comparison_first_name,  # The comparison specified above using ComparisonLevels\n        cl.levenshtein_at_thresholds(\n            \"dob\", [1, 2], term_frequency_adjustments=True\n        ),  # From comparison_library\n    ],\n}\n</code></pre> <pre><code>comparison_first_name = {\n    \"output_column_name\": \"first_name\",\n    \"comparison_description\": \"First name jaro dmeta\",\n    \"comparison_levels\": [\n        {\n            \"sql_condition\": \"first_name_l IS NULL OR first_name_r IS NULL\",\n            \"label_for_charts\": \"Null\",\n            \"is_null_level\": True,\n        },\n        {\n            \"sql_condition\": \"first_name_l = first_name_r\",\n            \"label_for_charts\": \"Exact match\",\n            \"tf_adjustment_column\": \"first_name\",\n            \"tf_adjustment_weight\": 1.0,\n            \"tf_minimum_u_value\": 0.001,\n        },\n        {\n            \"sql_condition\": \"dmeta_first_name_l = dmeta_first_name_r\",\n            \"label_for_charts\": \"Exact match\",\n            \"tf_adjustment_column\": \"dmeta_first_name\",\n            \"tf_adjustment_weight\": 1.0,\n        },\n        {\n            \"sql_condition\": \"jaro_winkler_sim(first_name_l, first_name_r) &gt; 0.8\",\n            \"label_for_charts\": \"Exact match\",\n            \"tf_adjustment_column\": \"first_name\",\n            \"tf_adjustment_weight\": 0.5,\n            \"tf_minimum_u_value\": 0.001,\n        },\n        {\"sql_condition\": \"ELSE\", \"label_for_charts\": \"All other comparisons\"},\n    ],\n}\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.first_name = r.first_name\",\n        \"l.surname = r.surname\",\n    ],\n    \"comparisons\": [\n        comparison_first_name,  # The comparison specified above using the dict\n        cl.levenshtein_at_thresholds(\n            \"dob\", [1, 2], term_frequency_adjustments=True\n        ),  # From comparison_library\n    ],\n}\n</code></pre> <pre><code>ctl.name_comparison(\"surname\").as_dict()\n</code></pre> <pre>\n<code>{'output_column_name': 'surname',\n 'comparison_levels': [{'sql_condition': '\"surname_l\" IS NULL OR \"surname_r\" IS NULL',\n   'label_for_charts': 'Null',\n   'is_null_level': True},\n  {'sql_condition': '\"surname_l\" = \"surname_r\"',\n   'label_for_charts': 'Exact match surname'},\n  {'sql_condition': 'damerau_levenshtein(\"surname_l\", \"surname_r\") &lt;= 1',\n   'label_for_charts': 'Damerau_levenshtein &lt;= 1'},\n  {'sql_condition': 'jaro_winkler_similarity(\"surname_l\", \"surname_r\") &gt;= 0.9',\n   'label_for_charts': 'Jaro_winkler_similarity &gt;= 0.9'},\n  {'sql_condition': 'jaro_winkler_similarity(\"surname_l\", \"surname_r\") &gt;= 0.8',\n   'label_for_charts': 'Jaro_winkler_similarity &gt;= 0.8'},\n  {'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'}],\n 'comparison_description': 'Exact match vs. Surname within levenshtein threshold 1 vs. Surname within damerau-levenshtein threshold 1 vs. Surname within jaro_winkler thresholds 0.9, 0.8 vs. anything else'}</code>\n</pre> <pre><code>\n</code></pre>"},{"location":"topic_guides/customising_comparisons.html#defining-and-customising-how-record-comparisons-are-made","title":"Defining and customising how record comparisons are made","text":"<p>A key feature of Splink is the ability to customise how record comparisons are made - that is, how similarity is defined for different data types.  For example, the definition of similarity that is appropriate for a date of birth field is different than for a first name field.</p> <p>By tailoring the definitions of similarity, linking models are more effectively able to distinguish beteween different gradations of similarity, leading to more accurate data linking models.</p> <p>Note that for performance reasons, Splink requires the user to define <code>n</code> discrete levels (gradations) of similarity.</p>"},{"location":"topic_guides/customising_comparisons.html#comparing-information","title":"Comparing information","text":"<p>Comparisons are defined on pairwise record comparisons.  Suppose for instance your data contains <code>first_name</code> and <code>surname</code> and <code>dob</code>:</p> id first_name surname dob 1 john smith 1991-04-11 2 jon smith 1991-04-17 3 john smyth 1991-04-11 <p>To compare these records, at the blocking stage, Splink will set these records against each other in a table of pairwise record comparisons:</p> id_l id_r first_name_l first_name_r surname_l surname_r dob_l dob_r 1 2 john jon smith smith 1991-04-11 1991-04-17 1 3 john john smith smyth 1991-04-11 1991-04-11 2 3 jon john smith smyth 1991-04-17 1991-04-11 <p>When defining comparisons, we are defining rules that operate on each row of this latter table of pairwise comparisons</p>"},{"location":"topic_guides/customising_comparisons.html#comparisons-comparisontemplates-and-comparisonlevels","title":"<code>Comparisons</code>, <code>ComparisonTemplates</code> and <code>ComparisonLevels</code>","text":"<p>A Splink model contains a collection of <code>Comparisons</code> and <code>ComparisonLevels</code> organised in a hierarchy.  An example is as follows:</p> <pre><code>Data Linking Model\n\u251c\u2500-- Comparison: Date of birth\n\u2502    \u251c\u2500-- ComparisonLevel: Exact match\n\u2502    \u251c\u2500-- ComparisonLevel: Up to one character difference\n\u2502    \u251c\u2500-- ComparisonLevel: Up to three character difference\n\u2502    \u251c\u2500-- ComparisonLevel: All other\n\u251c\u2500-- Comparison: Name\n\u2502    \u251c\u2500-- ComparisonLevel: Exact match on first name and surname\n\u2502    \u251c\u2500-- ComparisonLevel: Exact match on first name\n\u2502    \u251c\u2500-- etc.\n</code></pre> <p>A fuller description of <code>Comaprison</code>s and <code>ComparisonLevel</code>s can be found here and here respectively.</p> <p>How are these comparisons specified?</p>"},{"location":"topic_guides/customising_comparisons.html#three-ways-of-specifying-comparisons","title":"Three ways of specifying Comparisons","text":"<p>In Splink, there are three ways of specifying <code>Comparisons</code>:</p> <ul> <li>Using pre-baked comparisons from a backend's <code>ComparisonLibrary</code> or <code>ComparisonTemplateLibrary</code>.   (Most simple/succinct)</li> <li>Composing pre-defined <code>ComparisonLevels</code> from a backend's <code>ComparisonLevelLibrary</code></li> <li>Writing a full spec of a <code>Comparison</code> by hand (most verbose/flexible)</li> </ul>"},{"location":"topic_guides/customising_comparisons.html#method-1-using-the-comparisonlibrary","title":"Method 1: Using the <code>ComparisonLibrary</code>","text":"<p>The <code>ComparisonLibrary</code> for a each backend (<code>DuckDB</code>, <code>Spark</code>, etc.) contains pre-baked similarity functions that cover many common use cases.</p> <p>These functions generate an entire <code>Comparison</code>, composed of several <code>ComparisonLevels</code></p> <p>The following provides an example of using the <code>ComparisonLibrary</code> for DuckDB.</p>"},{"location":"topic_guides/customising_comparisons.html#method-2-using-the-comparisontemplatelibrary","title":"Method 2: Using the <code>ComparisonTemplateLibrary</code>","text":"<p>The <code>ComparisonTemplateLibrary</code> is very similar to <code>ComparisonLibrary</code> in that it contains pre-baked similarity functions for each backend (DuckDB, Spark, etc.) to cover common use cases.</p> <p>The key difference is that <code>ComparisonTemplateLibrary</code> contains functions to generate a 'best practice' <code>Comparison</code> based on the type of data in a given column. This includes: </p> <ul> <li>How comparison is structured (what comparison levels are included, and in what order) </li> <li>Default parameters (e.g. <code>damerau_levenshtein_thresholds = [1]</code>)</li> </ul> <p>The following provides an example of using the ComparisonTemplateLibrary for DuckDB.</p>"},{"location":"topic_guides/customising_comparisons.html#method-3-comparisonlevels","title":"Method 3: <code>ComparisonLevels</code>","text":"<p>The <code>ComparisonLevels</code> API provides a lower-level API that gives the user greater control over their comparisons.</p> <p>For example, the user may wish to specify a comparison that has levels for a match on dmetaphone and jaro_winkler of the <code>first_name</code> field.  </p> <p>The below example assumes the user has derived a column <code>dmeta_first_name</code> which contains the dmetaphone of the first name.</p>"},{"location":"topic_guides/customising_comparisons.html#method-4-providing-the-spec-as-a-dictionary","title":"Method 4: Providing the spec as a dictionary","text":"<p>Ultimately, comparisons are specified as a dictionary which conforms to the formal <code>jsonschema</code> specification of the settings dictionary and here.</p> <p>The library functions described above are convenience functions that provide a shorthand way to produce valid dictionaries.</p> <p>For maximium control over your settings, you can specify your comparisons as a dictionary.</p>"},{"location":"topic_guides/customising_comparisons.html#examples","title":"Examples","text":"<p>Below are some examples of how you can define the same comparison, but through different methods.</p> <p>Note: the following examples show working code for duckdb. In order to change to Where functions exist</p>"},{"location":"topic_guides/customising_comparisons.html#exact-match-comparison-with-term-frequency-adjustments","title":"Exact match Comparison with Term-Frequency Adjustments","text":"Example Comparison LibraryComparison Level LibrarySettings Dictionary <pre><code>import splink.duckdb.duckdb_comparison_library as cl\n\nfirst_name_comparison = cl.exact_match(\"first_name\", term_frequency_adjustments=True)\n</code></pre> <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\n\nfirst_name_comparison = {\n    \"output_column_name\": \"first_name\",\n    \"comparison_description\": \"Exact match vs. anything else\",\n    \"comparison_levels\": [\n        cll.null_level(\"first_name\"),\n        cll.exact_match_level(\"first_name\", term_frequency_adjustments=True),\n        cll.else_level(),\n    ],\n}\n</code></pre> <pre><code>first_name_comparison = {\n    'output_column_name': 'first_name',\n    'comparison_levels': [\n        {\n            'sql_condition': '\"first_name_l\" IS NULL OR \"first_name_r\" IS NULL',\n            'label_for_charts': 'Null',\n            'is_null_level': True\n        },\n        {\n            'sql_condition': '\"first_name_l\" = \"first_name_r\"',\n            'label_for_charts': 'Exact match',\n            'tf_adjustment_column': 'first_name',\n            'tf_adjustment_weight': 1.0\n        },\n        {\n            'sql_condition': 'ELSE', \n            'label_for_charts': 'All other comparisons'\n        }],\n    'comparison_description': 'Exact match vs. anything else'\n}\n</code></pre> <p>Each of which gives</p> <p><pre><code>{\n'output_column_name': 'first_name',\n'comparison_levels': [\n{\n'sql_condition': '\"first_name_l\" IS NULL OR \"first_name_r\" IS NULL',\n'label_for_charts': 'Null',\n'is_null_level': True\n},\n{\n'sql_condition': '\"first_name_l\" = \"first_name_r\"',\n'label_for_charts': 'Exact match',\n'tf_adjustment_column': 'first_name',\n'tf_adjustment_weight': 1.0\n},\n{\n'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'\n}],\n'comparison_description': 'Exact match vs. anything else'\n}\n</code></pre> in your settings dictionary.</p>"},{"location":"topic_guides/customising_comparisons.html#name-comparison","title":"Name Comparison","text":"Example Comparison Template LibraryComparison Level LibrarySettings Dictionary <pre><code>import splink.duckdb.duckdb_comparison_template_library as ctl\n\nsurname_comparison = ctl.name_comparison(\"surname\")\n</code></pre> <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\n\n    surname_comparison = {\n        \"output_column_name\": \"surname\",\n        \"comparison_description\": \"Exact match vs. Surname within jaro_winkler thresholds 0.95, 0.88 vs. anything else\",\n        \"comparison_levels\": [\n            cll.null_level(\"surname\"),\n            cll.exact_match_level(\"surname\"),\n            cll.damerau_levenshtein_level(\"surname\", 1)\n            cll.jaro_winkler_level(\"surname\", 0.9),\n            cll.jaro_winkler_level(\"surname\", 0.8),\n            cll.else_level(),\n        ],\n    }\n</code></pre> <pre><code>surname_comparison = {\n    'output_column_name': 'surname',\n    'comparison_levels': [\n        {\n            'sql_condition': '\"surname_l\" IS NULL OR \"surname_r\" IS NULL',\n            'label_for_charts': 'Null',\n            'is_null_level': True\n        },\n        {\n            'sql_condition': '\"surname_l\" = \"surname_r\"',\n            'label_for_charts': 'Exact match'\n        },\n        {\n            'sql_condition': 'damerau_levenshtein(\"surname_l\", \"surname_r\") &lt;= 1',\n            'label_for_charts': 'Damerau_levenshtein &lt;= 1'\n        },\n        {\n            'sql_condition': 'jaro_winkler_similarity(\"surname_l\", \"surname_r\") &gt;= 0.9',\n            'label_for_charts': 'Jaro_winkler_similarity &gt;= 0.9'\n        },\n        {\n            'sql_condition': 'jaro_winkler_similarity(\"surname_l\", \"surname_r\") &gt;= 0.8',\n            'label_for_charts': 'Jaro_winkler_similarity &gt;= 0.8'\n        },\n        {\n            'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'\n        }],\n         'comparison_description': 'Exact match vs. Surname within levenshtein threshold 1 vs. Surname within damerau-levenshtein threshold 1 vs. Surname within jaro_winkler thresholds 0.9, 0.8 vs. anything else'\n        }\n</code></pre> <p>Each of which gives</p> <p><pre><code>{\n'output_column_name': 'surname',\n'comparison_levels': [\n{\n'sql_condition': '\"surname_l\" IS NULL OR \"surname_r\" IS NULL',\n'label_for_charts': 'Null',\n'is_null_level': True\n},\n{\n'sql_condition': '\"surname_l\" = \"surname_r\"',\n'label_for_charts': 'Exact match'\n},\n{\n'sql_condition': 'damerau_levenshtein(\"surname_l\", \"surname_r\") &lt;= 1',\n'label_for_charts': 'Damerau_levenshtein &lt;= 1'\n},\n{\n'sql_condition': 'jaro_winkler_similarity(\"surname_l\", \"surname_r\") &gt;= 0.9',\n'label_for_charts': 'Jaro_winkler_similarity &gt;= 0.9'\n},\n{\n'sql_condition': 'jaro_winkler_similarity(\"surname_l\", \"surname_r\") &gt;= 0.8',\n'label_for_charts': 'Jaro_winkler_similarity &gt;= 0.8'\n},\n{\n'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'\n}],\n'comparison_description': 'Exact match vs. Surname within levenshtein threshold 1 vs. Surname within damerau-levenshtein threshold 1 vs. Surname within jaro_winkler thresholds 0.9, 0.8 vs. anything else'\n}\n</code></pre> in your settings dictionary.</p>"},{"location":"topic_guides/customising_comparisons.html#levenshtein-comparison","title":"Levenshtein Comparison","text":"Example Comparison LibraryComparison Level LibrarySettings Dictionary <pre><code>import splink.duckdb.duckdb_comparison_library as cl\n\nemail_comparison = cl.levenshtein_at_thresholds(\"email\", [2, 4])\n</code></pre> <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\n\nemail_comparison = {\n    \"output_column_name\": \"email\",\n    \"comparison_description\": \"Exact match vs. Email within levenshtein thresholds 2, 4 vs. anything else\",\n    \"comparison_levels\": [\n        cll.null_level(\"email\"),\n        cll.exact_match_level(\"surname\"),\n        cll.levenshtein_level(\"surname\", 2),\n        cll.levenshtein_level(\"surname\", 4),\n        cll.else_level(),\n    ],\n}\n</code></pre> <pre><code>email_comparison = {\n    'output_column_name': 'email',\n    'comparison_levels': [{'sql_condition': '\"email_l\" IS NULL OR \"email_r\" IS NULL',\n    'label_for_charts': 'Null',\n    'is_null_level': True},\n    {\n        'sql_condition': '\"email_l\" = \"email_r\"',\n        'label_for_charts': 'Exact match'\n    },\n    {\n        'sql_condition': 'levenshtein(\"email_l\", \"email_r\") &lt;= 2',\n        'label_for_charts': 'Levenshtein &lt;= 2'\n    },\n    {\n        'sql_condition': 'levenshtein(\"email_l\", \"email_r\") &lt;= 4',\n        'label_for_charts': 'Levenshtein &lt;= 4'\n    },\n    {\n        'sql_condition': 'ELSE', \n        'label_for_charts': 'All other comparisons'\n    }],\n    'comparison_description': 'Exact match vs. Email within levenshtein thresholds 2, 4 vs. anything else'}\n</code></pre> <p>Each of which gives</p> <pre><code>{\n'output_column_name': 'email',\n'comparison_levels': [\n{\n'sql_condition': '\"email_l\" IS NULL OR \"email_r\" IS NULL',\n'label_for_charts': 'Null',\n'is_null_level': True},\n{\n'sql_condition': '\"email_l\" = \"email_r\"',\n'label_for_charts': 'Exact match'\n},\n{\n'sql_condition': 'levenshtein(\"email_l\", \"email_r\") &lt;= 2',\n'label_for_charts': 'Levenshtein &lt;= 2'\n},\n{\n'sql_condition': 'levenshtein(\"email_l\", \"email_r\") &lt;= 4',\n'label_for_charts': 'Levenshtein &lt;= 4'\n},\n{\n'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'\n}],\n'comparison_description': 'Exact match vs. Email within levenshtein thresholds 2, 4 vs. anything else'\n}\n</code></pre> <p>in your settings dictionary.</p>"},{"location":"topic_guides/customising_comparisons.html#date-comparison","title":"Date Comparison","text":"Example Comparison Template LibraryComparison Level LibrarySettings Dictionary <pre><code>import splink.duckdb.duckdb_comparison_template_library as ctl\n\ndob_comparison = ctl.date_comparison(\"date_of_birth\")\n</code></pre> <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\n\ndob_comparison = {\n            \"output_column_name\": \"date_of_birth\",\n            \"comparison_description\": \"Exact match vs. Date_Of_Birth within levenshtein thresholds 1, 2 vs. Dates within the following thresholds Year(s): 1, Year(s): 10 vs. anything else\",\n            \"comparison_levels\": [\n                cll.null_level(\"date_of_birth\"),\n                cll.exact_match_level(\"date_of_birth\"),\n                cll.levenshtein_level(\"date_of_birth\", 1),\n                cll.levenshtein_level(\"date_of_birth\", 2),\n                cll.datediff_level(\"date_of_birth\",\n                                    date_threshold=1,\n                                    date_metric=\"year\"),\n                cll.datediff_level(\"date_of_birth\",\n                                    date_threshold=10,\n                                    date_metric=\"year\"),\n                cll.else_level(),\n            ],\n        }\n</code></pre> <pre><code>dob_comparison = {\n    'output_column_name': 'date_of_birth',\n    'comparison_levels': [\n        {\n            'sql_condition': '\"date_of_birth_l\" IS NULL OR \"date_of_birth_r\" IS NULL',\n            'label_for_charts': 'Null',\n            'is_null_level': True\n        },\n        {\n            'sql_condition': '\"date_of_birth_l\" = \"date_of_birth_r\"',\n            'label_for_charts': 'Exact match'\n        },\n        {\n            'sql_condition': 'levenshtein(\"date_of_birth_l\", \"date_of_birth_r\") &lt;= 1',\n        'label_for_charts': 'Levenshtein &lt;= 1'\n        },\n        {\n            'sql_condition': 'levenshtein(\"date_of_birth_l\", \"date_of_birth_r\") &lt;= 2',\n        'label_for_charts': 'Levenshtein &lt;= 2'\n        },\n        {\n            'sql_condition': 'abs(date_diff(\\'year\\', \"date_of_birth_l\", \"date_of_birth_r\")) &lt;= 1',\n            'label_for_charts': 'Within 1 year'\n        },\n        {\n            'sql_condition': 'abs(date_diff(\\'year\\', \"date_of_birth_l\", \"date_of_birth_r\")) &lt;= 10',\n            'label_for_charts': 'Within 10 years'},\n        {\n            'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'\n        }],\n    'comparison_description': 'Exact match vs. Date_Of_Birth within levenshtein thresholds 1, 2 vs. Dates within the following thresholds Year(s): 1, Year(s): 10 vs. anything else'\n}\n</code></pre> <p>Each of which gives</p> <p><pre><code>{\n'output_column_name': 'date_of_birth',\n'comparison_levels': [\n{\n'sql_condition': '\"date_of_birth_l\" IS NULL OR \"date_of_birth_r\" IS NULL',\n'label_for_charts': 'Null',\n'is_null_level': True\n},\n{\n'sql_condition': '\"date_of_birth_l\" = \"date_of_birth_r\"',\n'label_for_charts': 'Exact match'\n},\n{\n'sql_condition': 'levenshtein(\"date_of_birth_l\", \"date_of_birth_r\") &lt;= 1',\n'label_for_charts': 'Levenshtein &lt;= 1'\n},\n{\n'sql_condition': 'levenshtein(\"date_of_birth_l\", \"date_of_birth_r\") &lt;= 2',\n'label_for_charts': 'Levenshtein &lt;= 2'\n},\n{\n'sql_condition': 'abs(date_diff(\\'year\\', \"date_of_birth_l\", \"date_of_birth_r\")) &lt;= 1',\n'label_for_charts': 'Within 1 year'\n},\n{\n'sql_condition': 'abs(date_diff(\\'year\\', \"date_of_birth_l\", \"date_of_birth_r\")) &lt;= 10',\n'label_for_charts': 'Within 10 years'},\n{\n'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'\n}],\n'comparison_description': 'Exact match vs. Date_Of_Birth within levenshtein thresholds 1, 2 vs. Dates within the following thresholds Year(s): 1, Year(s): 10 vs. anything else'\n}\n</code></pre> in your settings dictionary.</p>"},{"location":"topic_guides/customising_comparisons.html#km-distance-between-coordinates","title":"KM Distance between coordinates","text":"Example Comparison LibraryComparison Level LibrarySettings Dictionary <pre><code>import splink.duckdb.duckdb_comparison_library as cl\n\ndistance_comparison = cl.distance_in_km_at_thresholds(\"lat_col\",\n                        \"long_col\",\n                        km_thresholds = [0.1, 1, 10]\n                        )\n</code></pre> <pre><code>distance_comparison = {\n                \"output_column_name\": \"custom_lat_col_long_col\",\n                \"comparison_description\": \"Km distance within the following thresholds Km threshold(s): 0.1, Km threshold(s): 1, Km threshold(s): 10 vs. anything else\",\n                \"comparison_levels\": [\n                    cll.or_(\n                        cll.null_level(\"lat_col\"),\n                        cll.null_level(\"long_col\"),\n                        ),\n                    cll.distance_in_km_level(\n                        \"lat_col\",\n                        \"long_col\",\n                        km_threshold=0.1),\n                    cll.distance_in_km_level(\n                        \"lat_col\",\n                        \"long_col\",\n                        km_threshold=1),\n                    cll.distance_in_km_level(\n                        \"lat_col\",\n                        \"long_col\",\n                        km_threshold=10),\n                    cll.else_level(),\n                ],\n            }\n</code></pre> <pre><code>distance_comparison = {\n    'output_column_name': 'custom_lat_col_long_col',\n    'comparison_levels': [\n        {\n            'sql_condition': '(lat_col_l IS NULL OR lat_col_r IS NULL) \\nOR (long_col_l IS NULL OR long_col_r IS NULL)',\n            'label_for_charts': 'Null',\n            'is_null_level': True\n        },\n        {\n            'sql_condition': 'cast(acos(case when (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) &gt; 1 then 1 when (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) &lt; -1 then -1 else (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) end) * 6371 as float)&lt;= 0.1',\n            'label_for_charts': 'Distance less than 0.1km'\n        },\n        {\n            'sql_condition': 'cast(acos(case when (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) &gt; 1 then 1 when (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) &lt; -1 then -1 else (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) end) * 6371 as float)&lt;= 1',\n            'label_for_charts': 'Distance less than 1km'\n        },\n        {\n            'sql_condition': 'cast(acos(case when (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) &gt; 1 then 1 when ( sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) &lt; -1 then -1 else (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) end) * 6371 as float)&lt;= 10',\n            'label_for_charts': 'Distance less than 10km'\n        },\n        {\n            'sql_condition': 'ELSE', \n            'label_for_charts': 'All other comparisons'\n        }],\n    'comparison_description': 'Km distance within the following thresholds Km threshold(s): 0.1, Km threshold(s): 1, Km threshold(s): 10 vs. anything else'\n}\n</code></pre> <p>Each of which gives</p> <pre><code>    {\n'output_column_name': 'custom_lat_col_long_col',\n'comparison_levels': [\n{\n'sql_condition': '(lat_col_l IS NULL OR lat_col_r IS NULL) \\nOR (long_col_l IS NULL OR long_col_r IS NULL)',\n'label_for_charts': 'Null',\n'is_null_level': True\n},\n{\n'sql_condition': 'cast(acos(case when (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) &gt; 1 then 1 when (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) &lt; -1 then -1 else (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) end) * 6371 as float)&lt;= 0.1',\n'label_for_charts': 'Distance less than 0.1km'\n},\n{\n'sql_condition': 'cast(acos(case when (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) &gt; 1 then 1 when (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) &lt; -1 then -1 else (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) end) * 6371 as float)&lt;= 1',\n'label_for_charts': 'Distance less than 1km'\n},\n{\n'sql_condition': 'cast(acos(case when (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) &gt; 1 then 1 when ( sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) &lt; -1 then -1 else (sin( radians(\"lat_col_l\") ) * sin( radians(\"lat_col_r\") ) + cos( radians(\"lat_col_l\") ) * cos( radians(\"lat_col_r\") ) * cos( radians(\"long_col_r\" - \"long_col_l\") )) end) * 6371 as float)&lt;= 10',\n'label_for_charts': 'Distance less than 10km'\n},\n{\n'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'\n}],\n'comparison_description': 'Km distance within the following thresholds Km threshold(s): 0.1, Km threshold(s): 1, Km threshold(s): 10 vs. anything else'\n}\n</code></pre>"},{"location":"topic_guides/drivers_of_performance.html","title":"Run times, performance and linking large data","text":"","tags":["Performance","Blocking"]},{"location":"topic_guides/drivers_of_performance.html#run-times-performance-and-linking-large-data","title":"Run times, performance, and linking large data","text":"<p>This topic guide covers the fundamental drivers of the run time of Splink jobs. It also describes the tools that are built into Splink that help you to understand how long a job is likely to take.</p> <p>In summary, your choice of blocking rules is by far the most important driver of performance.</p> <p>Additional factors which affect performance are:</p> <ul> <li>the complexity of your comparisons, whether you apply term frequency adjustments,</li> <li>whether you choose to set <code>retain_matching_columns</code> and <code>retain_intermediate_calculation_columns</code> to <code>True</code> in your settings,</li> <li>whether you filter out comparisons with a match score below a given threshold (using a <code>threshold_match_probability</code> or <code>threshold_match_weight</code> when you call <code>predict()</code>).</li> </ul>","tags":["Performance","Blocking"]},{"location":"topic_guides/drivers_of_performance.html#blocking-rules","title":"Blocking rules","text":"<p>In most large datasets, it is computationally intractable to compare every row with every other row.</p> <p>The number of comparisons grows with the square of the number of input records, using the formula \\(\\frac{n\\left(n-1\\right)}2\\) . For instance, a million input records implies around 500bn comparisons.</p> <p>In Splink, we use a technique called blocking to dramatically reduce the number of comparisons by comparing only records that adhere to certain rules, such as that the first name and date of birth must be equal . Blocking is described further here.</p> <p>Even after blocking, the number of comparisons generated is usually much higher than the number of input records - often between 10 and 1,000 times higher. As a result, the performance of Splink is influenced most heavily by the number of comparisons generated by the blocking rules, rather than the number of input records.</p> <p>This is the case for both main uses of blocking rules in Splink: estimating parameters using expectation maximisation, and generating predictions. (See here for more information on this distinction).</p>","tags":["Performance","Blocking"]},{"location":"topic_guides/drivers_of_performance.html#how-many-comparisons-will-be-generated-by-a-blocking-rule","title":"How many comparisons will be generated by a blocking rule?","text":"<p>The <code>linker.count_num_comparisons_from_blocking_rule()</code>, documented here will compute the number of comparisons that will be generated from a blocking rule.</p> <p>Users are recommended to use this function before attempting linkage, since some blocking rules may imply trillions of comparisons, resulting in record linkage jobs which run for hours and never complete.</p> <p>In general, we recommend a strategy of starting with strict blocking rules, and gradually loosening them. Sticking to less than 10 million comparisons is a good place to start, before scaling jobs up to 100s of millions (DuckDB on a laptop), or sometimes billions (Athena or Spark).</p>","tags":["Performance","Blocking"]},{"location":"topic_guides/drivers_of_performance.html#examples-of-strict-and-loose-blocking-rules","title":"Examples of strict and loose blocking rules","text":"<p>To give an example of how <code>blocking_rules_to_generate_predictions</code> rules may be incrementally loosened, we may start with the following rule:</p> <p><code>l.first_name = r.first_name and l.surname = r.surname and l.dob = r.dob</code>.</p> <p>This is a very strict rule, and will only create comparisons where full name and date of birth match. This has the advantage of creating few record comparisons, but the disadvantage that the rule will miss true matches where there are typos or nulls in any of these three fields.</p> <p>This blocking rule could be loosened to:</p> <p><code>substr(l.first_name,1,1) = substr(r.first_name,1,1) and l.surname = r.surname and l.year_of_birth = r.year_of_birth</code></p> <p>Now it allows for typos or aliases in the first name, so long as the first letter is the same, and errors in month or day of birth.</p> <p>Depending on the side of your input data, the rule could be further loosened to</p> <p><code>substr(l.first_name,1,1) = substr(r.first_name,1,1) and l.surname = r.surname</code></p> <p>or even</p> <p><code>l.surname = r.surname</code></p> <p>The user could use the <code>linker.count_num_comparisons_from_blocking_rule()</code> function to select which rule is appropriate for their data.</p>","tags":["Performance","Blocking"]},{"location":"topic_guides/feature_engineering.html","title":"Feature Engineering for Data Linkage","text":"<p>During record linkage, the features in a given dataset are used to provide evidence as to whether two records are a match. Like any predictive model, the quality of a Splink model is dictated by the features provided. </p> <p>Below are some examples of features that be created from common columns, and how to create more detailed comparisons with them in a Splink model.</p>","tags":["API","Feature Engineering","Comparisons","Postcode","Phonetic Transformations","Soundex","Metaphone","Double Metaphone"]},{"location":"topic_guides/feature_engineering.html#postcodes","title":"Postcodes","text":"<p>A sensible approach to comparing postcodes is to consider their consituent components. For example, UK postcodes can be broken down into the following substrings:</p> <p> See image source for more details.</p> <p>Splink already includes a pre-built postcode comparison template which does this for you, generating by default a comparison with levels for an exact match on full postcode, sector, district and area in turn. These individual postcode components are engineered under-the-hood using the <code>regex_extract</code> argument (see below and comparison_templates.ipynb for more details).</p> <p>Code examples to use the comparison template:</p> DuckDBSparkAthena <pre><code>import splink.duckdb.duckdb_comparison_template_library as ctl\n\npc_comparison = ctl.postcode_comparison(\"postcode\")\nprint(pc_comparison.human_readable_description)\n</code></pre> <pre><code>import splink.spark.spark_comparison_template_library as ctl\n\npc_comparison = ctl.postcode_comparison(\"postcode\")\nprint(pc_comparison.human_readable_description)\n</code></pre> <pre><code>import splink.athena.athena_comparison_template_library as ctl\n\npc_comparison = ctl.postcode_comparison(\"postcode\")\nprint(pc_comparison.human_readable_description)\n</code></pre> Output <p>Comparison 'Exact match on full postcode vs. exact match on sector vs. exact match on district vs. exact match on area vs. all other comparisons' of \"postcode\".</p> <p>Similarity is assessed using the following ComparisonLevels:</p> <ul> <li>'Null' with SQL rule: \"postcode_l\" IS NULL OR \"postcode_r\" IS NULL</li> <li>'Exact match postcode' with SQL rule: \"postcode_l\" = \"postcode_r\"</li> <li> <p>'Exact match Postcode Sector' with SQL rule:          regexp_extract(\"postcode_l\", '^[A-Z]{1,2}0-9? [0-9]')     =          regexp_extract(\"postcode_r\", '^[A-Z]{1,2}0-9? [0-9]')</p> </li> <li> <p>'Exact match Postcode District' with SQL rule:          regexp_extract(\"postcode_l\", '^[A-Z]{1,2}0-9?')     =          regexp_extract(\"postcode_r\", '^[A-Z]{1,2}0-9?')</p> </li> <li> <p>'Exact match Postcode Area' with SQL rule:          regexp_extract(\"postcode_l\", '^[A-Z]{1,2}')     =          regexp_extract(\"postcode_r\", '^[A-Z]{1,2}')</p> </li> <li> <p>'All other comparisons' with SQL rule: ELSE</p> </li> </ul> <p>Note that the 'Exact match Postcode District' level also captures matches on subdistricts where they exist in the data.</p> <p>However, performing comparisons based on substrings alone doesn't always give the best sense of whether two postcodes are close together since locations which are geographically close can be in different postcode regions e.g. London postcodes starting 'N' vs 'SW'.</p> <p>Fortunately, Splink includes functions cll.distance_in_km_level() and cl.distance_in_km_at_thresholds() to calculate the physical distance between two sets of latitude and longitude coordinates.  Users have the option to include <code>cll.distance_in_km_level()</code> as additional levels in the <code>postcode_comparison()</code> template by supplying <code>lat_col</code>, <code>long_col</code> and <code>km_thresholds</code> arguments. Doing so can help to improve results. Latitude and longitude coordinates can be derived from a postcode column as described in the example below.</p>","tags":["API","Feature Engineering","Comparisons","Postcode","Phonetic Transformations","Soundex","Metaphone","Double Metaphone"]},{"location":"topic_guides/feature_engineering.html#example","title":"Example","text":"<p>There are a number of open source repositories of geospatial data that can be used for linkage, one example is geonames. </p> <p>Below is an example of adding latitude and longitude columns from geonames to create a more nuanced comparison.</p> <p>Read in a dataset with postcodes:</p> <pre><code>import pandas as pd\n\ndf = pd.read_parquet(\"/PATH/TO/DEMO/DATA/historical_figures_with_errors_50k.parquet\")\ndf[\"postcode_fake\"] = df[\"postcode_fake\"].str.upper()\ndf.head()\n</code></pre> Output unique_id cluster full_name first_and_surname first_name surname dob birth_place postcode_fake gender occupation 0 Q2296770-1 Q2296770 thomas clifford, 1st baron clifford of chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon TQ13 8DF male politician 1 Q2296770-2 Q2296770 thomas of chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon TQ13 8DF male politician 2 Q2296770-3 Q2296770 tom 1st baron clifford of chudleigh tom chudleigh tom chudleigh 1630-08-01 devon TQ13 8DF male politician 3 Q2296770-4 Q2296770 thomas 1st chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon TQ13 8HU politician 4 Q2296770-5 Q2296770 thomas clifford, 1st baron chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon TQ13 8DF politician <p>Then read in a list of GB postcodes downloaded from geonames.</p> <pre><code>import pandas as pd\n\nnames = ['country_code', 'postal_code', 'place_name', 'admin_name1', 'admin_code1', 'admin_name2', 'admin_code2', 'admin_name3', 'admin_code3', 'latitude', 'longitude','accuracy']\npostcodes = pd.read_csv(\"GB_full.txt\", sep=\"\\t\", header = None, names=names)\npostcodes.head(5)\n</code></pre> Output country_code postal_code place_name admin_name1 admin_code1 admin_name2 admin_code2 admin_name3 admin_code3 latitude longitude accuracy 0 GB AL3 8QE Slip End England ENG Bedfordshire nan Central Bedfordshire E06000056 51.8479 -0.4474 6 1 GB AL5 3NG Harpenden England ENG Bedfordshire nan Central Bedfordshire E06000056 51.8321 -0.383 6 2 GB AL5 3NS Hyde England ENG Bedfordshire nan Central Bedfordshire E06000056 51.8333 -0.3763 6 3 GB AL5 3QF Hyde England ENG Bedfordshire nan Central Bedfordshire E06000056 51.8342 -0.3851 6 4 GB B10 0AB Birmingham England ENG West Midlands nan Birmingham District (B) E08000025 52.4706 -1.875 6 <p>Now combine the lat-long coordinates from the <code>GB_full.txt</code> lookup.</p> <pre><code>df_with_coordinates = df.merge(postcodes[[\"postal_code\", \"latitude\", \"longitude\"]], \n                                    left_on=\"postcode_fake\", \n                                    right_on=\"postal_code\",\n                                    how=\"left\")\ndf_with_coordinates = df_with_coordinates.rename({'postcode_fake':'postcode'}, axis=1)\n\ndf_with_coordinates.head()\n</code></pre> Output unique_id cluster full_name first_and_surname first_name surname dob birth_place postcode gender occupation postal_code latitude longitude 0 Q2296770-1 Q2296770 thomas clifford, 1st baron clifford of chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon TQ13 8DF male politician TQ13 8DF 50.6927 -3.8139 1 Q2296770-2 Q2296770 thomas of chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon TQ13 8DF male politician TQ13 8DF 50.6927 -3.8139 2 Q2296770-3 Q2296770 tom 1st baron clifford of chudleigh tom chudleigh tom chudleigh 1630-08-01 devon TQ13 8DF male politician TQ13 8DF 50.6927 -3.8139 3 Q2296770-4 Q2296770 thomas 1st chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon TQ13 8HU politician TQ13 8HU 50.6876 -3.8958 4 Q2296770-5 Q2296770 thomas clifford, 1st baron chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon TQ13 8DF politician TQ13 8DF 50.6927 -3.8139 <p>Now that coordinates have been added, a more detailed postcode comparison can be produced using the <code>postcode_comparison</code> template:</p> DuckDBSparkAthena <pre><code>import splink.duckdb.duckdb_comparison_template_library as ctl\n\npc_comparison = ctl.postcode_comparison(\n    \"postcode\",\n    lat_col=\"lat\",\n    long_col=\"long\",\n    km_thresholds=[1, 10, 50]\n)\nprint(pc_comparison.human_readable_description)\n</code></pre> <pre><code>import splink.spark.spark_comparison_template_library as ctl\n\npc_comparison = ctl.postcode_comparison(\n    \"postcode\",\n    lat_col=\"lat\",\n    long_col=\"long\",\n    km_thresholds=[1, 10, 50]\n)\nprint(pc_comparison.human_readable_description)\n</code></pre> <pre><code>import splink.athena.athena_comparison_template_library as ctl\n\npc_comparison = ctl.postcode_comparison(\n    \"postcode\",\n    lat_col=\"lat\",\n    long_col=\"long\",\n    km_thresholds=[1, 10, 50]\n)\nprint(pc_comparison.human_readable_description)\n</code></pre> Output <p>Comparison 'Exact match on full postcode vs. exact match on sector vs. exact match on district vs. exact match on area vs. Postcode within km_distance thresholds 1, 10, 50 vs. all other comparisons' of \"postcode\", \"long\" and \"lat\".</p> <p>Similarity is assessed using the following ComparisonLevels:</p> <pre><code>- 'Null' with SQL rule: \n    regexp_extract(\"postcode_l\", '^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9][A-Z]{2}$')\n    IS NULL OR \n    regexp_extract(\"postcode_r\", '^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9][A-Z]{2}$')\n    IS NULL OR\n    regexp_extract(\"postcode_l\", '^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9][A-Z]{2}$')\n    =='' OR \n    regexp_extract(\"postcode_r\", '^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9][A-Z]{2}$')\n    ==''\n- 'Exact match postcode' with SQL rule: \"postcode_l\" = \"postcode_r\"\n- 'Exact match Postcode Sector' with SQL rule: \n    regexp_extract(\"postcode_l\", '^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9]')\n    = \n    regexp_extract(\"postcode_r\", '^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9]')\n\n- 'Exact match Postcode District' with SQL rule: \n    regexp_extract(\"postcode_l\", '^[A-Z]{1,2}[0-9][A-Z0-9]?')\n    = \n    regexp_extract(\"postcode_r\", '^[A-Z]{1,2}[0-9][A-Z0-9]?')\n\n- 'Exact match Postcode Area' with SQL rule: \n    regexp_extract(\"postcode_l\", '^[A-Z]{1,2}')\n    = \n    regexp_extract(\"postcode_r\", '^[A-Z]{1,2}')\n\n- 'Distance less than 1km' with SQL rule:\n\n    cast(\n        acos(\n\n    case\n        when (\n    sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n    cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n        * cos( radians(\"long_r\" - \"long_l\") )\n) &gt; 1 then 1\n        when (\n    sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n    cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n        * cos( radians(\"long_r\" - \"long_l\") )\n) &lt; -1 then -1\n        else (\n    sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n    cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n        * cos( radians(\"long_r\" - \"long_l\") )\n)\n    end\n\n        ) * 6371\n        as float\n    )\n&lt;= 1\n\n- 'Distance less than 10km' with SQL rule:\n\n    cast(\n        acos(\n\n    case\n        when (\n    sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n    cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n        * cos( radians(\"long_r\" - \"long_l\") )\n) &gt; 1 then 1\n        when (\n    sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n    cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n        * cos( radians(\"long_r\" - \"long_l\") )\n) &lt; -1 then -1\n        else (\n    sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n    cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n        * cos( radians(\"long_r\" - \"long_l\") )\n)\n    end\n\n        ) * 6371\n        as float\n    )\n&lt;= 10\n\n- 'Distance less than 50km' with SQL rule:\n\n    cast(\n        acos(\n\n    case\n        when (\n    sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n    cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n        * cos( radians(\"long_r\" - \"long_l\") )\n) &gt; 1 then 1\n        when (\n    sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n    cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n        * cos( radians(\"long_r\" - \"long_l\") )\n) &lt; -1 then -1\n        else (\n    sin( radians(\"lat_l\") ) * sin( radians(\"lat_r\") ) +\n    cos( radians(\"lat_l\") ) * cos( radians(\"lat_r\") )\n        * cos( radians(\"long_r\" - \"long_l\") )\n)\n    end\n\n        ) * 6371\n        as float\n    )\n&lt;= 50\n\n- 'All other comparisons' with SQL rule: ELSE\n</code></pre> <p>or by using <code>cll.distance_in_km_level()</code> in conjunction with other comparison levels: </p> DuckDBSparkAthena <pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\n\npostcode_comparison = {\n    'output_column_name': 'postcode',\n    'comparison_description': 'Postcode',\n    'comparison_levels': [\n        cll.null_level(\"postcode\"),\n        cll.exact_match_level(\"postcode\"),\n        cll.distance_in_km_level(\"latitude\", \"longitude\", 1),\n        cll.distance_in_km_level(\"latitude\", \"longitude\", 10),\n        cll.distance_in_km_level(\"latitude\", \"longitude\", 50),\n        cll.else_level()\n    ],\n}\n</code></pre> <pre><code>import splink.spark.spark_comparison_level_library as cll\n\npostcode_comparison = {\n    'output_column_name': 'postcode',\n    'comparison_description': 'Postcode',\n    'comparison_levels': [\n        cll.null_level(\"postcode\"),\n        cll.exact_match_level(\"postcode\"),\n        cll.distance_in_km_level(\"latitude\", \"longitude\", 1),\n        cll.distance_in_km_level(\"latitude\", \"longitude\", 10),\n        cll.distance_in_km_level(\"latitude\", \"longitude\", 50),\n        cll.else_level()\n    ],\n}\n</code></pre> <pre><code>import splink.athena.athena_comparison_level_library as cll\n\npostcode_comparison = {\n    'output_column_name': 'postcode',\n    'comparison_description': 'Postcode',\n    'comparison_levels': [\n        cll.null_level(\"postcode\"),\n        cll.exact_match_level(\"postcode\"),\n        cll.distance_in_km_level(\"latitude\", \"longitude\", 1),\n        cll.distance_in_km_level(\"latitude\", \"longitude\", 10),\n        cll.distance_in_km_level(\"latitude\", \"longitude\", 50),\n        cll.else_level()\n    ],\n}\n</code></pre>","tags":["API","Feature Engineering","Comparisons","Postcode","Phonetic Transformations","Soundex","Metaphone","Double Metaphone"]},{"location":"topic_guides/feature_engineering.html#phonetic-transformations","title":"Phonetic transformations","text":"<p>Phonetic transformation algorithms can be used to identify words that sound similar, even if they are spelled differently. These are particularly useful for names and can be used as an additional comparison level within name comparisons.</p> <p>For a more detailed explanation on phonetic transformation algorithms, see the topic guide.</p>","tags":["API","Feature Engineering","Comparisons","Postcode","Phonetic Transformations","Soundex","Metaphone","Double Metaphone"]},{"location":"topic_guides/feature_engineering.html#example_1","title":"Example","text":"<p>There are a number of python packages which support phonetic transformations that can be applied to a pandas dataframe, which can then be loaded into the DuckDBLinker. For example, creating a Double Metaphone column with the phonetics python library:</p> <pre><code>import pandas as pd\nimport phonetics\n\ndf = pd.read_parquet(\"PATH/TO/DATA/fake_1000.parquet\")\n\n# Define a function to apply the dmetaphone phonetic algorithm to each name in the column\ndef dmetaphone_name(name):\n    if name is None:\n        pass\n    else:\n        return phonetics.dmetaphone(name)\n\n# Apply the function to the \"first_name\" and surname columns using the apply method\ndf['first_name_dm'] = df['first_name'].apply(dmetaphone_name)\ndf['surname_dm'] = df['surname'].apply(dmetaphone_name)\n\ndf.head()\n</code></pre> Output unique_id first_name surname dob city email group first_name_dm surname_dm 0 0 Julia 2015-10-29 London hannah88@powers.com 0 ('JL', 'AL') 1 1 Julia Taylor 2015-07-31 London hannah88@powers.com 0 ('JL', 'AL') ('TLR', '') 2 2 Julia Taylor 2016-01-27 London hannah88@powers.com 0 ('JL', 'AL') ('TLR', '') 3 3 Julia Taylor 2015-10-29 hannah88opowersc@m 0 ('JL', 'AL') ('TLR', '') 4 4 oNah Watson 2008-03-23 Bolton matthew78@ballard-mcdonald.net 1 ('AN', '') ('ATSN', 'FTSN') <p>Note: Soundex and Metaphone are also supported in phoneitcs </p> <p>Now that the dmetaphone columns have been added, they can be used within comparisons. For example, using the name_comparison function from the comparison template library.</p> DuckDBSpark <pre><code>import splink.duckdb.duckdb_comparison_template_library as ctl\n\nfirst_name_comparison = ctl.name_comparison(\n                        \"first_name\",\n                        phonetic_col_name = \"first_name_dm\")\nprint(first_name_comparison.human_readable_description)\n</code></pre> <pre><code>import splink.spark.spark_comparison_template_library as ctl\n\nfirst_name_comparison = ctl.name_comparison(\n                        \"first_name\",\n                        phonetic_col_name = \"first_name_dm\")\n</code></pre> Output <p>Comparison 'Exact match vs. Names with phonetic exact match vs. First_Name within jaro_winkler thresholds 0.95, 0.88 vs. anything else' of \"first_name\" and \"first_name_dm\".</p> <p>Similarity is assessed using the following ComparisonLevels:</p> <ul> <li>'Null' with SQL rule: \"first_name_l\" IS NULL OR \"first_name_r\" IS NULL</li> <li>'Exact match first_name' with SQL rule: \"first_name_l\" = \"first_name_r\"</li> <li>'Exact match first_name_dm' with SQL rule: \"first_name_dm_l\" = \"first_name_dm_r\"</li> <li>'Jaro_winkler_similarity &gt;= 0.95' with SQL rule: jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &gt;= 0.95</li> <li>'Jaro_winkler_similarity &gt;= 0.88' with SQL rule: jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &gt;= 0.88</li> <li>'All other comparisons' with SQL rule: ELSE</li> </ul>","tags":["API","Feature Engineering","Comparisons","Postcode","Phonetic Transformations","Soundex","Metaphone","Double Metaphone"]},{"location":"topic_guides/feature_engineering.html#full-name","title":"Full name","text":"<p>When comparing names, it can be helpful to construct a single comparison for for comparing the forename and surname of two records. If a splink model has a single comparison for forename and surname, one of the major benefits is being able to consider the term frequency of the full name, as well as for forename and surname individually.</p> <p>For example, in the UK, \u201cMohammed Khan\u201d is a relatively common full name despite neither \"Mohammed\" or \"Khan\" occurring frequently as forename or surname, respectively. </p> <p>For more on term frequency, see the dedicated topic guide.</p>","tags":["API","Feature Engineering","Comparisons","Postcode","Phonetic Transformations","Soundex","Metaphone","Double Metaphone"]},{"location":"topic_guides/feature_engineering.html#example_2","title":"Example","text":"<p>It is very simple to create a full name column from a <code>forename</code> and a <code>surname</code> in python.</p> <pre><code>import pandas as pd\n\ndf = pd.read_parquet(\"PATH/TO/DATA/fake_1000.parquet\")\n\n# Create a new column \"full_name\" by combining \"first_name\" and \"surname\" columns\ndf['full_name'] = df['first_name'] + ' ' + df['surname']\n\ndf.head()\n</code></pre> Output unique_id first_name surname dob city email group full_name 0 0 Julia 2015-10-29 London hannah88@powers.com 0 nan 1 1 Julia Taylor 2015-07-31 London hannah88@powers.com 0 Julia  Taylor 2 2 Julia Taylor 2016-01-27 London hannah88@powers.com 0 Julia  Taylor 3 3 Julia Taylor 2015-10-29 hannah88opowersc@m 0 Julia  Taylor 4 4 oNah Watson 2008-03-23 Bolton matthew78@ballard-mcdonald.net 1 oNah Watson <p>Now that the <code>full_name</code> column has been added, it can be used within comparisons. For example, using the forenname_surname_comparison function from the comparison template library.</p> DuckDBSpark <pre><code>import splink.duckdb.duckdb_comparison_template_library as ctl\n\nfull_name_comparison = ctl.forename_surname_comparison(\n    \"first_name\",\n    \"surname\",\n    term_frequency_adjustments=True,\n    tf_adjustment_col_forename_and_surname=\"full_name\",\n)\nprint(full_name_comparison.human_readable_description)\n</code></pre> <pre><code>import splink.spark.spark_comparison_template_library as ctl\n\nfull_name_comparison = ctl.forename_surname_comparison(\n    \"first_name\",\n    \"surname\",\n    term_frequency_adjustments=True,\n    tf_adjustment_col_forename_and_surname=\"full_name\",\n)\nprint(full_name_comparison.human_readable_description)\n</code></pre> Output <p>Comparison 'Exact match vs. Forename and surname columns reversed vs. Surname exact match vs. Forename exact match vs. Surname within jaro-winkler threshold 0.88 vs. First_Name within jaro-winkler threshold 0.88 vs. anything else' of \"surname\" and \"first_name\".</p> <p>Similarity is assessed using the following ComparisonLevels:</p> <ul> <li>'Null' with SQL rule: (\"first_name_l\" IS NULL OR \"first_name_r\" IS NULL) AND (\"surname_l\" IS NULL OR \"surname_r\" IS NULL)  </li> <li>'Full name exact match' with SQL rule: \"first_name_l\" = \"first_name_r\" AND \"surname_l\" = \"surname_r\"</li> <li>'Exact match on reversed cols' with SQL rule: \"first_name_l\" = \"surname_r\" and \"first_name_r\" = \"surname_l\"</li> <li>'Exact match surname' with SQL rule: \"surname_l\" = \"surname_r\"</li> <li>'Exact match first_name' with SQL rule: \"first_name_l\" = \"first_name_r\"</li> <li>'Jaro_winkler_similarity surname &gt;= 0.88' with SQL rule: jaro_winkler_similarity(\"surname_l\", \"surname_r\") &gt;= 0.88</li> <li>'Jaro_winkler_similarity first_name &gt;= 0.88' with SQL rule: jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &gt;= 0.88</li> <li>'All other comparisons' with SQL rule: ELSE</li> </ul>","tags":["API","Feature Engineering","Comparisons","Postcode","Phonetic Transformations","Soundex","Metaphone","Double Metaphone"]},{"location":"topic_guides/link_type.html","title":"Link type: Linking, Deduping or Both","text":"<p>Splink allows data to be linked, deduplicated or both.</p> <p>Linking refers to finding links between datasets, whereas deduplication finding links within datasets.</p> <p>Data linking is therefore only meaningful when more than one dataset is provided.</p> <p>This guide shows how to specify the settings dictionary and initialise the linker for the three link types.</p>","tags":["Dedupe","Link","Link and Dedupe"]},{"location":"topic_guides/link_type.html#deduplication","title":"Deduplication","text":"<p>The <code>dedupe_only</code> link type expects the user to provide a single input table, and is specified as follows</p> DuckDBSparkAthenaSQLite <pre><code>from splink.duckdb.duckdb_linker import DuckDBLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    # etc.\n}\nlinker = DuckDBLinker(df, settings)\n</code></pre> <pre><code>from splink.spark.spark_linker import SparkLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    # etc.\n}\nlinker = SparkLinker(df, settings)\n</code></pre> <pre><code>from splink.athena.athena_linker import AthenaLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    # etc.\n}\nlinker = AthenaLinker(df, settings)\n</code></pre> <pre><code>from splink.sqlite.sqlite_linker import SQLiteLinker\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    # etc.\n}\nlinker = SQLiteLinker(df, settings)\n</code></pre>","tags":["Dedupe","Link","Link and Dedupe"]},{"location":"topic_guides/link_type.html#link-only","title":"Link only","text":"<p>The <code>link_only</code> link type expects the user to provide a list of input tables, and is specified as follows:</p> DuckDBSparkAthenaSQLite <pre><code>from splink.duckdb.duckdb_linker import DuckDBLinker\n\nsettings = {\n    \"link_type\": \"link_only\",\n    # etc.\n    }\n\ninput_aliases = [\"table_1\", \"table_2\", \"table_3\"]\nlinker = DuckDBLinker([df_1, df_2, df_3], settings, input_table_aliases=input_aliases)\n</code></pre> <pre><code>from splink.spark.spark_linker import SparkLinker\n\nsettings = {\n    \"link_type\": \"link_only\",\n    # etc.\n    }\n\ninput_aliases = [\"table_1\", \"table_2\", \"table_3\"]\nlinker = SparkLinker([df_1, df_2, df_3], settings, input_table_aliases=input_aliases)\n</code></pre> <pre><code>from splink.athena.athena_linker import AthenaLinker\n\nsettings = {\n    \"link_type\": \"link_only\",\n    # etc.\n    }\n\ninput_aliases = [\"table_1\", \"table_2\", \"table_3\"]\nlinker = AthenaLinker([df_1, df_2, df_3], settings, input_table_aliases=input_aliases)\n</code></pre> <pre><code>from splink.sqlite.sqlite_linker import SQLiteLinker\n\nsettings = {\n    \"link_type\": \"link_only\",\n    # etc.\n    }\n\ninput_aliases = [\"table_1\", \"table_2\", \"table_3\"]\nlinker = SQLiteLinker([df_1, df_2, df_3], settings, input_table_aliases=input_aliases)\n</code></pre> <p>The <code>input_table_aliases</code> argument is optional and are used to label the tables in the outputs. If not provided, defaults will be automatically chosen by Splink.</p>","tags":["Dedupe","Link","Link and Dedupe"]},{"location":"topic_guides/link_type.html#link-and-dedupe","title":"Link and dedupe","text":"<p>The <code>link_and_dedupe</code> link type expects the user to provide a list of input tables, and is specified as follows:</p> DuckDBSparkAthenaSQLite <pre><code>from splink.duckdb.duckdb_linker import DuckDBLinker\n\nsettings = {\n    \"link_type\": \"link_and_dedupe\",\n    # etc.\n    }\n\ninput_aliases = [\"table_1\", \"table_2\", \"table_3\"]\nlinker = DuckDBLinker([df_1, df_2, df_3], settings, input_table_aliases=input_aliases)\n</code></pre> <pre><code>from splink.spark.spark_linker import SparkLinker\n\nsettings = {\n    \"link_type\": \"link_and_dedupe\",\n    # etc.\n    }\n\ninput_aliases = [\"table_1\", \"table_2\", \"table_3\"]\nlinker = SparkLinker([df_1, df_2, df_3], settings, input_table_aliases=input_aliases)\n</code></pre> <pre><code>from splink.athena.athena_linker import AthenaLinker\n\nsettings = {\n    \"link_type\": \"link_and_dedupe\",\n    # etc.\n    }\n\ninput_aliases = [\"table_1\", \"table_2\", \"table_3\"]\nlinker = AthenaLinker([df_1, df_2, df_3], settings, input_table_aliases=input_aliases)\n</code></pre> <pre><code>from splink.sqlite.sqlite_linker import SQLiteLinker\n\nsettings = {\n    \"link_type\": \"link_and_dedupe\",\n    # etc.\n    }\n\ninput_aliases = [\"table_1\", \"table_2\", \"table_3\"]\nlinker = SQLiteLinker([df_1, df_2, df_3], settings, input_table_aliases=input_aliases)\n</code></pre> <p>The <code>input_table_aliases</code> argument is optional and are used to label the tables in the outputs. If not provided, defaults will be automatically chosen by Splink.</p>","tags":["Dedupe","Link","Link and Dedupe"]},{"location":"topic_guides/optimising_spark.html","title":"Optimising Spark performance","text":"","tags":["Performance","Spark","Salting","Parallelism"]},{"location":"topic_guides/optimising_spark.html#optimising-spark-jobs","title":"Optimising Spark jobs","text":"<p>This topic guide describes how to configue Spark to optimise performance - especially large linkage jobs which are slow or are not completing using default settings.</p> <p>It is assumed readers have already read the more general guide to linking big data, and blocking rules are proportionate to the size of the Spark cluster. As a very rough guide, on a small cluster of (say) 8 machines, we recommend starting with blocking rules that generate around 100 million comparisons. Once this is working, loosening the blocking rules to around 1 billion comparisons or more is often achievable.</p>","tags":["Performance","Spark","Salting","Parallelism"]},{"location":"topic_guides/optimising_spark.html#summary","title":"Summary:","text":"<ul> <li>Ensure blocking rules are not generating too many comparisons.</li> <li>We recommend setting the <code>break_lineage_method</code> to <code>\"parquet\"</code>, which is the default</li> <li><code>num_partitions_on_repartition</code> should be set so that each file in the output of <code>predict()</code> is roughly 100MB.</li> <li>Try setting <code>spark.default.parallelism</code> to around 5x the number of CPUs in your cluster</li> </ul> <p>For a cluster with 10 CPUs, that outputs about 8GB of data in parquet format, the following setup may be appropriate:</p> <pre><code>spark.conf.set(\"spark.default.parallelism\", \"50\")\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"50\")\n\nlinker = SparkLinker(\n    person_standardised_nodes,\n    settings,\n    break_lineage_method=\"parquet\",\n    num_partitions_on_repartition=80,\n)\n</code></pre>","tags":["Performance","Spark","Salting","Parallelism"]},{"location":"topic_guides/optimising_spark.html#breaking-lineage","title":"Breaking lineage","text":"<p>Splink uses an iterative algorithm for model training, and more generally, lineage is long and complex. We have found that big jobs fail to complete without further optimisation. This is a well-known problem:</p> <p>\"This long lineage bottleneck is widely known by sophisticated Spark application programmers. A common practice for dealing with long lineage is to have the application program strategically checkpoint RDDs at code locations that truncate much of the lineage for checkpointed data and resume computation immediately from the checkpoint.\"</p> <p>Splink will automatically break lineage in sensible places. We have found in practice that, when running Spark jobs backed by AWS S3, the fastest method of breaking lineage is persisting outputs to <code>.parquet</code> file.</p> <p>You can do this using the <code>break_lineage_method</code> parameter as follows:</p> <pre><code>linker = SparkLinker(\n    person_standardised_nodes,\n    settings,\n    break_lineage_method=\"parquet\"\n)\n</code></pre> <p>Other options are <code>checkpoint</code> and <code>persist</code>. For different Spark setups, particularly if you have fast local storage, you may find these options perform better.</p>","tags":["Performance","Spark","Salting","Parallelism"]},{"location":"topic_guides/optimising_spark.html#spark-parallelism","title":"Spark Parallelism","text":"<p>We suggest setting default parallelism to roughly 5x the number of CPUs in your cluster. This is a very rough rule of thumb, and if you're encountering performance problems you may wish to experiment with different values.</p> <p>One way to set default parallelism is as follows:</p> <pre><code>from pyspark.context import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\n\nconf = SparkConf()\n\nconf.set(\"spark.default.parallelism\", \"50\")\nconf.set(\"spark.sql.shuffle.partitions\", \"50\")\n\nsc = SparkContext.getOrCreate(conf=conf)\nspark = SparkSession(sc)\n</code></pre> <p>In general, increasing parallelism will make Spark 'chunk' your job into a larger amount of smaller tasks. This may solve memory issues. But note there is a tradeoff here: if you increase parallelism too high, Spark may take too much time scheduling large numbers of tasks, and may even run out of memory performing this work. See here. Also note that when blocking, jobs cannot be split into a large number of tasks than the cardinality of the blocking rule. For example, if you block on month of birth, this will be split into 12 tasks, irrespective of the parallelism setting. See here. You can use salting (below) to partially address this limitation.</p>","tags":["Performance","Spark","Salting","Parallelism"]},{"location":"topic_guides/optimising_spark.html#repartition-after-blocking","title":"Repartition after blocking","text":"<p>For some jobs, setting <code>repartition_after_blocking=True</code> when you initialise the <code>SparkLinker</code> may improve performance.</p>","tags":["Performance","Spark","Salting","Parallelism"]},{"location":"topic_guides/optimising_spark.html#salting","title":"Salting","text":"<p>For very large jobs, you may find that salting your blocking keys results in faster run times.</p>","tags":["Performance","Spark","Salting","Parallelism"]},{"location":"topic_guides/optimising_spark.html#general-spark-config","title":"General Spark config","text":"<p>Splink generates large numbers of record comparisons from relatively small input datasets. This is an unusual type of workload, and so default Spark parameters are not always appropriate. Some of the issues encountered are similar to performance issues encountered with cartesian joins - so some of the tips in relevant articles may help.</p>","tags":["Performance","Spark","Salting","Parallelism"]},{"location":"topic_guides/phonetic.html","title":"Phonetic transformation algorithms","text":"<p>Phonetic transformation algorithms can be used to identify words that sound similar, even if they are spelled differently (e.g. \"Stephen\" vs \"Steven\"). These algorithms to give another type of fuzzy match and are often generated in the Feature Engineering step of record linkage.</p> <p>Once generated, phonetic matches can be used within comparisons &amp; comparison levels and blocking rules.</p> <p>E.g. For a comparison including a Double Metaphone phonetic match using the name_comparison function from the comparison template library:</p> DuckDBSpark <pre><code>import splink.duckdb.duckdb_comparison_template_library as ctl\n\nfirst_name_comparison = ctl.name_comparison(\n                        \"first_name\",\n                        phonetic_col_name = \"first_name_dm\")\nprint(first_name_comparison.human_readable_description)\n</code></pre> <pre><code>import splink.spark.spark_comparison_template_library as ctl\n\nfirst_name_comparison = ctl.name_comparison(\n                        \"first_name\",\n                        phonetic_col_name = \"first_name_dm\")\nprint(first_name_comparison.human_readable_description)\n</code></pre> <p>Comparison 'Exact match vs. First_Name within levenshtein threshold 1 vs. First_Name within damerau-levenshtein threshold 1 vs. First_Name within jaro_winkler thresholds 0.9, 0.8 vs. anything else' of \"first_name\".</p> <p>Similarity is assessed using the following ComparisonLevels:</p> <ul> <li>'Null' with SQL rule: \"first_name_l\" IS NULL OR \"first_name_r\" IS NULL</li> <li>'Exact match first_name' with SQL rule: \"first_name_l\" = \"first_name_r\"</li> <li>'Exact match first_name_dm' with SQL rule: \"first_name_dm_l\" = \"first_name_dm_r\"</li> <li>'Damerau_levenshtein &lt;= 1' with SQL rule: damerau_levenshtein(\"first_name_l\", \"first_name_r\") &lt;= 1</li> <li>'Jaro_winkler_similarity &gt;= 0.9' with SQL rule: jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &gt;= 0.9</li> <li>'Jaro_winkler_similarity &gt;= 0.8' with SQL rule: jaro_winkler_similarity(\"first_name_l\", \"first_name_r\") &gt;= 0.8</li> <li>'All other comparisons' with SQL rule: ELSE</li> </ul>","tags":["API","Phonetic Transformations","Comparisons","Blocking","Soundex","Metaphone","Double Metaphone"]},{"location":"topic_guides/phonetic.html#algorithms","title":"Algorithms","text":"<p>Below are some examples of well known phonetic transformation algorithmns.</p>","tags":["API","Phonetic Transformations","Comparisons","Blocking","Soundex","Metaphone","Double Metaphone"]},{"location":"topic_guides/phonetic.html#soundex","title":"Soundex","text":"<p>Soundex is a phonetic algorithm that assigns a code to words based on their sound. The Soundex algorithm works by converting a word into a four-character code, where the first character is the first letter of the word, and the next three characters are numerical codes representing the word's remaining consonants. Vowels and some consonants, such as H, W, and Y, are ignored.</p> Algorithm Steps <p>The Soundex algorithm works by following these steps:</p> <ol> <li> <p>Retain the first letter of the word and remove all other vowels and the letters \"H\", \"W\", and \"Y\".</p> </li> <li> <p>Replace each remaining consonant (excluding the first letter) with a numerical code as follows:</p> <ol> <li>B, F, P, and V are replaced with \"1\"</li> <li>C, G, J, K, Q, S, X, and Z are replaced with \"2\"</li> <li>D and T are replaced with \"3\"</li> <li>L is replaced with \"4\"</li> <li>M and N are replaced with \"5\"</li> <li>R is replaced with \"6\"</li> </ol> </li> <li> <p>Combine the first letter and the numerical codes to form a four-character code. If there are fewer than four characters, pad the code with zeros.</p> </li> </ol> Example <p>You can test out the Soundex transformation between two strings through the phonetics package.</p> <pre><code>import phonetics\nprint(phonetics.soundex(\"Smith\"), phonetics.soundex(\"Smyth\"))\n</code></pre> <p>S5030 S5030</p>","tags":["API","Phonetic Transformations","Comparisons","Blocking","Soundex","Metaphone","Double Metaphone"]},{"location":"topic_guides/phonetic.html#metaphone","title":"Metaphone","text":"<p>Metaphone is an improved version of the Soundex algorithm that was developed to handle a wider range of words and languages. The Metaphone algorithm assigns a code to a word based on its phonetic pronunciation, but it takes into account the sound of the entire word, rather than just its first letter and consonants. The Metaphone algorithm works by applying a set of rules to the word's pronunciation, such as converting the \"TH\" sound to a \"T\" sound, or removing silent letters. The resulting code is a variable-length string of letters that represents the word's pronunciation.</p> Algorithm Steps <p>The Metaphone algorithm works by following these steps:</p> <ol> <li> <p>Convert the word to uppercase and remove all non-alphabetic characters.</p> </li> <li> <p>Apply a set of pronunciation rules to the word, such as:</p> <ol> <li>Convert the letters \"C\" and \"K\" to \"K\"</li> <li>Convert the letters \"PH\" to \"F\"</li> <li>Convert the letters \"W\" and \"H\" to nothing if they are not at the beginning of the word</li> </ol> </li> <li> <p>Apply a set of replacement rules to the resulting word, such as:</p> <ol> <li>Replace the letter \"G\" with \"J\" if it is followed by an \"E\", \"I\", or \"Y\"</li> <li>Replace the letter \"C\" with \"S\" if it is followed by an \"E\", \"I\", or \"Y\"</li> <li>Replace the letter \"X\" with \"KS\"</li> </ol> </li> <li> <p>If the resulting word ends with \"S\", remove it.</p> </li> <li> <p>If the resulting word ends with \"ED\", \"ING\", or \"ES\", remove it.</p> </li> <li> <p>If the resulting word starts with \"KN\", \"GN\", \"PN\", \"AE\", \"WR\", or \"WH\", remove the first letter.</p> </li> <li> <p>If the resulting word starts with a vowel, retain the first letter.</p> </li> <li> <p>Retain the first four characters of the resulting word, or pad it with zeros if it has fewer than four characters.</p> </li> </ol> Example <p>You can test out the Metaphone transformation between two strings through the phonetics package.</p> <pre><code>import phonetics\nprint(phonetics.metaphone(\"Smith\"), phonetics.metaphone(\"Smyth\"))\n</code></pre> <p>SM0 SM0</p>","tags":["API","Phonetic Transformations","Comparisons","Blocking","Soundex","Metaphone","Double Metaphone"]},{"location":"topic_guides/phonetic.html#double-metaphone","title":"Double Metaphone","text":"<p>Double Metaphone is an extension of the Metaphone algorithm that generates two codes for each word, one for the primary pronunciation and one for an alternate pronunciation. The Double Metaphone algorithm is designed to handle a wide range of languages and dialects, and it is more accurate than the original Metaphone algorithm.</p> <p>The Double Metaphone algorithm works by applying a set of rules to the word's pronunciation, similar to the Metaphone algorithm, but it generates two codes for each word. The primary code is the most likely pronunciation of the word, while the alternate code represents a less common pronunciation.</p> Algorithm Steps Standard Double MetaphoneAlternative Double Metaphone <p>The Double Metaphone algorithm works by following these steps:</p> <ol> <li> <p>Convert the word to uppercase and remove all non-alphabetic characters.</p> </li> <li> <p>Apply a set of pronunciation rules to the word, such as:</p> <ol> <li>Convert the letters \"C\" and \"K\" to \"K\"</li> <li>Convert the letters \"PH\" to \"F\"</li> <li>Convert the letters \"W\" and \"H\" to nothing if they are not at the beginning of the word</li> </ol> </li> <li> <p>Apply a set of replacement rules to the resulting word, such as:</p> <ol> <li>Replace the letter \"G\" with \"J\" if it is followed by an \"E\", \"I\", or \"Y\"</li> <li>Replace the letter \"C\" with \"S\" if it is followed by an \"E\", \"I\", or \"Y\"</li> <li>Replace the letter \"X\" with \"KS\"</li> </ol> </li> <li> <p>If the resulting word ends with \"S\", remove it.</p> </li> <li> <p>If the resulting word ends with \"ED\", \"ING\", or \"ES\", remove it.</p> </li> <li> <p>If the resulting word starts with \"KN\", \"GN\", \"PN\", \"AE\", \"WR\", or \"WH\", remove the first letter.</p> </li> <li> <p>If the resulting word starts with \"X\", \"Z\", \"GN\", or \"KN\", retain the first two characters.</p> </li> <li> <p>Apply a second set of rules to the resulting word to generate an alternative code.</p> </li> <li> <p>Return the primary and alternative codes as a tuple.</p> </li> </ol> <p>The Alternative Double Metaphone algorithm takes into account different contexts in the word and is generated by following these steps:</p> <ol> <li> <p>Apply a set of prefix rules, such as:</p> <ol> <li>Convert the letter \"G\" at the beginning of the word to \"K\" if it is followed by \"N\", \"NED\", or \"NER\"</li> <li>Convert the letter \"A\" at the beginning of the word to \"E\" if it is followed by \"SCH\"</li> </ol> </li> <li> <p>Apply a set of suffix rules, such as:</p> <ol> <li>Convert the letters \"E\" and \"I\" at the end of the word to \"Y\"</li> <li>Convert the letters \"S\" and \"Z\" at the end of the word to \"X\"</li> <li>Remove the letter \"D\" at the end of the word if it is preceded by \"N\"</li> </ol> </li> <li> <p>Apply a set of replacement rules, such as:</p> <ol> <li>Replace the letter \"C\" with \"X\" if it is followed by \"IA\" or \"H\"</li> <li>Replace the letter \"T\" with \"X\" if it is followed by \"IA\" or \"CH\"</li> </ol> </li> <li> <p>Retain the first four characters of the resulting word, or pad it with zeros if it has fewer than four characters.</p> </li> <li> <p>If the resulting word starts with \"X\", \"Z\", \"GN\", or \"KN\", retain the first two characters.</p> </li> <li> <p>Return the alternative code.</p> </li> </ol> Example <p>You can test out the Metaphone transformation between two strings through the phonetics package.</p> <pre><code>import phonetics\nprint(phonetics.dmetaphone(\"Smith\"), phonetics.dmetaphone(\"Smyth\"))\n</code></pre> <p>('SM0', 'XMT') ('SM0', 'XMT')</p>","tags":["API","Phonetic Transformations","Comparisons","Blocking","Soundex","Metaphone","Double Metaphone"]},{"location":"topic_guides/querying_splink_results.html","title":"Retrieving and Querying Splink results with the SplinkDataFrame","text":"<p>Splink returns tables of results using a class called a <code>SplinkDataFrame</code>. e.g. when you run <code>df_predict = linker.predict()</code> <code>df_predict</code> is a <code>SplinkDataFrame</code></p> <p>A <code>SplinkDataFrame</code> is a abstraction Splink's results, which under the hood are a table in the underlying database.</p> <p>It's possible to convert a <code>SplinkDataFrame</code> into a Pandas dataframe using <code>splink_df.as_pandas_dataframe()</code>. However, this is not recommended because Splink results can be very large, so converting them into pandas can be slow and result in out of memory errors.</p> <p>You can find out the name of the table in the underlying database using <code>df_predict.physical_name</code>. This enables you to run SQL queries directly against the results.</p> <p>You can execute queries using <code>linker.query_sql</code>.</p> <p>This is the recommended approach as it's typically faster and more memory efficient than using pandas dataframes.</p> <p>The following is an example of this approach, in which we use SQL to find the best match to each input record in a <code>link_type=\"link_only\"</code> job (i.e remove duplicate matches):</p> <pre><code># Linker is a duckdb linker with link_type set to \"link_only\"\nresults = linker.predict(threshold_match_probability=0.75)\n\nsql = f\"\"\"\nwith ranked as\n\n(\nselect *,\nrow_number() OVER (\n    PARTITION BY unique_id_l order by match_weight desc\n    ) as row_number\nfrom {results.physical_name}\n)\n\nselect *\nfrom ranked\nwhere row_number = 1\n\n\n\"\"\"\nlinker.query_sql(sql)\n</code></pre> <p>Note that <code>linker.query_sql</code> will return a pandas dataframe by default, but you can instead return a SplinkDataFrame as follows:</p> <pre><code>linker.query_sql(sql, output_type='splink_df')\n</code></pre>"},{"location":"topic_guides/salting.html","title":"Salting blocking rules","text":"<p>For very large linkages using Apache Spark, Splink supports salting blocking rules.</p> <p>Under certain conditions, this can help Spark better parallelise workflows, leading to shorter run times, and avoiding out of memory errors. It is most likely to help where you have blocking rules that create very large numbers of comparisons (100m records+) and where there is skew in how record comparisons are made (e.g. blocking on full name creates more comparisons amongst 'John Smith's than many other names).</p> <p>Further information about the motivation for salting can be found here.</p> <p>Note that salting is only available for the Spark backend</p>","tags":["Performance","Salting","Spark"]},{"location":"topic_guides/salting.html#how-to-use-salting","title":"How to use salting","text":"<p>To enable salting using the <code>SparkLinker</code>, you provide some of your blocking rules as a dictionary rather than a string.</p> <p>This enables you to choose the number of salts for each blocking rule.</p> <p>Blocking rules provided as plain strings default to no salting (<code>salting_partitions = 1</code>)</p> <p>The following code snippet illustrates:</p> <pre><code>import logging\n\nfrom pyspark.context import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\n\nfrom splink.spark.spark_linker import SparkLinker\nfrom splink.spark.spark_comparison_library import levenshtein_at_thresholds, exact_match\n\nconf = SparkConf()\nconf.set(\"spark.driver.memory\", \"12g\")\nconf.set(\"spark.sql.shuffle.partitions\", \"8\")\nconf.set(\"spark.default.parallelism\", \"8\")\n\nsc = SparkContext.getOrCreate(conf=conf)\nspark = SparkSession(sc)\n\n\nsettings = {\n    \"probability_two_random_records_match\": 0.01,\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.dob = r.dob\",\n        {\"blocking_rule\": \"l.first_name = r.first_name\", \"salting_partitions\": 4},\n    ],\n    \"comparisons\": [\n        levenshtein_at_thresholds(\"first_name\", 2),\n        exact_match(\"surname\"),\n        exact_match(\"dob\"),\n        exact_match(\"city\", term_frequency_adjustments=True),\n        exact_match(\"email\"),\n    ],\n    \"retain_matching_columns\": True,\n    \"retain_intermediate_calculation_columns\": True,\n    \"additional_columns_to_retain\": [\"group\"],\n    \"max_iterations\": 1,\n    \"em_convergence\": 0.01,\n}\n\n\ndf = spark.read.csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\", header=True)\n\n\nlinker = SparkLinker(df, settings)\nlogging.getLogger(\"splink\").setLevel(5)\n\nlinker.load_settings(settings)\nlinker.deterministic_link()\n</code></pre> <p>And we can see that salting has been applied by looking at the SQL generated in the log:</p> <pre><code>SELECT\n  l.unique_id AS unique_id_l,\n  r.unique_id AS unique_id_r,\n  l.first_name AS first_name_l,\n  r.first_name AS first_name_r,\n  l.surname AS surname_l,\n  r.surname AS surname_r,\n  l.dob AS dob_l,\n  r.dob AS dob_r,\n  l.city AS city_l,\n  r.city AS city_r,\n  l.tf_city AS tf_city_l,\n  r.tf_city AS tf_city_r,\n  l.email AS email_l,\n  r.email AS email_r,\n  l.`group` AS `group_l`,\n  r.`group` AS `group_r`,\n  '0' AS match_key\nFROM __splink__df_concat_with_tf AS l\nINNER JOIN __splink__df_concat_with_tf AS r\n  ON l.dob = r.dob\nWHERE\n  l.unique_id &lt; r.unique_id\nUNION ALL\nSELECT\n  l.unique_id AS unique_id_l,\n  r.unique_id AS unique_id_r,\n  l.first_name AS first_name_l,\n  r.first_name AS first_name_r,\n  l.surname AS surname_l,\n  r.surname AS surname_r,\n  l.dob AS dob_l,\n  r.dob AS dob_r,\n  l.city AS city_l,\n  r.city AS city_r,\n  l.tf_city AS tf_city_l,\n  r.tf_city AS tf_city_r,\n  l.email AS email_l,\n  r.email AS email_r,\n  l.`group` AS `group_l`,\n  r.`group` AS `group_r`,\n  '1' AS match_key\nFROM __splink__df_concat_with_tf AS l\nINNER JOIN __splink__df_concat_with_tf AS r\n  ON l.first_name = r.first_name\n  AND CEIL(l.__splink_salt * 4) = 1\n  AND NOT (\n    COALESCE((\n        l.dob = r.dob\n    ), FALSE)\n  )\nWHERE\n  l.unique_id &lt; r.unique_id\nUNION ALL\nSELECT\n  l.unique_id AS unique_id_l,\n  r.unique_id AS unique_id_r,\n  l.first_name AS first_name_l,\n  r.first_name AS first_name_r,\n  l.surname AS surname_l,\n  r.surname AS surname_r,\n  l.dob AS dob_l,\n  r.dob AS dob_r,\n  l.city AS city_l,\n  r.city AS city_r,\n  l.tf_city AS tf_city_l,\n  r.tf_city AS tf_city_r,\n  l.email AS email_l,\n  r.email AS email_r,\n  l.`group` AS `group_l`,\n  r.`group` AS `group_r`,\n  '1' AS match_key\nFROM __splink__df_concat_with_tf AS l\nINNER JOIN __splink__df_concat_with_tf AS r\n  ON l.first_name = r.first_name\n  AND CEIL(l.__splink_salt * 4) = 2\n  AND NOT (\n    COALESCE((\n        l.dob = r.dob\n    ), FALSE)\n  )\nWHERE\n  l.unique_id &lt; r.unique_id\nUNION ALL\nSELECT\n  l.unique_id AS unique_id_l,\n  r.unique_id AS unique_id_r,\n  l.first_name AS first_name_l,\n  r.first_name AS first_name_r,\n  l.surname AS surname_l,\n  r.surname AS surname_r,\n  l.dob AS dob_l,\n  r.dob AS dob_r,\n  l.city AS city_l,\n  r.city AS city_r,\n  l.tf_city AS tf_city_l,\n  r.tf_city AS tf_city_r,\n  l.email AS email_l,\n  r.email AS email_r,\n  l.`group` AS `group_l`,\n  r.`group` AS `group_r`,\n  '1' AS match_key\nFROM __splink__df_concat_with_tf AS l\nINNER JOIN __splink__df_concat_with_tf AS r\n  ON l.first_name = r.first_name\n  AND CEIL(l.__splink_salt * 4) = 3\n  AND NOT (\n    COALESCE((\n        l.dob = r.dob\n    ), FALSE)\n  )\nWHERE\n  l.unique_id &lt; r.unique_id\nUNION ALL\nSELECT\n  l.unique_id AS unique_id_l,\n  r.unique_id AS unique_id_r,\n  l.first_name AS first_name_l,\n  r.first_name AS first_name_r,\n  l.surname AS surname_l,\n  r.surname AS surname_r,\n  l.dob AS dob_l,\n  r.dob AS dob_r,\n  l.city AS city_l,\n  r.city AS city_r,\n  l.tf_city AS tf_city_l,\n  r.tf_city AS tf_city_r,\n  l.email AS email_l,\n  r.email AS email_r,\n  l.`group` AS `group_l`,\n  r.`group` AS `group_r`,\n  '1' AS match_key\nFROM __splink__df_concat_with_tf AS l\nINNER JOIN __splink__df_concat_with_tf AS r\n  ON l.first_name = r.first_name\n  AND CEIL(l.__splink_salt * 4) = 4\n  AND NOT (\n    COALESCE((\n        l.dob = r.dob\n    ), FALSE)\n  )\nWHERE\n  l.unique_id &lt; r.unique_id\n</code></pre>","tags":["Performance","Salting","Spark"]},{"location":"topic_guides/term-frequency.html","title":"Term-Frequency Adjustments","text":"","tags":["Term Frequency","Comparisons"]},{"location":"topic_guides/term-frequency.html#problem-statement","title":"Problem Statement","text":"<p>A common shortcoming of the Fellegi-Sunter model is that it doesn\u2019t account for skew in the distributions of linking variables. One of the starkest examples is a binary variable such as gender in the prison population, where male offenders outnumber female offenders by 10:1. </p> <p></p>","tags":["Term Frequency","Comparisons"]},{"location":"topic_guides/term-frequency.html#how-does-this-affect-our-m-and-u-probabilities","title":"How does this affect our m and u probabilities?","text":"<ul> <li> <p>m probability is unaffected - given two records are a match, the gender field should also match with roughly the same probability for males and females</p> </li> <li> <p>Given two records are not a match, however, it is far more likely that both records will be male than that they will both be female - u probability is too low for the more common value (male) and too high otherwise. </p> </li> </ul> <p>In this example, one solution might be to create an extra comparison level for matches on gender:</p> <ul> <li> <p><code>l.gender = r.gender AND l.gender = 'Male'</code></p> </li> <li> <p><code>l.gender = r.gender AND l.gender = 'Female'</code></p> </li> </ul> <p>However, this complexity forces us to estimate two m probabilities when one would do, and it becomes impractical if we extend to higher-cardinality variables like surname, requiring thousands of additional comparison levels.</p> <p></p> <p>This problem used to be addressed with an ex-post (after the fact) solution - once the linking is done, we have a look at the average match probability for each value in a column to determine which values tend to be stronger indicators of a match. If the average match probability for records pairs that share a surname is 0.2 but the average for the specific surname Smith is 0.1 then we know that the match weight for name should be adjusted downwards for Smiths. </p> <p>The shortcoming of this option is that in practice, the model training is conducted on the assumption that all name matches are equally informative, and all of the underlying probabilities are evaluated accordingly. Ideally, we want to be able to account for term frequencies within the Fellegi-Sunter framework as trained by the EM algorithm. </p>","tags":["Term Frequency","Comparisons"]},{"location":"topic_guides/term-frequency.html#toy-example","title":"Toy Example","text":"<p>Below is an illustration of 2 datasets (10 records each) with skewed distributions of first name. A <code>link_and_dedupe</code> Splink model concatenates these two tables and deduplicates those 20 records.</p> <p></p> <p>In principle, u probabilities for a small dataset like this can be estimated directly - out of 190 possible pairwise comparisons, 77 of them have the same first name. Based on the assumption that matches are rare (i.e. the vast majority of these comparisons are non-matches), we use this as a direct estimate of u. Random sampling makes the same assumption, but by using a manageable-sized sample of a much larger dataset where it would be prohibitively costly to perform all possible comparisons (a Cartesian join).</p> <p>Once we have concatenated our input tables, it is useful to calculate the term frequencies (TF) of each value. Rather than keep a separate TF table, we can add a TF column to the concatenated table - this is what <code>df_concat_with_tf</code> refers to within Splink.</p> <p>Building on the example above, we can define the m and u probabilities for a specific first name value, and work out an expression for the resulting match weight.</p> <p></p> <p>Just as we can add independent match weights for name, DOB and other comparisons (as shown in the Splink waterfall charts), we can also add an independent TF adjustment term for each comparison. This is useful because:</p> <ul> <li> <p>The TF adjustment doesn't depend on m, and therefore does not have to be estimated by the EM algorithm - it is known already</p> </li> <li> <p>The EM algorithm benefits from the TF adjustment (rather than previous post hoc implementations)</p> </li> <li> <p>It is trivially easy to \u201cturn off\u201d TF adjustments in our final match weights if we wish</p> </li> <li> <p>We can easily disentangle and visualise the aggregate significance of a particular column, separately from the deviations within it (see charts below)</p> </li> </ul> <p></p>","tags":["Term Frequency","Comparisons"]},{"location":"topic_guides/term-frequency.html#visualising-tf-adjustments","title":"Visualising TF Adjustments","text":"<p>For an individual comparison of two records, we can see the impact of TF adjustments in the waterfall charts:</p> <p></p> <p>We can also see these match weights and TF adjustments summarised using a chart like the below to highlight common and uncommon names. We do this already using the Splink linker's profile_columns method, but once we know the u probabilities for our comparison columns, we can show these outliers in terms of their impact on match weight:</p> <p></p>","tags":["Term Frequency","Comparisons"]},{"location":"topic_guides/term-frequency.html#applying-tf-adjustments-in-splink","title":"Applying TF adjustments in Splink","text":"<p>Depending on how you compose your Splink settings, TF adjustments can be applied to a specific comparison level in different ways:</p>","tags":["Term Frequency","Comparisons"]},{"location":"topic_guides/term-frequency.html#comparison-template-library-functions","title":"Comparison (template) library functions","text":"<pre><code>import splink.duckdb.duckdb_comparison_library as cl\nimport splink.duckdb.duckdb_comparison_template_library as ctl\n\nsex_comparison = cl.exact_match(\"sex\", \n  term_frequency_adjustments = True\n)\n\nname_comparison = cl.distance_function_at_thresholds(\"name\",\n  distance_function_name = 'jaccard',\n  distance_threshold_or_thresholds = [0.9, 0.7],\n  term_frequency_adjustments = True\n)\n\ndob_comparison = ctl.date_comparison(\"date_of_birth\", \n  term_frequency_adjustments = True\n)\n</code></pre>","tags":["Term Frequency","Comparisons"]},{"location":"topic_guides/term-frequency.html#comparison-level-library-functions","title":"Comparison level library functions","text":"<pre><code>import splink.duckdb.duckdb_comparison_level_library as cll\n\nname_comparison = {\n    \"output_column_name\": \"name\",\n    \"comparison_description\": \"Full name\",\n    \"comparison_levels\": [\n        cll.null_level(\"full_name\"),\n        cll.exact_match_level(\"full_name\", term_frequency_adjustments = True),\n        cll.columns_reversed_level(\"first_name\", \"surname\", \n          tf_adjustment_column = \"surname\"\n        ),\n        cll.else_level(),\n    ],\n}\n</code></pre>","tags":["Term Frequency","Comparisons"]},{"location":"topic_guides/term-frequency.html#providing-a-detailed-spec-as-a-dictionary","title":"Providing a detailed spec as a dictionary","text":"<pre><code>comparison_first_name = {\n    \"output_column_name\": \"first_name\",\n    \"comparison_description\": \"First name jaro dmeta\",\n    \"comparison_levels\": [\n        {\n            \"sql_condition\": \"first_name_l IS NULL OR first_name_r IS NULL\",\n            \"label_for_charts\": \"Null\",\n            \"is_null_level\": True,\n        },\n        {\n            \"sql_condition\": \"first_name_l = first_name_r\",\n            \"label_for_charts\": \"Exact match\",\n            \"tf_adjustment_column\": \"first_name\",\n            \"tf_adjustment_weight\": 1.0,\n            \"tf_minimum_u_value\": 0.001,\n        },\n        {\n            \"sql_condition\": \"jaro_winkler_sim(first_name_l, first_name_r) &gt; 0.8\",\n            \"label_for_charts\": \"Exact match\",\n            \"tf_adjustment_column\": \"first_name\",\n            \"tf_adjustment_weight\": 0.5,\n            \"tf_minimum_u_value\": 0.001,\n        },\n        {\"sql_condition\": \"ELSE\", \"label_for_charts\": \"All other comparisons\"},\n    ],\n}\n</code></pre>","tags":["Term Frequency","Comparisons"]},{"location":"topic_guides/term-frequency.html#more-advanced-applications","title":"More advanced applications","text":"<p>The code examples above show how we can use term frequencies for different columns for different comparison levels, and demonstrated a few other features of the TF adjustment implementation in Splink:</p>","tags":["Term Frequency","Comparisons"]},{"location":"topic_guides/term-frequency.html#multiple-columns","title":"Multiple columns","text":"<p>Each comparison level can be adjusted on the basis of a specified column. In the case of exact match levels, this is trivial but it allows some partial matches to be reframed as exact matches on a different derived column.  One example could be ethnicity, often provided in codes as a letter (W/M/B/A/O - the ethnic group) and a number. Without TF adjustments, an ethnicity comparison might have 3 levels - exact match, match on ethnic group (<code>LEFT(ethnicity,1)</code>), no match. By creating a derived column <code>ethnic_group = LEFT(ethnicity,1)</code> we can apply TF adjustments to both levels.</p> <pre><code>ethnicity_comparison = {\n    \"output_column_name\": \"ethnicity\",\n    \"comparison_description\": \"Self-defined ethnicity\",\n    \"comparison_levels\": [\n        cll.null_level(\"ethnicity\"),\n        cll.exact_match_level(\"ethnicity\", term_frequency_adjustments = True),\n        cll.exact_match_level(\"ethnic_group\", term_frequency_adjustments = True),\n        cll.else_level(),\n    ],\n}\n</code></pre> <p>A more critical example would be a full name comparison that uses separate first name and surname columns. Previous implementations would apply TF adjustments to each name component independently, so \u201cJohn Smith\u201d would be adjusted down for the common name \u201cJohn\u201d and then again for the common name \u201cSmith\u201d. However, the frequencies of names are not generally independent (e.g. \u201cMohammed Khan\u201d is a relatively common full name despite neither name occurring frequently). A simple full name comparison could therefore be structured as follows:</p> <pre><code>name_comparison = {\n    \"output_column_name\": \"name\",\n    \"comparison_description\": \"Full name\",\n    \"comparison_levels\": [\n        cll.null_level(\"full_name\"),\n        cll.exact_match_level(\"full_name\", term_frequency_adjustments = True),\n        cll.exact_match_level(\"first_name\", term_frequency_adjustments = True),\n        cll.exact_match_level(\"surname\", term_frequency_adjustments = True),\n        cll.else_level(),\n    ],\n}\n</code></pre>","tags":["Term Frequency","Comparisons"]},{"location":"topic_guides/term-frequency.html#fuzzy-matches","title":"Fuzzy matches","text":"<p>All of the above discussion of TF adjustments has assumed an exact match on the column in question, but this need not be the case. Where we have a \u201cfuzzy\u201d match between string values, it is generally assumed that there has been some small corruption in the text, for a number of possible reasons. A trivial example could be <code>\"Smith\"</code> vs <code>\"Smith \"</code> which we know to be equivalent if not an exact string match.</p> <p>In the case of a fuzzy match, we may decide it is desirable to apply TF adjustments for the same reasons as an exact match, but given there are now two distinct sides to the comparison, there are also two different TF adjustments. Building on our assumption that one side is the \u201ccorrect\u201d or standard value and the other contains some mistake, Splink will simply use the greater of the two term frequencies. There should be more <code>\"Smith\"</code>s than <code>\"Smith \"</code>s, so the former provides the best estimate of the true prevalence of the name Smith in the data. </p> <p>In cases where this assumption might not hold and both values are valid and distinct (e.g. <code>\"Alex\"</code> v <code>\"Alexa\"</code>), this behaviour is still desirable. Taking the most common of the two ensures that we err on the side of lowering the match score for a more common name than increasing the score by assuming the less common name.  </p> <p>TF adjustments will not be applied to any comparison level without explicitly being turned on, but to allow for some middle ground when applying them to fuzzy match column, there is a <code>tf_adjustment_weight</code> setting that can down-weight the TF adjustment. A weight of zero is equivalent to turning TF adjustments off, while a weight of 0.5 means the match weights are halved, mitigating their impact:</p> <pre><code>{\n  \"sql_condition\": \"jaro_winkler_sim(first_name_l, first_name_r) &gt; 0.8\",\n  \"label_for_charts\": \"Exact match\",\n  \"tf_adjustment_column\": \"first_name\",\n  \"tf_adjustment_weight\": 0.5\n}\n</code></pre>","tags":["Term Frequency","Comparisons"]},{"location":"topic_guides/term-frequency.html#low-frequency-outliers","title":"Low-frequency outliers","text":"<p>Another example of where you may wish to limit the impact of TF adjustments is for exceedingly rare values. As defined above, the TF-adjusted match weight, K is inversely proportional to the term frequency, allowing K to become very large in some cases. </p> <p>Let\u2019s say we have a handful of records with the misspelt first name \u201cSiohban\u201d (rather than \u201cSiobhan\u201d). Fuzzy matches between the two spellings will rightly be adjusted on the basis of the frequency of the correct spelling, but there will be a small number of cases where the misspellings match one another. Given we suspect these values are more likely to be misspellings of more common names, rather than a distinct and very rare name, we can mitigate this effect by imposing a minimum value on the term frequency used (equivalent to the u value). This can be added to your full settings dictionary as in the example above using <code>\"tf_minimum_u_value\": 0.001</code>. This means that for values with a frequency of &lt;1 in 1000, it will be set to 0.001.</p>","tags":["Term Frequency","Comparisons"]}]}